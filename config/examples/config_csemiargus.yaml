#=====================Dataset Configuration============================#

samples: config/samples_csemiargus.tsv

units: config/units.tsv

dataset: "csemiargus"

#===================== Reference Configuration ========================#

# Before using your reference it is important it meets certain 
# requirements for downstream analyses. The first is that it should be 
# uncompressed, or, if compressed, only in bgzip format (from samtools).
# The second is that scaffold names should be concise (10-15 char.) 
# with only letters, numbers, and underscores.
#
# chunk_size - Most analyses can be run per scaffold and merged later
# If a value is given, the genome will be broken into chunks of roughly
# this size which will be analyzed in parallel when appropriate. In the 
# current implementation, scaffolds will not be split if they roll over 
# the chunk size, instead, it will fit however many whole scaffolds it 
# can in that chunk. This may be improved later on.
#
# fasta - A string with the absolute or relative path to a local copy of
# the fasta.
#
# mito - Scaffold containing mitochondria, will be excluded 
# from autosomal analyses. Leave blank if not present in reference or 
# cannot be identified.
#
# XZ - Scaffold(s) that are a part of XZ chromosomes. Will be excluded 
# from autosomal analyses and included in sex-chromosome analyses.
#
# exclude - Scaffold(s) to exclude (i.e. abnormally sex-linked, lower-
# quality) from all analyses. Scaffolds smaller than the min_size 
# option don't need to be specified here.
#
# min_size - Scaffolds shorter than this number (bp) will be excluded.

chunk_size: 30000000

reference:
  name: "ilCyaSemi1.1"
  fasta: "results/ref/ilCyaSemi1.1/GCA_905187585.1_ilCyaSemi1.1_genomic.fna"
  mito: ["LR994570.1"]
  XZ: ["LR994547.1"]
  exclude: []
  min_size: 1000000

#===================== Sample Set Configuration =======================#

# Sometimes you will want to exclude individuals from downstream 
# analyses (too low coverage, relatives in dataset, duplicated), but do 
# not want to remove them from the sample list. List these individuals 
# here and they will be removed.

exclude_ind: ["MZLU107455","MZLU153221","MZLU107501"]

#==================== Downsampling Configuration ======================#

downsample_cov: 6

#====================== Analysis Selection ============================#
# Here you select what steps in the pipeline you would like to perform.
# Individual and dataset level analyses are performed for all 
# individuals in the dataset. 'populations' option can take a list of 
# populations to perform population level analyses on. If this is left
# empty, population level analyses will be performed for each unique
# population name in the sample list, excluding single sample 
# populations in analyses that are biased by or do not support them.

populations:

analyses:
  # filtering
  genmap: true
  repeatmasker:
    local_lib:
    dfam_lib:
    build_lib: true
  extreme_depth: true
  dataset_missing_data: true
  # excess_hetero: true
  # quality control
  qualimap: true
  damageprofiler: true
  relatedness: true
  # population genomic analyses
  pca_pcangsd: true
  admix_ngsadmix: true
  thetas_angsd: true
  heterozygosity_angsd: true
  fst_angsd:
    populations: true
    individuals: false
  inbreeding_ngsf-hmm: true

#===================== Software Configuration =========================#

mapQ: 30
baseQ: 30

params:
  genmap:
    K: "100"
    E: "2"
    map_thresh: 1
  picard:
    # By default we set Picard to remove duplicate reads, change to 
    # false to flag only.
    MarkDuplicates: "REMOVE_DUPLICATES=true OPTICAL_DUPLICATE_PIXEL_DISTANCE=2500"
  angsd:
    # gl_model - 1=SAMtools, 2=GATK, 3=SOAPsnp, 4=SYK; default is 1
    gl_model: 2
    # max_missing - Exclude sites based on missing data in individuals 
    # where 0.0 allows sites with completely missing data and 1.0 
    # permits no missing data. You have the option to set different 
    # values for the dataset as a whole (such as for making a PCA) and 
    # for individual populations (calculating thetas, F, ROH, etc.). We 
    # set the default to 0.2 (20% missing data) for populations and 0.0 
    # for the whole dataset.
    max_missing_pop: 0.2
    max_missing_dataset: 0.1
    # min_depth_mult - A multiplier to define the minimum depth to include a 
    # site in an analysis, it is multiplied by the average depth. A hard coded 
    # minimum depth of 3 per individual is always included.
    min_depth_mult: 0.33
    # max_depth_mult - A multiplier to define the maximum depth to include a 
    # site in an analysis, it is multiplied by the average depth.
    max_depth_mult: 3
    # extra - any additional filtering parameters to be used across all 
    # analyses with angsd. By default, we use -C 50 to correct for 
    # excessive mismatches, only include paired reads when both mates map,
    # remove reads with multiple best hits, and reduce base quality scores 
    # based on baq (penalizes bases around indels)
    extra: "-C 50 -only_proper_pairs 1 -uniqueOnly 1"
    # fold - Define if SFS should be folded when made. If you don't 
    # have ancestral states to assign to -anc, it is best to set to 1. 
    # We set the default to 1.
    fold: 1
    # sfsboot - Define number of bootstrap SFS to produce, we default 
    # to 100.
    sfsboot: 100
    snp_pval: "1e-6"
    min_maf: 0.05
  ngsadmix:
    kvalues: [1,2,3,4,5,6,7,8,9,10]
    extra: "-maxiter 4000"
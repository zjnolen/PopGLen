{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the documentation for PopGLen","text":"<p>PopGLen is aimed at enabling users to run population genomic analyses on their data within a genotype likelihood framework in an automated and reproducible fashion. Genotype likelihood based analyses avoid genotype calling, instead performing analyses on the likelihoods of each possible genotype, incorporating uncertainty about the true genotype into the analysis. This makes them especially suited for datasets with low coverage or that vary in coverage.</p> <p>This pipeline was developed in initially to make my own analyses easier. I work with many species being mapped to their own references within the same project. I developed this pipeline so that I could ensure standardized processing for datasets within the same project and to automate the many steps that go into performing these analyses. As it needed to fit many datasets, it is generalizable and customizable through a single configuration file, uses a common workflow utilized by ANGSD users, and provides reporting features to quickly assess sample quality, filtering, and results. As such, it is applicable to a wide variety of studies that utilize population genomics on whole genome data.</p> <p>This webpage contains documentation so others can utilize the features of this pipeline, including a short getting started page, a detailed configuration summary, a tutorial to walk users through running all analyses in the pipeline, as well as more specialized documentation describing how to configure the pipeline to run on a HPC cluster, how to modify the requested resources for jobs, how to extend the workflow with additional rules, and the file paths for all major output files for reference.</p> Questions? Feature requests? Just ask! <p>I'm glad to answer questions on the GitHub Issues page for the project, as well as take suggestions for features or improvements!</p>"},{"location":"#pipeline-summary","title":"Pipeline Summary","text":"<p>The pipeline aims to follow the general path many users will use when working with ANGSD and other GL based tools. Raw sequencing data is processed into BAM files (with optional configuration for historical degraded samples) or BAM files are provided directly. From there, several quality control reports are generated to help determine what samples are included. The pipeline then builds a 'sites' file to perform analyses with. This sites file is made from several user-configured filters (including identification and removal of repetitive content, sites with extreme global sequencing depth, and/or low mappability), intersecting all and outputing a list of sites for the analyses to be performed on across all samples. This can also be extended by user-provided filter lists (e.g. to limit to neutral sites, genic regions, etc.).</p> <p>After samples have been processed, quality control reports produced, and the sites file has been produced, the pipeline can continue to the analyses.</p> <ul> <li>Linkage disequilibrium estimation, LD decay, LD pruning (ngsLD)</li> <li>PCA (PCAngsd)</li> <li>Admixture (NGSAdmix)</li> <li>Inbreeding/Runs of Homozygosity (ngsF-HMM)</li> <li>Relatedness (NgsRelate, IBSrelate)</li> <li>Identity by state matrix (ANGSD)</li> <li>Site frequency spectrum (ANGSD)</li> <li>Watterson's estimator (\u03b8<sub>w</sub>), Nucleotide diversity (\u03c0), Tajima's D   (ANGSD)</li> <li>Individual heterozygosity with bootstrapped confidence intervals (ANGSD)</li> <li>Pairwise F<sub>ST</sub> (ANGSD)</li> <li>Population allele frequencies (ANGSD)</li> </ul> <p>These all can be enabled and processed independently, and the pipeline will generate genotype likelihood input files using ANGSD and share them across analyses as appropriate, deleting temporary intermediate files when they are no longer needed.</p> <p>At any point after a successful completion of a portion of the pipeline, a report can be made that contains tables and figures summarizing the results for the currently enabled parts of the pipeline.</p> <p>If you're interested in using this, head to the Getting Started page!</p>"},{"location":"#manuscripts-utilizing-popglen","title":"Manuscripts utilizing PopGLen","text":"<p>Early versions of PopGLen have been used in the following manuscripts, which may help to give an idea of the scope of analyses that it can be utilized for:</p> <p>Nolen ZJ, Rundl\u00f6f M, Runemark A. 2024 Species-specific erosion of genetic diversity in grassland butterflies depends on landscape land cover. Biological Conservation 296, 110694. (doi:10.1016/j.biocon.2024.110694)</p> <p>Nolen ZJ, Jamelska P, Torres Lara AS, Wahlberg N, Runemark A. 2024 [preprint] Species-specific loss of genetic diversity and accumulation of genetic load following agricultural intensification. bioRxiv, 2024.10.07.616612. (doi:10.1101/2024.10.07.616612)</p>"},{"location":"#software-utilized-by-popglen","title":"Software utilized by PopGLen","text":"<p>PopGLen is a workflow that incorporates several underlying tools. Here, you will find a list of references for the main underlying tools and when they are utilized. Please cite the underlying tool if you utilize it through PopGLen.</p> <p>fastp: Chen, S., Zhou, Y., Chen, Y., &amp; Gu, J. (2018). fastp: An ultra-fast all-in-one FASTQ preprocessor. Bioinformatics, 34(17), i884\u2013i890. https://doi.org/10.1093/bioinformatics/bty560 (Adapter trimming, read collapsing, fastq QC)</p> <p>BWA: Li, H., &amp; Durbin, R. (2009). Fast and accurate short read alignment with Burrows\u2013Wheeler transform. Bioinformatics, 25(14), 1754\u20131760. https://doi.org/10.1093/bioinformatics/btp324 (Read mapping)</p> <p>SAMtools: Danecek, P., Bonfield, J. K., Liddle, J., Marshall, J., Ohan, V., Pollard, M. O., Whitwham, A., Keane, T., McCarthy, S. A., Davies, R. M., &amp; Li, H. (2021). Twelve years of SAMtools and BCFtools. GigaScience, 10(2). https://doi.org/10.1093/gigascience/giab008 (Endogenous content calculation, depth subsampling, bam file manipulation)</p> <p>Picard: Picard toolkit. (2019). Broad Institute, GitHub Repository. https://broadinstitute.github.io/picard/ (Duplicate removal for paired end reads)</p> <p>Dedup: Peltzer, A., J\u00e4ger, G., Herbig, A., Seitz, A., Kniep, C., Krause, J., &amp; Nieselt, K. (2016). EAGER: Efficient ancient genome reconstruction. Genome Biology, 17(1), Article 1. https://doi.org/10.1186/s13059-016-0918-z (Duplicate removal for collapsed reads)</p> <p>GATK: Auwera, G. A. V. der, &amp; O\u2019Connor, B. D. (2020). Genomics in the Cloud: Using Docker, GATK, and WDL in Terra. O\u2019Reilly Media, Inc. (Realignment around indels)</p> <p>mapDamage2.0: J\u00f3nsson, H., Ginolhac, A., Schubert, M., Johnson, P. L. F., &amp; Orlando, L. (2013). mapDamage2.0: Fast approximate Bayesian estimates of ancient DNA damage parameters. Bioinformatics, 29(13), 1682\u20131684. https://doi.org/10.1093/bioinformatics/btt193 (Estimation of post-mortem DNA damage, base recalibration for damage correction)</p> <p>DamageProfiler: Neukamm, J., Peltzer, A., &amp; Nieselt, K. (2021). DamageProfiler: Fast damage pattern calculation for ancient DNA. Bioinformatics, 37(20), 3652\u20133653. https://doi.org/10.1093/bioinformatics/btab190 (Estimation of post-mortem DNA damage)</p> <p>bamUtil: Jun, G., Wing, M. K., Abecasis, G. R., &amp; Kang, H. M. (2015). An efficient and scalable analysis framework for variant extraction and refinement from population scale DNA sequence data. Genome Research, gr.176552.114. https://doi.org/10.1101/gr.176552.114 (Clipping of overlapping reads so they are not double counted by ANGSD)</p> <p>Qualimap: Garc\u00eda-Alcalde, F., Okonechnikov, K., Carbonell, J., Cruz, L. M., G\u00f6tz, S., Tarazona, S., Dopazo, J., Meyer, T. F., &amp; Conesa, A. (2012). Qualimap: Evaluating next-generation sequencing alignment data. Bioinformatics, 28(20), 2678\u20132679. https://doi.org/10.1093/bioinformatics/bts503 (General BAM file QC)</p> <p>ngsLD: Fox, E. A., Wright, A. E., Fumagalli, M., &amp; Vieira, F. G. (2019). ngsLD: Evaluating linkage disequilibrium using genotype likelihoods. Bioinformatics, 35(19), 3855\u20133856. https://doi.org/10.1093/bioinformatics/btz200 (Estimation of linkage disequilibrium for pruning, LD decay, LD sampling)</p> <p>PCAngsd: Meisner, J., &amp; Albrechtsen, A. (2018). Inferring Population Structure and Admixture Proportions in Low-Depth NGS Data. Genetics, 210(2), 719\u2013731. https://doi.org/10.1534/genetics.118.301336 (Principal component analysis)</p> <p>NGSadmix: Skotte, L., Korneliussen, T. S., &amp; Albrechtsen, A. (2013). Estimating Individual Admixture Proportions from Next Generation Sequencing Data. Genetics, 195(3), 693\u2013702. https://doi.org/10.1534/genetics.113.154138 (Admixture analysis)</p> <p>ngsRelateV2: Hangh\u00f8j, K., Moltke, I., Andersen, P. A., Manica, A., &amp; Korneliussen, T. S. (2019). Fast and accurate relatedness estimation from high-throughput sequencing data in the presence of inbreeding. GigaScience, 8(5), giz034. https://doi.org/10.1093/gigascience/giz034 (Relatedness estimation)</p> <p>IBSrelate: Waples, R. K., Albrechtsen, A., &amp; Moltke, I. (2019). Allele frequency-free inference of close familial relationships from genotypes or low-depth sequencing data. Molecular Ecology, 28(1), 35\u201348. https://doi.org/10.1111/mec.14954 (Relatedness estimation)</p> <p>ANGSD: Korneliussen, T. S., Albrechtsen, A., &amp; Nielsen, R. (2014). ANGSD: Analysis of Next Generation Sequencing Data. BMC Bioinformatics, 15(1), Article \\1. https://doi.org/10.1186/s12859-014-0356-4 (Depth calculation, genotype likelihood estimation, SNP calling, allele frequencies, SFS, diverity and neutrality statistics, heterozygosity, $F_{ST}$)</p> <p>ngsF-HMM: Vieira, F. G., Albrechtsen, A., &amp; Nielsen, R. (2016). Estimating IBD tracts from low coverage NGS data. Bioinformatics, 32(14), 2096\u20132102. https://doi.org/10.1093/bioinformatics/btw212 (Inbreeding and runs of homozygosity estimation)</p> <p>RepeatModeler2: Flynn, J. M., Hubley, R., Goubert, C., Rosen, J., Clark, A. G., Feschotte, C., &amp; Smit, A. F. (2020). RepeatModeler2 for automated genomic discovery of transposable element families. Proceedings of the National Academy of Sciences, 117(17), 9451\u20139457. https://doi.org/10.1073/pnas.1921046117 (Building repeat library from reference genome)</p> <p>RepeatMasker: Smit, A., Hubley, R., &amp; Green, P. (2013). RepeatMasker Open-4.0 (4.1.2) [Computer software]. http://www.repeatmasker.org (Identifying repeat regions from repeat library)</p> <p>GenMap: Pockrandt, C., Alzamel, M., Iliopoulos, C. S., &amp; Reinert, K. (2020). GenMap: Ultra-fast computation of genome mappability. Bioinformatics, 36(12), 3687\u20133692. https://doi.org/10.1093/bioinformatics/btaa222 (Estimation of per-base mappability scores)</p> <p>BEDTools: Quinlan, A. R. &amp; Hall, I. M. (2010). BEDTools: a flexible suite of utilities for comparing genomic features. Bioinformatics, 26(6), 841-842. https://doi.org/10.1093/bioinformatics/btq033 (Transforming and intersecting filter files)</p>"},{"location":"cluster-execution/","title":"Running PopGLen on a cluster using a job queue","text":"<p>PopGLen is primarily made for running on a high performance computing system that has a job scheduler. This system allows for each rule to be submitted as a single job on a system with a large number of nodes, enabling large levels of parallelization. Snakemake is well integrated to many job scheduling systems, and some examples can be found on the Snakemake plugin catalog.</p>"},{"location":"cluster-execution/#command-line-options-for-cluster-execution","title":"Command line options for cluster execution","text":"<p>Here, we will walk through what an execution might look like on an HPC with the SLURM job scheduler. This requires a conda environment with Snakemake installed, along with the snakemake-executor-plugin-slurm installed:</p> <pre><code>conda create \\\n    -n popglen \\\n    -c conda-forge -c bioconda \\\n    snakemake=8.25.0 \\\n    snakemake-executor-plugin-slurm\n\nconda activate popglen\n\n## OR, if you already set up a popglen environment and just need to add the\n## executor plugin:\n\nconda activate popglen\n\nconda install -c conda-forge -c bioconda snakemake-executor-plugin-slurm\n</code></pre> <p>Once you've set up a working directory and workflow for PopGLen, you can run it using the following command:</p> <pre><code>snakemake --use-conda --use-singularity --executor slurm \\\n    --set-resources slurm_account=&lt;project&gt; slurm_partition=&lt;partition name&gt;\n</code></pre> <p>This will ensure that rules will be submitted as jobs to the job queue, by default using the SLURM account and partition defined in <code>--set-resources</code>. The values entered for these will depend on your system, but would be the equivalent of the account and partition options you set in the header of SLURM scripts:</p> <pre><code>#SBATCH -A &lt;project&gt;\n#SBATCH -p &lt;partition name&gt;\n</code></pre>"},{"location":"cluster-execution/#combining-command-line-options-into-a-profile","title":"Combining command line options into a profile","text":"<p>You may wish to set a few more options, such as the maximum number of threads available to each job, the maximum number of jobs to have running and in the queue at once based on your queue's limits, and how many local cores to use for rules that run outside the job system (in PopGLen these are quick commands like making lists of BAM files or making symbolic links). These quickly begin to add up, so take a look at how we define these all in an example profile for the HPC system Dardel at PDC:</p> profiles/dardel/config.yaml<pre><code>restart-times: 3\nlocal-cores: 1\nuse-conda: true\nuse-singularity: true\njobs: 999\nkeep-going: true\nmax-threads: 128\nexecutor: slurm\nsingularity-args: '--tmp-sandbox -B /cfs/klemming'\ndefault-resources:\n  - \"mem_mb=(threads*1700)\"\n  - \"runtime=60\"\n  - \"slurm_account=&lt;account&gt;\"\n  - \"slurm_partition=shared\"\n  - \"tmpdir='/cfs/klemming/scratch/u/user'\"\n</code></pre> <p>Assuming we place this file in the working directory under <code>profiles/dardel</code>, we can run Snakemake with simply:</p> <pre><code>snakemake --profile ./profiles/dardel\n</code></pre> <p>and it will automatically use the options defined in the profile. Here, we allow up to 3 retries for failed jobs, 1 local core for running local jobs, enable both conda and singularity usage, set a max of 999 jobs to be running at once, tell rules to keep going if one fails, set a maximum of 128 threads for a single job (the size of a node on Dardel), define SLURM as the executor, set some required arguments to be passed to Singularity on Dardel, and set several default resources, including giving each rule 1.7GB memory per thread, which is the amount available on Dardel's shared partition. We also set the temporary directory to use, as it is not set on Dardel by default.</p> <p>Note, that if you're not on Dardel, this will need some changes, so make a new profile that matches your system. You'll likely need to change <code>max-threads</code>, <code>slurm_account</code>, <code>slurm_partition</code>, and <code>tmpdir</code>. <code>mem_mb</code> can have 1700 changed to match the amount of memory your system has per core. <code>singularity-args</code> is specific to Dardel, and can be omitted, unless your system requires something similar. At the minimum, <code>-B /cfs/klemming</code> will need to be changed or removed.</p>"},{"location":"cluster-execution/#executing-the-workflow-while-away-using-screen","title":"Executing the workflow while away using Screen","text":"<p>If you run Snakemake in the login shell of your system, it will cancel when you logout, which is not ideal for the expected long runtimes for processing WGS data. If your system lets you submit new jobs from within other jobs, you can submit your Snakemake command as a long running SLURM job. Be sure to activate the conda environment for PopGLen either inside the job before running Snakemake or before submitting the job, as the job inherits your active environment.</p> <p>If you can't submit jobs from inside other jobs, or you would like the option to interact with it while its running, you can run it inside a screen, if screen is available on your system. First start a new screen:</p> <pre><code>screen -S project-name\n</code></pre> <p>This will open up a virtual terminal that will stay active as long as your system is online. Inside this terminal, you can activate snakemake, do dry runs, and start the workflow:</p> <pre><code>conda activate popglen\n\n# do a dry run\nsnakemake --profile ./profiles/my-cluster -n\n\n# do a real run\nsnakemake --profile ./profiles/my-cluster\n</code></pre> <p>Snakemake will then submit jobs from inside the screen. You can disconnect from the screen with CTRL-A + D, and safely log out without Snakemake being interrupted. Then, you can go back in by resuming your screen:</p> <pre><code>screen -r project-name\n</code></pre> <p>and see how far it has gotten (or if it fails and you need to change something).</p> <p>When you're done with a screen, you can kill it with CTRL-A + K.</p>"},{"location":"config-cheat/","title":"Configuration cheat sheet","text":"<p>This contains the same information largely as the configuration docs, but in an interactive <code>config.yaml</code> file. This can be helpful if you have your config in front of you and want to check what a setting does. Just click the <code>+</code> next to each line to get info on that setting.</p> config/config.yaml<pre><code>#=====================Dataset Configuration============================#\n\nsamples: config/samples.tsv # (1)\n\nunits: config/units.tsv # (2)\n\ndataset: \"dataset-name\" # (3)\n\n#===================== Reference Configuration ========================#\n\nchunk_size: 50000000 # (4)\n\nreference:\n  name: \"dataset-ref\" # (5)\n  fasta: \"resources/ref/reference-genome.fa\" # (6)\n  mito: [\"mitochondrion\"] # (7)\n  sex-linked: [\"Z\"] # (8)\n  exclude: [] # (9)\n  min_size: 1000000 # (10)\n\nancestral: # (11)\n\n#===================== Sample Set Configuration =======================#\n\nexclude_ind: [] # (12)\n\nexcl_pca-admix: [] # (13)\n\n#====================== Analysis Selection ============================#\n\npopulations: [] # (14)\n\nanalyses:\n  # mapping options\n  mapping:\n    historical_only_collapsed: true # (15)\n    historical_collapsed_aligner: \"aln\" # (16)\n  # sites file filters\n  pileup-mappability: # (17)\n  repeatmasker: # (18)\n    bed: # (19)\n    local_lib: # (20)\n    build_lib: true # (21)\n  extreme_depth: true # (22)\n  dataset_missing_data: # (23)\n  population_missing_data: # (24)\n  # quality control\n  qualimap: true # (25)\n  ibs_ref_bias: true # (26)\n  damageprofiler: true # (27)\n  mapdamage_rescale: # (28)\n  # population genomic analyses\n  estimate_ld: # (29)\n  ld_decay: true # (30)\n  pca_pcangsd: true # (31)\n  admix_ngsadmix: true # (32)\n  relatedness: # (33)\n    ibsrelate_ibs: # (34)\n    ibsrelate_sfs: # (35)\n    ngsrelate_ibsrelate-only: true # (36)\n    ngsrelate_freqbased: # (37)\n  1dsfs: # (38)\n  1dsfs_boot: # (39)\n  2dsfs: # (40)\n  2dsfs_boot: # (41)\n  thetas_angsd: true # (42)\n  heterozygosity_angsd: true # (43)\n  fst_angsd: # (44)\n    populations: true # (45)\n    individuals: # (46)\n  inbreeding_ngsf-hmm: true # (47)\n  ibs_matrix: # (48)\n  pop_allele_freqs: true # (49)\n\n#==================== Downsampling Configuration ======================#\n\nsubsample_dp: [4] # (50)\n\nsubsample_by: \"sitefilt\" # (51) three options: unfilt, mapqbaseq, sitefilt; sitefilt recommended (subsamples bams to `subsample_dp` within filtered regions)\nredo_depth_filts: false # (52) has no effect when `subsample_by` == \"sitefilt\"\ndrop_samples: [] # (53) These samples won't be included in the downsampled dataset\n\nsubsample_analyses: # (54)\n  estimate_ld:\n  ld_decay:\n  pca_pcangsd:\n  admix_ngsadmix:\n  relatedness:\n    ibsrelate_ibs:\n    ibsrelate_sfs:\n    ngsrelate_ibsrelate-only:\n    ngsrelate_freqbased:\n  1dsfs:\n  1dsfs_boot:\n  2dsfs:\n  2dsfs_boot:\n  thetas_angsd: true\n  heterozygosity_angsd: true\n  fst_angsd:\n    populations: true\n    individuals:\n  inbreeding_ngsf-hmm: true\n  ibs_matrix:\n  pop_allele_freqs: true\n\n#=========================== Filter Sets ==============================#\n\nfilter_beds: #(55)\n  example:\n\nonly_filter_beds: false # (56)\n\n#===================== Software Configuration =========================#\n\nmapQ: 30 # (57)\nbaseQ: 20 # (58)\n\nparams:\n  clipoverlap:\n    clip_user_provided_bams: false # (59)\n  fastp:\n    extra: \"-p -g\" # (60) don't put --merge or --overlap_len_require here, they're implicit\n    min_overlap_hist: 30 # (61)\n  bwa_aln:\n    extra: \"-l 16500 -n 0.01 -o 2\" # (62)\n  picard:\n    MarkDuplicates: \"--REMOVE_DUPLICATES true --OPTICAL_DUPLICATE_PIXEL_DISTANCE 2500\" #(63)\n  damageprofiler:\n    profile_modern: false # (64)\n  genmap: # (65)\n    K: \"30\" # (66)\n    E: \"2\" # (67)\n    map_thresh: 1 # (68)\n  extreme_depth_filt: # (69)\n    method: \"percentile\" # (70)\n    bounds: [ 0.025, 0.975 ] # (71)\n    filt-on-dataset: true # (72)\n    filt-on-depth-classes: true # (73)\n  samtools:\n    subsampling_seed: \"$RANDOM\" # (74)\n  angsd: # (75)\n    gl_model: 2 # (76) gl_model - 1=SAMtools, 2=GATK, 3=SOAPsnp, 4=SYK\n    maxdepth: 100 # (77) for depth calc only\n    mindepthind: 1 # (78)\n    minind_dataset: 0.75 # (79) as a proportion of N\n    minind_pop: 0.75 # (80) as a proportion of N\n    rmtrans: true # (81)\n    extra: \"\" # (82) goes to all ANGSD runs\n    extra_saf: \"\" # (83) goes to all -doSaf runs\n    extra_beagle: \"\" # (84) goes to all -doGlf 2 runs\n    domajorminor: 1 # (85) Currently only compatible with values 1, 2, 4, 5\n    domaf: 1 # (87) Currently only compatible with values 1 &amp; 2\n    snp_pval: \"1e-6\" # (86)\n    min_maf: 0.05 # (88) Set -1 to disable\n    mindepthind_heterozygosity: 1 # (89)\n  ngsld: # (90)\n    max_kb_dist_est-ld: 4000 # (91)\n    rnd_sample_est-ld: 0.001 # (92)\n    max_kb_dist_decay: 100 # (93)\n    rnd_sample_decay: 0.01 # (94)\n    fit_LDdecay_extra: \"--fit_level 1 --plot_size 3,6 --plot_y_lim 1\" # (95)\n    fit_LDdecay_n_correction: true # (96)\n    max_kb_dist_pruning_dataset: 25 # (97) used for PCA, Admix, not ngsF-HMM\n    pruning_min-weight_dataset: 0.2 # (98) used for PCA, Admix, not ngsF-HMM\n  ngsrelate: # (99)\n    ibsrelate-only-extra: \"\" #(100)\n    freqbased-extra: \"\" # (101)\n  ngsf-hmm: # (102)\n    estimate_in_pops: true # (103)\n    prune: true # (104)\n    max_kb_dist_pruning_pop: 25 # (105) used only for ngsf-hmm\n    pruning_min-weight_pop: 0.4 # (106) used only for ngsf-hmm\n    min_roh_length: 50000 # (107)\n    roh_bins: [ 100000, 250000, 500000, 1000000, 2500000, 5000000 ] # (108) in ascending order, bins are between adjacent values and between min_roh_length and value 1 and the last value and infinity\n  realsfs: # (109)\n    fold: 1 # (110) Should only be set to 1, unless ancestral reference is given above\n    sfsboot: 50 # (111)\n  fst: # (112)\n    whichFst: 1 # (113)\n    win_size: 25000 # (114)\n    win_step: 10000 # (115)\n  thetas: # (116)\n    win_size: 25000 # (117)\n    win_step: 10000 # (118)\n    minsites: 1000 # (119)\n  ngsadmix: # (120)\n    kvalues: [1,2,3,4,5] # (121)\n    reps: 100 # (122)\n    minreps: 20 # (123)\n    thresh: 2 # (124)\n    conv: 3 # (125)\n    extra: \"-maxiter 4000\" # (126)\n  ibs: # (127)\n    doibs: 1 # (128)\n</code></pre> <ol> <li>An absolute or relative path from the working directory to the <code>samples.tsv</code>    file.</li> <li>An absolute or relative path from the working directory to the <code>units.tsv</code>    file.</li> <li>A name for this dataset run - an identifier for a batch of samples to be    analysed together with the same configuration.</li> <li>A size in bp (integer). Your reference will be analyzed in    'chunks' of contigs of this size to parallelize processing. This size should    be larger than the largest contig in your genome. A larger number means fewer    jobs that run longer. A smaller number means more jobs that run shorter. The    best fit will depend on the reference and the compute resources you have    available. Leaving this blank will not divide the reference up into chunks.</li> <li>A name for your reference genome, will go in the file names and be    used to organize the reference filter outputs.</li> <li>A path to the reference fasta file (currently only supports    uncompressed fasta files).</li> <li>Mitochondrial contig name(s), will be removed from analysis. Should    be listed as a list of strings within brackets <code>[]</code>.</li> <li>Sex-linked contig name(s), will be removed from analysis.    Should be listed as a list of strings within brackets <code>[]</code>.</li> <li>Additional contig name(s) to exclude from analysis. Should be    listed as a list of strings within brackets <code>[]</code>.</li> <li>A size in bp (integer). All contigs below this size will be excluded from     analysis.</li> <li>A path to a fasta file containing the ancestral states in your     reference genome. This is optional, and is used to polarize allele     frequencies in SAF files to ancestral/derived. If you leave this empty,     the reference genome itself will be used as ancestral, and you should be     sure the [<code>params</code>] [<code>realSFS</code>] [<code>fold</code>] is set to <code>1</code>. If you put a fasta     here, you can set that to <code>0</code>.</li> <li>Sample name(s) that will be excluded from the workflow. Should     be a list in <code>[]</code>. As an alternative, putting a <code>#</code> in front of the sample     in the sample list also will remove it. Mainly used to drop samples with     poor quality after initial processing.</li> <li>Sample name(s) that will be excluded only from PCA and     Admixture analyses. Useful for close relatives that violate the assumptions     of these analyses, but that you want in others. Should be a list in <code>[]</code>. If     you   want relatives out of all downstream analyses, not just PCA/Admix, put     them in <code>exclude_ind</code> instead. Note this will trigger a re-run for     relatedness analyses, but you can just disable them now as they've already     been run.</li> <li>A list of populations found in your sample list to limit     population analyses to. Might be useful if you want to perform individual     analyses on some samples, but not include them in any population level     analyses. Leave blank (<code>[]</code>) if you want population level analyses on all     the populations defined in your <code>samples.tsv</code> file.</li> <li>Historical samples are expected to have     fragmented DNA. For this reason, overlapping (i.e. shorter, usually     &lt;270bp) read pairs are collapsed in this workflow for historical samples.     Setting this option to <code>true</code> will only map only these collapsed reads,     and is recommended to target primarily endogenous content. However, in     the event you want to map both the collapsed and uncollapsed reads, you     can set this to <code>false</code>. (<code>true</code>/<code>false</code>)</li> <li>Aligner used to map collapsed historical     sample reads. <code>aln</code> is the recommended for this, but this is here in case     you would like to select <code>mem</code> instead. Uncollapsed historical reads     will be mapped with <code>mem</code> if <code>historical_only_collapsed</code> is set to     <code>false</code>, regardless of what is put here. (<code>aln</code>/<code>mem</code>)</li> <li>Filter out sites with low 'pileup mappability', which     describes how uniquely fragments of a given size can map to the reference     (<code>true</code>/<code>false</code>)</li> <li>(NOTE: Only one of the three options should be filled/true)</li> <li>Supply a path to a bed file that contains regions with repeats.     This is for those who want to filter out repetitive content, but don't     need to run Repeatmodeler or masker in the workflow because it has     already been done for the genome you're using. Be sure the contig names     in the bed file match those in the reference supplied. GFF or other     filetypes that work with <code>bedtools subtract</code> may also work, but haven't     been tested.</li> <li>Filter repeats by supplying a repeat library you have locally     (such as ones downloaded for Darwin Tree of Life genomes) and using it to     identify repeats with RepeatMasker. Should be file path, not a URL.</li> <li>Use RepeatModeler to build a library of repeats from the     reference itself, then identify them with RepeatMasker and filter them out     (<code>true</code>/<code>false</code>).</li> <li>Filter out sites with extremely high or low global     sequencing depth. Set the parameters for this filtering in the <code>params</code>     section of the yaml. (<code>true</code>/<code>false</code>)</li> <li>A floating point value between 0 and 1. Sites with     data for fewer than this proportion of individuals across the whole dataset     will be filtered out in all analyses using the filtered sites file. (This is     only needed if you need to ensure all your analyses are using exactly the     same sites, which I find may result in coverage biases in results,     especially heterozygosity. Unless you explicitly need to ensure all groups     and analyses use the same sites, I would leave this blank, instead using     the [<code>params</code>][<code>angsd</code>][<code>minind_dataset</code>] to set a minimum individual     threshold for dataset level analyses, allowing analyses to maximize sites     per group/sample. This is how most papers do it.)</li> <li>A floating point value between 0 and 1. Sites     with data for fewer than this proportion of individuals in any population     will be filtered out in all populations using the filtered sites file.     (This is only needed if you need to ensure all your populations are using     exactly the same sites, which I find may result in coverage biases in     results, especially heterozygosity. Unless you explicitly need to ensure all     groups and analyses use the same sites, I would leave this blank, instead     using the [<code>params</code>][<code>angsd</code>][<code>minind_pop</code>] to set a minimum individual     threshold for each analyses, allowing analyses to maximize sites per     group/sample. This is how most papers do it.)</li> <li>Perform Qualimap bamqc on bam files for general quality stats     (<code>true</code>/<code>false</code>)</li> <li>Enable reference bias calculation. For each sample, one read     is randomly sampled at each position and compared to the reference base.     These are summarized as the proportion of the genome that is identical by     state to the reference for each sample to quantify reference bias. This is     done for all filter sets as well as for all sites without site filtering.     If transition removal or other arguments are passed to ANGSD, they are     included here. (<code>true</code>/<code>false</code>)</li> <li>Estimate post-mortem DNA damage on historical samples     with Damageprofiler (<code>true</code>/<code>false</code>)</li> <li>Rescale base quality scores using MapDamage2 to help     account for post-mortem damage in analyses (if you only want to assess     damage, use damageprofiler instead, they return the same results)     (<code>true</code>/<code>false</code>) [docs]</li> <li>Estimate pairwise linkage disquilibrium between sites with     ngsLD for each popualation and the whole dataset. Note, only set this if     you want to generate the LD estimates for use in downstream analyses     outside this workflow. Other analyses within this workflow that require LD     estimates (LD decay/pruning) will function properly regardless of the     setting here. (<code>true</code>/<code>false</code>) [docs]</li> <li>Use ngsLD to plot LD decay curves for each population and for     the dataset as a whole (<code>true</code>/<code>false</code>)     [docs]</li> <li>Perform Principal Component Analysis with PCAngsd. Currently     requires at least 4 samples to finish, as it will by default try to plot     PCs1-4. (<code>true</code>/<code>false</code>)     [docs]</li> <li>Perform admixture analysis with NGSadmix (<code>true</code>/<code>false</code>)     [docs]</li> <li>Relatedness is estimated using two methods: IBSrelate and     NgsRelate v2. IBSrelate does not require allele frequencies, which is     useful if you do not have sufficient sample size to confidently estimate     allele frequencies for your populations. In this pipeline, it is can be run     three ways: using the (1) IBS and (2) SFS based methods described in Waples     et al. 2019, Mol. Ecol. using ANGSD or (3) using the SFS based method's     implementation in NgsRelate v2 (which still does not require allele     frequencies). NgsRelate v2 also offers an allele frequency based method,     which enables co-inference of inbreeding and relatedness coefficients. If     using this method, PopGLen will calculate the allele frequencies for your     populations and input them into NgsRelate. These different methods have     trade-offs in memory usage and run time in addition to the methodological     effects on the estimates themselves. Generally, I recommend starting with     NgsRelate, using IBSrelate only (<code>ngsrelate_ibsrelate-only</code>), adding the     other approaches as you need them.</li> <li>Estimate pairwise relatedness with the IBS based method     from. This can use a lot of memory, as it has genotype likelihoods for all     sites from all samples loaded into memory, so it is done per 'chunk',     which still takes a lot of time and memory. NOTE: For those removing     transitions, this method does not include transition removal. All other     relatedness methods here do. (<code>true</code>/<code>false</code>)     [docs]</li> <li>Estimate pairwise relatedness with the SFS based method.     Enabling this can greatly increase the time needed to build the workflow     DAG if you have many samples. As a form of this method is implemented in     NGSrelate, it may be more efficient to only enable that. (<code>true</code>/<code>false</code>)     [docs]</li> <li>Performs the IBSrelate SFS method, but on SNPs     only using NgsRelate. Does not need to estimate allele frequencies.     (<code>true</code>/<code>false</code>) [docs]</li> <li>Performs the allele frequency based co-inference of     relatedness and inbreeding that NgsRelate is primarly intended for. Will     estimate allele frequencies per population and use them in the analysis.     Also runs the IBSrelate SFS method in NgsRelate. (<code>true</code>/<code>false</code>)     [docs]</li> <li>Generates a one dimensional site frequency spectrum for all     populations in the sample list. Automatically enabled if <code>thetas_angsd</code> is     enabled. (<code>true</code>/<code>false</code>)     [docs]</li> <li>Generates N bootstrap replicates of the 1D site frequency     spectrum for each population. N is determined from the <code>sfsboot</code> setting     below (<code>true</code>/<code>false</code>)     [docs]</li> <li>Generates a two dimensional site frequency spectrum for all unique     populations pairings in the sample list. Automatically enabled if     <code>fst_angsd</code> is enabled. (<code>true</code>/<code>false</code>)     [docs]</li> <li>Generates N bootstrap replicates of the 2D site frequency     spectrum for each population pair. N is determined from the <code>sfsboot</code>     setting below (<code>true</code>/<code>false</code>)     [docs]</li> <li>Estimate nucleotide diversity (pi), Watterson's theta, and     Tajima's D for each population in windows across the genome using ANGSD. If     polarized to an ancestral reference, additional measures of theta and     neutrality statistics in the output will be relevant (see 'Unknown ancestral     state (folded sfs)' in the ANGSD docs for more details on this)     (<code>true</code>/<code>false</code>)     [docs]</li> <li>Estimate individual genome-wide heterozygosity     using ANGSD. Calculates confidence intervals from bootstraps.     (<code>true</code>/<code>false</code>)     [docs]</li> <li>Estimate pairwise F<sub>ST</sub> using ANGSD. Set one or both of the     below options. Estimates both globally and in windows across the genome.</li> <li>Pairwise F<sub>ST</sub> is calculated between all possible     population pairs (<code>true</code>/<code>false</code>)     [docs]</li> <li>Pairwise F<sub>ST</sub> is calculated between all possible     individual pairs. NOTE: This can be really intensive on the DAG building     process, so I don't recommend enabling unless you're certain you want     this (<code>true</code>/<code>false</code>) [docs]</li> <li>Estimates inbreeding coefficients and runs of     homozygosity using ngsF-HMM. Inbreeding coefficients are output in both the     model based form from ngsF-HMM, as well as converted into F<sub>RoH</sub>, which     describes the proportion of the genome in runs of homozygosity over a     certain length. (<code>true</code>/<code>false</code>)     [docs]</li> <li>Estimate pairwise identity by state distance between all     samples using ANGSD. (<code>true</code>/<code>false</code>)     [docs]</li> <li>Estimates population-specific minor allele frequencies     for each population in the dataset using ANGSD. Two outputs are generated     per population: (1) population-specific minor allele frequencies, where only     sites variable in the population are included and the minor allele is set     to the minor of the population, and (2) dataset-wide minor allele     frequencies, where the minor allele is set to the minor of the entire     dataset and includes sites that are fixed within the population if they are     variable in the dataset. Alternatively, major alleles can be polarized to     the reference or ancestral state in both outputs if <code>domajorminor</code> is set to     <code>4</code> or <code>5</code>. [docs]</li> <li>A list of mean depths to subsample your reads to. This will be     done per sample, and subsample from all the reads. Leaving the list empty     disables subsampling. The list can contain multiple depths to subsample to.     If a sample already has the same, or lower, depth than this number, it will     just be used as is in the analysis. (List <code>[]</code> of integers)</li> <li>This determines how the 'full' sequencing depth of a sample     is calculated to determine the amount of subsampling needed to reach the     target depth. This should be one of three options: (1) <code>\"unfilt\"</code> will treat     the sequencing depth of all (unfiltered) reads and sites as the 'full'     depth; (2) <code>\"mapqbaseq\"</code> will filter out reads that don't pass the     configured mapping or base quality, then calculate depth across all sites as     the 'full' depth, (3) <code>\"sitefilt\"</code> will filter reads just as <code>\"mapqbaseq\"</code>     does, but will only calculate the 'full' depth from sites passing the sites     filter. As the main goal of subsampling is to make depth uniform for     analyses, this latter option is preferred, as it will most accurately bring     the depth of the samples to the target depth for analyses.     (<code>\"unfilt\"</code>/<code>\"mapqbaseq\"</code>/<code>\"sitefilt\"</code>)</li> <li>If <code>subsample_by</code> is set to <code>\"unfilt\"</code> or <code>\"mapqbaseq\"</code>,     then it would be possible to recaculate extreme depth filters for the     subsampled dataset. Enable this to do so, otherwise, the depth filters from     the full depth bams will be used. If <code>subsample_by</code> is set to <code>\"sitefilt\"</code>     this will have no effect, as the subsampling is already in reference to a     set site list. (<code>true</code>/<code>false</code>)</li> <li>When performing depth subsampling, you may want to leave some     samples out that you kept in your 'full' dataset. These can be listed here     and they will be removed from ALL depth subsampled analyses. A use case for     this might be if you have a couple samples that are below your targeted     subsample depth, and you don't want to include them. Note that if you     configure multiple <code>subsample_dp</code>, these samples will be dropped from all of     them. If you need to perform mutliple depth subsamplings with different     subsets of samples, its best to run each depth individually. Alternatively,     a config file can be made for each subsampled depth, however you may run     into issues of file locking blocking both from running at the same time.     (list of strings: <code>[]</code>)</li> <li>Individually enable analyses to be performed with the     subsampled data. These have the same function as described in the     [<code>analyses</code>]section above. Enabling here     will only run the analysis for the subsampled data, if you want to run it     for the full data as well, you need to enable it in the analyses section as     well. (<code>true</code>/<code>false</code>)</li> <li> <p>By default, this workflow will perform all analyses requested in the above     section on all sites that pass the filters set in the above section. These     outputs will contain <code>allsites-filts</code> in the filename and in the report.     However, many times, it is useful to perform an analysis on different     subsets of sites, for instance, to compare results for genic vs. intergenic     regions, neutral sites, exons vs. introns, etc. Here, users can set an     arbitrary number of additional filters using BED files. For each BED file     supplied, the contents will be intersected with the sites passing the     filters set in the above section, and all analyses will be performed     additionally using those sites.</p> <p>For instance, given a BED file containing putatively neutral sites, one could set the following:</p> <pre><code>filter_beds:\n  neutral: \"resources/neutral_sites.bed\"\n</code></pre> <p>In this case, for each requested analysis, in addition to the <code>allsites-filts</code> output, a <code>neutral-filts</code> (named after the key assigned to the BED file in <code>config.yaml</code>) output will also be generated, containing the results for sites within the specified BED file that passed any set filters. More than one BED file can be set, up to an arbitrary number:</p> <pre><code>filter_beds:\n  neutral: \"resources/neutral_sites.bed\"\n  intergenic: \"resources/intergenic_sites.bed\"\n  introns: \"resources/introns.bed\"\n</code></pre> </li> <li> <p>It may also sometimes be desireable to skip analyses on <code>allsites-filts</code>,     say if you are trying to only generate diversity estimates or generate SFS     for a set of neutral sites you supply.</p> <p>To skip running any analyses for <code>allsites-filts</code> and only perform them for the BED files you supply, you can set <code>only_filter_beds: true</code> in the config file. This may also be useful in the event you have a set of already filtered sites, and want to run the workflow on those, ignoring any of the built in filter options by setting them to <code>false</code>.</p> </li> <li> <p>Phred-scaled mapping quality filter. Reads below this threshold will     be filtered out. (integer)</p> </li> <li>Phred-scaled base quality filter. Reads below this threshold will be     filtered out. (integer)</li> <li>Determines whether overlapping read pairs will     be clipped in BAM files supplied by users. This is useful as many variant     callers will account for overlapping reads in their processing, but ANGSD     will double count overlapping reads. If BAMs were prepped without this in     mind, it can be good to apply before running through ANGSD. However, it     essentially creates a BAM file of nearly equal size for every sample, so     it may be nice to leave it off by applying it to the BAMs you feed into     the pipeline. (<code>true</code>/<code>false</code>)</li> <li>Additional options to pass to fastp trimming. (string)     [docs]</li> <li>Minimum overlap to collapse historical reads. Default     in fastp is 30. This effectively overrides the <code>--length_required</code> option     if it is larger than that. (INT) [docs]</li> <li>Additional options to pass to bwa aln for mapping of historical     sample reads. (string) [docs]</li> <li>Additional options to pass to Picard MarkDuplicates.     <code>--REMOVE_DUPLICATES true</code> is recommended. (string)     [docs]</li> <li>Enable to run damage profiler on modern samples in     addition to historical (<code>true</code>/<code>false</code>)</li> <li>Parameters for pileup mappability analysis, see     GenMap's documentation for more     details.</li> <li>Length of k-mers to calculate mappability for. (integer)</li> <li>Allowed mismatches per k-mer. (integer)</li> <li>A threshold mappability score. Each site gets an average     mappability score taken by averaging the mappability of all K-mers that     would overlap it. A score of 1 means all K-mers are uniquely mappable,     allowing for <code>E</code> mismatches. This is done via a custom script, and may     eventually be replaced by the SNPable method, which is more common.     (float, 0-1)</li> <li>Parameters for excluding sites based on extreme high     and/or low global depth. The final sites list will contain only sites that     pass the filters for all categories requested (i.e the whole dataset     and/or the depth categories set in <code>samples.tsv</code>).</li> <li>Whether you will generate extreme thresholds as a multiple of     the median global depth (<code>\"median\"</code>) or as percentiles of the     global depth distribution (<code>\"percentile\"</code>)</li> <li>The bounds of the depth cutoff, defined as a numeric list. For     the median method, the values will be multiplied by the median of the     distribution to set the thresholds (i.e. <code>[0.5,1.5]</code> would generate     a lower threshold at 0.5*median and an upper at 1.5*median). For the     percentile method, these define the lower and upper percentiles to filter     out (i.e [0.01,0.99] would remove the lower and upper 1% of the depth     distributions). (<code>[FLOAT, FLOAT]</code>)</li> <li>Whether to perform this filter on the dataset as a     whole (may want to set to false if your dataset global depth distribution     is multi-modal). (<code>true</code>/<code>false</code>)</li> <li>Whether to perform this filter on the depth     classes defined in the <code>samples.tsv</code> file. This will generate a global     depth distribution for samples in the same category, and perform the     filtering on these distributions independently. Then, the sites that pass     for all the classes will be included. (<code>true</code>/<code>false</code>)</li> <li>Seed to use when subsampling bams to lower depth.     <code>\"$RANDOM\"</code> can be used to set a random seed, or any integer can be used     to set a consistent seed. (string or int)     [docs]</li> <li>General options in ANGSD, relevant doc pages are linked</li> <li>Genotype likelihood model to use in calculation     (<code>-GL</code> option in ANGSD, [docs])</li> <li>When calculating individual depth, sites with depth higher     than this will be binned to this value. Should be fine for most to leave     at <code>100</code>. (integer, [docs])</li> <li>Individuals with sequencing depth below this value at a     position will be treated as having no data at that position by ANGSD.     ANGSD defaults to 1 for this. Note that this can be separately set for     individual heterozygosity estimates with <code>mindepthind_heterozygosity</code>     below. (integer, <code>-setMinDepthInd</code> option in ANGSD (INT)     [docs])</li> <li>Used to fill the <code>-minInd</code> option for any dataset wide     ANGSD outputs (like Beagles for PCA/Admix). Should be a floating point     value between 0 and 1 describing what proportion of the dataset must have     data at a site to include it in the output. (FLOAT)     [docs])</li> <li>Used to fill the <code>-minInd</code> option for any population level     ANGSD outputs (like SAFs or Beagles for ngsF-HMM). Should be a floating     point value between 0 and 1 describing what proportion of the population     must have data at a site to include it in the output. (FLOAT)     [docs])</li> <li>Removes transitions using ANGSD, effectively removing them     from downstream analyses. This is useful for removing DNA damage from     analyses, and will automatically set the appropriate ANGSD flags (i.e.     using     <code>-noTrans 1</code>     for SAF files and     <code>-rmTrans 1</code> for Beagle     files.) (<code>true</code>/<code>false</code>)</li> <li>Additional options to pass to ANGSD during genotype likelihood     calculation at all times. This is primarily useful for adding BAM input     filters. Note that <code>--remove_bads</code> and <code>-only_proper_pairs</code> are enabled     by default, so they only need to be included if you want to turn them     off or explicitly show they are enabled. I've also found that for some     datasets, <code>-C 50</code> and <code>-baq 1</code> can create a strong relationship between     sample depth and detected diversity, effectively removing the benefits of     ANGSD for low/variable depth data. I recommend that these aren't included     unless you know you need them. Since the workflow uses BWA to map,     <code>-uniqueOnly 1</code> doesn't do anything if your minimum mapping quality is     &gt; 0, but you may wish to add it if you're bringing in your own files from     another mapper. Mapping and base quality thresholds are also not needed,     it will use the ones defined above automatically. If you prefer to correct     for historical damage by trimming the ends of reads, this is where you'd     want to put <code>-trim INT</code>. (string)     (string, [docs])</li> <li>Same as <code>extra</code>, but only used when making SAF files (used     for SFS, thetas, F<sub>ST</sub>, IBSrelate (not NgsRelate version),     heterozygosity). Doesn't require options already in <code>extra</code> or defined via     other params in the YAML (such as <code>notrans</code>, <code>minind</code>, <code>GL</code>, etc.)     (string)</li> <li>Same as <code>extra</code>, but only used when making Beagle and Maf     files (used for PCA, Admix, ngsF-HMM, doIBS, NgsRelate, allele     frequencies). Doesn't require options already in <code>extra</code> or defined via     other params in the YAML (such as <code>rmtrans</code>, <code>minind</code>, <code>GL</code>, etc.)     (string)</li> <li>Method for inferring the major and minor alleles. Set to     1 to infer from the genotype likelihoods, see     documentation     for other options. <code>1</code>, <code>2</code>, and <code>4</code> can be set without any additional     configuration. <code>5</code> must also have an ancestral reference provided in the     config, otherwise it will be the same as <code>4</code>. <code>3</code> is currently not     possible, and is used for generating the dataset polarized MAFs. If you     have a use for <code>3</code> that PopGLen is suited for, please open an issue and I     will look into it. (integer)</li> <li>The p-value to use for calling SNPs (float or string)     [docs]</li> <li>Method for inferring minor allele frequencies. Set to <code>1</code> to     infer from genotype likelihoods using a known major and minor from the     <code>domajorminor</code> setting above. Set to <code>2</code> to assume the minor is unknown.     See [docs] for     more information. <code>3</code> is possible, and will estimate the frequencies both     assuming a known and unknown minor. If you choose this option, you'll get     both in the MAF outputs, but only the known will be passed to NgsRelate if     you also use that. Other values are currently unsupported in PopGLen.     (integer)</li> <li>The minimum minor allele frequency required to call a SNP.     This is set when generating the Beagle file, so will filter SNPs for     PCAngsd, NGSadmix, ngsF-HMM, and NgsRelate. If you would like each tool     to handle filtering for maf on its own you can set this to <code>-1</code>     (disabled). (float, [docs])</li> <li>When estimating individual heterozygosity,     sites with sequencing depth lower than this value will be dropped.     (integer, <code>-setMinDepthInd</code> option in ANGSD) (integer)</li> <li>Settings for ngsLD [docs]</li> <li>For the LD estimates generated when setting     <code>estimate_ld: true</code> above, set the maximum distance between sites in kb     that LD will be estimated for (<code>--max_kb_dist</code> in ngsLD, integer)</li> <li>For the LD estimates generated when setting     <code>estimate_ld: true</code> above, randomly sample this proportion of pairwise     linkage estimates rather than estimating all (<code>--rnd_sample</code> in ngsLD,     float)</li> <li>The same as <code>max_kb_dist_est-ld:</code>, but used when     estimating LD decay when setting <code>ld_decay: true</code> above (integer)</li> <li>The same as <code>rnd_sample_est-ld:</code>, but used when     estimating LD decay when setting <code>ld_decay: true</code> above (float)</li> <li>Additional plotting arguments to pass to     <code>fit_LDdecay.R</code> when estimating LD decay (string)</li> <li>When estimating LD decay, should the sample     size corrected r<sup>2</sup> model be used? (<code>true</code>/<code>false</code>, <code>true</code> is the     equivalent of passing a sample size to <code>fit_LDdecay.R</code> in ngsLD using     <code>--n_ind</code>)</li> <li>The same as <code>max_kb_dist_est-ld:</code>, but     used when linkage pruning SNPs as inputs for PCAngsd and NGSadmix. Pruning     is performed on the whole dataset. Any positions above this distance will     be assumed to be in linkage equilibrium during the pruning process.     (integer)</li> <li>The minimum r<sup>2</sup> to assume two positions are     in linkage disequilibrium when pruning for PCAngsd and NGSadmix analyses.     (float)</li> <li>Settings for NGSrelate</li> <li>Any extra command line parameters to be passed to      NgsRelate when performing an IBSrelate only (no allele frequencies) run.      (string)</li> <li>Any extra command line parameters to be passed to      NgsRelate when performing a standard, allele frequency based, run.      (string)</li> <li>Settings for ngsF-HMM [docs]</li> <li>Set to <code>true</code> to run ngsF-HMM separately for each      population in your dataset. Set to <code>false</code> to run for whole dataset at      once. ngsF-HMM assumes Hardy-Weinberg Equilibrium (aside from inbreeding)      in the input data, so select the option that most reflects this in your      data. (<code>true</code>/<code>false</code>)</li> <li>Whether or not to prune SNPs for LD before running the analysis.      ngsF-HMM assumes independent sites, so it is preferred to set this to      <code>true</code> to satisfy that expectation. (<code>true</code>/<code>false</code>)</li> <li>The maximum distance between sites in kb      that will be treated as in LD when pruning for the ngsF-HMM input. (INT)</li> <li>The minimum r<sup>2</sup> to assume two positions are in      linkage disequilibrium when pruning for the ngsF-HMM input. Note, that      this likely will be substantially higher for individual populations than      for the whole dataset, as background LD should be higher when no      substructure is present. (float)</li> <li>Minimum ROH size in base pairs to include in inbreeding      coefficient calculation. Set if short ROH might be considered low      confidence for your data. (integer)</li> <li>A list of integers that describe the size classes in base      pairs you would like to partition the inbreeding coefficient by. This can      help visualize how much of the coefficient comes from ROH of certain size      classes (and thus, ages). List should be in ascending order and the first      entry should be greater than <code>min_roh_length</code>. The first bin will group      ROH between <code>min_roh_length</code> and the first entry, subsequent bins will      group ROH with sizes between adjacent entries in the list, and the final      bin will group all ROH larger than the final entry in the list. (list <code>[]</code>      of integers)</li> <li>Settings for realSFS</li> <li>Whether or not to fold the produced SFS. Set to 1 if you have not      provided an ancestral-state reference (0 or 1,      [docs])</li> <li>Determines number of bootstrap replicates to use when      requesting bootstrapped SFS. Is used for both 1dsfs and 2dsfs (this is      very easy to separate, open an issue if desired). Automatically used      for heterozygosity analysis to calculate confidence intervals. (integer)</li> <li>Settings for F<sub>ST</sub> calculation in ANGSD</li> <li>Determines which F<sub>ST</sub> estimator is used by ANGSD. With 0      being the default Reynolds 1983 and 1 being the Bhatia-Hudson 2013      estimator. The latter is preferable for small or uneven sample sizes      (0 or 1, [docs])</li> <li>Window size in bp for sliding window analysis (integer)</li> <li>Window step size in bp for sliding window analysis (integer)</li> <li>Settings for pi, theta, and Tajima's D estimation</li> <li>Window size in bp for sliding window analysis (integer)</li> <li>Window step size in bp for sliding window analysis (integer)</li> <li>Minimum sites to include window in report plot. This does not      remove them from the actual output, just the report plot and averages.</li> <li>Settings for admixture analysis with NGSadmix. This analysis is      performed for a set of K groupings, and each K has several replicates      performed. Replicates will continue until a set of N highest likelihood      replicates converge, or the number of replicates reaches an upper threshold      set here. Defaults for <code>reps</code>, <code>minreps</code>, <code>thresh</code>, and <code>conv</code> can be left      as default for most. Based on method described by      Pe\u010dnerov\u00e1 et al. 2021, Curr. Biol.</li> <li>A list of values of K to fit the data to (list <code>[]</code> of      integers)</li> <li>The maximum number of replicates to perform per K. Default is 100.      (integer)</li> <li>The minimum number of replicates to perform, even if      replicates have converged. Default is 20. (integer)</li> <li>The convergence threshold - the top replicates must all be      within this value of log-likelihood units to consider the run converged.      Default is 2. (integer)</li> <li>The number of top replicates to include in convergence      assessment. Default is 3. (integer)</li> <li>Additional arguments to pass to NGSadmix (for instance,      increasing <code>-maxiter</code>). (string,      [docs])</li> <li>Settings for identity by state calculation with ANGSD</li> <li>Whether to use a random (1) or consensus (2) base in IBS      distance calculation      ([docs])</li> </ol>"},{"location":"config/","title":"Configuring the workflow","text":"<p>Running the workflow requires configuring three files: <code>config.yaml</code>, <code>samples.tsv</code>, and <code>units.tsv</code>. <code>config.yaml</code> is used to configure the analyses, <code>samples.tsv</code> categorizes your samples into groups, and <code>units.tsv</code> connects sample names to their input data files. The workflow will use <code>config/config.yaml</code> automatically, but it can be good to name it something informative and point to it when running snakemake with <code>--configfile &lt;path&gt;</code>.</p>"},{"location":"config/#samplestsv","title":"<code>samples.tsv</code>","text":"<p>This file contains your sample list, and has four tab separated columns:</p> config/samples.tsv<pre><code>sample\tpopulation\ttime\tdepth\nMZLU153246\tESkane1956\thistorical\tlow\nMZLU153247\tESkane1956\thistorical\tlow\nMZLU153248\tESkane1956\thistorical\tlow\nMZLU153251\tESkane1956\thistorical\tlow\nMZLU153250\tESkane1956\thistorical\tlow\nMZLU153204\tWSkane1951\thistorical\tlow\nMZLU153205\tWSkane1951\thistorical\tlow\nMZLU153206\tWSkane1951\thistorical\tlow\nMZLU153216\tWSkane1951\thistorical\tlow\nMZLU153221\tWSkane1951\thistorical\tlow\nMZLU107458\tESkane2021\tmodern\thigh\nMZLU107463\tESkane2021\tmodern\thigh\nMZLU107503\tESkane2021\tmodern\thigh\nMZLU107504\tESkane2021\tmodern\thigh\nMZLU107507\tESkane2021\tmodern\thigh\nMZLU107457\tWSkane2021\tmodern\thigh\nMZLU107460\tWSkane2021\tmodern\thigh\nMZLU107497\tWSkane2021\tmodern\thigh\nMZLU107498\tWSkane2021\tmodern\thigh\nMZLU107500\tWSkane2021\tmodern\thigh\n</code></pre> <ul> <li> <p><code>sample</code> contains the ID of a sample. It is best if it only contains   alphanumeric characters.</p> </li> <li> <p><code>population</code> contains the population the sample comes from and will be used   to group samples for population-level analyses.</p> </li> <li> <p><code>time</code> sets whether a sample should be treated as fresh DNA or historical DNA   in the sequence processing workflow. Doesn't change anything if you're   starting with bam files.</p> </li> <li> <p><code>depth</code> puts the sample in a sequencing depth category. Used for filtering -   if enabled in the configuration, extreme depth filters will be performed for   depth categories individually.</p> </li> </ul>"},{"location":"config/#unitstsv","title":"<code>units.tsv</code>","text":"<p>This file connects your samples to input files and has a potential for eight tab separated columns:</p> config/units.tsv<pre><code>sample\tunit\tlib\tplatform\tfq1\tfq2\nMZLU107457\tAHW5NGDSX2.3\tUDP0046\tILLUMINA\tdata/MZLU107457.R1.sub.fastq.gz\tdata/MZLU107457.R2.sub.fastq.gz\nMZLU107458\tAHW5NGDSX2.3\tUDP0047\tILLUMINA\tdata/MZLU107458.R1.sub.fastq.gz\tdata/MZLU107458.R2.sub.fastq.gz\nMZLU107460\tAHW5NGDSX2.3\tUDP0049\tILLUMINA\tdata/MZLU107460.R1.sub.fastq.gz\tdata/MZLU107460.R2.sub.fastq.gz\nMZLU107463\tAHW5NGDSX2.3\tUDP0052\tILLUMINA\tdata/MZLU107463.R1.sub.fastq.gz\tdata/MZLU107463.R2.sub.fastq.gz\nMZLU107497\tAHW5NGDSX2.3\tUDP0084\tILLUMINA\tdata/MZLU107497.R1.sub.fastq.gz\tdata/MZLU107497.R2.sub.fastq.gz\nMZLU107498\tAHW5NGDSX2.3\tUDP0085\tILLUMINA\tdata/MZLU107498.R1.sub.fastq.gz\tdata/MZLU107498.R2.sub.fastq.gz\nMZLU107500\tAHW5NGDSX2.3\tUDP0087\tILLUMINA\tdata/MZLU107500.R1.sub.fastq.gz\tdata/MZLU107500.R2.sub.fastq.gz\nMZLU107503\tAHW5NGDSX2.3\tUDP0090\tILLUMINA\tdata/MZLU107503.R1.sub.fastq.gz\tdata/MZLU107503.R2.sub.fastq.gz\nMZLU107504\tAHW5NGDSX2.3\tUDP0091\tILLUMINA\tdata/MZLU107504.R1.sub.fastq.gz\tdata/MZLU107504.R2.sub.fastq.gz\nMZLU107507\tAHW5NGDSX2.3\tUDP0094\tILLUMINA\tdata/MZLU107507.R1.sub.fastq.gz\tdata/MZLU107507.R2.sub.fastq.gz\nMZLU153221\tBHVN22DSX2.2\t208188\tILLUMINA\tdata/MZLU153221.R1.sub.fastq.gz\tdata/MZLU153221.R2.sub.fastq.gz\nMZLU153206\tBHVN22DSX2.2\t207139\tILLUMINA\tdata/MZLU153206.R1.sub.fastq.gz\tdata/MZLU153206.R2.sub.fastq.gz\nMZLU153204\tBHVN22DSX2.2\t003091\tILLUMINA\tdata/MZLU153204.R1.sub.fastq.gz\tdata/MZLU153204.R2.sub.fastq.gz\nMZLU153205\tBHVN22DSX2.2\t008096\tILLUMINA\tdata/MZLU153205.R1.sub.fastq.gz\tdata/MZLU153205.R2.sub.fastq.gz\nMZLU153248\tBHVN22DSX2.2\t021130\tILLUMINA\tdata/MZLU153248.R1.sub.fastq.gz\tdata/MZLU153248.R2.sub.fastq.gz\nMZLU153216\tBHVN22DSX2.2\t093148\tILLUMINA\tdata/MZLU153216.R1.sub.fastq.gz\tdata/MZLU153216.R2.sub.fastq.gz\nMZLU153251\tBHVN22DSX2.2\t028116\tILLUMINA\tdata/MZLU153251.R1.sub.fastq.gz\tdata/MZLU153251.R2.sub.fastq.gz\nMZLU153250\tBHVN22DSX2.2\t036137\tILLUMINA\tdata/MZLU153250.R1.sub.fastq.gz\tdata/MZLU153250.R2.sub.fastq.gz\nMZLU153247\tBHVN22DSX2.2\t043143\tILLUMINA\tdata/MZLU153247.R1.sub.fastq.gz\tdata/MZLU153247.R2.sub.fastq.gz\nMZLU153246\tBHVN22DSX2.2\t052142\tILLUMINA\tdata/MZLU153246.R1.sub.fastq.gz\tdata/MZLU153246.R2.sub.fastq.gz\n</code></pre> <ul> <li><code>sample</code> contains the ID of a sample. Must be same as in <code>samples.tsv</code> and   may be listed multiple times when inputting multiple sequencing   runs/libraries. Required for all input types.</li> <li><code>unit</code> contains the sequencing unit, i.e. the sequencing lane barcode and   lane number. This is used in the PU and (part of) the ID read groups. If you   don't have multiple sequencing lanes per samples, this won't impact anything.   Only required for FASTQ or SRA input.</li> <li><code>lib</code> contains the name of the library identifier for the entry. Fills in   the LB and (part of) the ID read groups and is used for PCR duplicate removal.   Best practice would be to have the combination of <code>unit</code> and <code>lib</code> to be   unique per line. An easy way to use this is to use the Illumina library   identifier or another unique library identifier, or simply combine a generic   name with the sample name (sample1A, sample1B, etc.). Only required for FASTQ   or SRA input.</li> <li><code>platform</code> is used to fill the PL read group. Commonly is just 'ILLUMINA'.   Only required for FASTQ or SRA input.</li> <li><code>fq1</code> and <code>fq2</code> provides the absolute or relative to the working directory   paths to the raw sequencing files corresponding to the metadata in the   previous columns.</li> </ul> <p>Two additional columns are possible that are not shown above:</p> <ul> <li><code>bam</code> provides the absolute or relative to the working directory path of   pre-processed BAM files. Only one BAM files should be provided per sample in   the units file.</li> <li><code>sra</code> provides the NCBI SRA accession number for a set of paired end fastq   files that will be downloaded to be processed. If a sample has multiple runs   you would like to include, each run should be its own line in the units sheet,   just as separate sequencing runs would be.</li> </ul> <p>A <code>units.tsv</code> where only BAMs are provided might look like this:</p> config/units.tsv<pre><code>sample\tbam\nMZLU107457\tdata/bams/MZLU107457.tutorial-ref.bam\nMZLU107458\tdata/bams/MZLU107458.tutorial-ref.bam\nMZLU107460\tdata/bams/MZLU107460.tutorial-ref.bam\nMZLU107463\tdata/bams/MZLU107463.tutorial-ref.bam\nMZLU107497\tdata/bams/MZLU107497.tutorial-ref.bam\nMZLU107498\tdata/bams/MZLU107498.tutorial-ref.bam\nMZLU107500\tdata/bams/MZLU107500.tutorial-ref.bam\nMZLU107503\tdata/bams/MZLU107503.tutorial-ref.bam\nMZLU107504\tdata/bams/MZLU107504.tutorial-ref.bam\nMZLU107507\tdata/bams/MZLU107507.tutorial-ref.bam\nMZLU153221\tdata/bams/MZLU153221.tutorial-ref.bam\nMZLU153206\tdata/bams/MZLU153206.tutorial-ref.bam\nMZLU153204\tdata/bams/MZLU153204.tutorial-ref.bam\nMZLU153205\tdata/bams/MZLU153205.tutorial-ref.bam\nMZLU153248\tdata/bams/MZLU153248.tutorial-ref.bam\nMZLU153216\tdata/bams/MZLU153216.tutorial-ref.bam\nMZLU153251\tdata/bams/MZLU153251.tutorial-ref.bam\nMZLU153250\tdata/bams/MZLU153250.tutorial-ref.bam\nMZLU153247\tdata/bams/MZLU153247.tutorial-ref.bam\nMZLU153246\tdata/bams/MZLU153246.tutorial-ref.bam\n</code></pre> <p>Mixing samples with different starting points</p> <p>It is possible to have different samples start from different inputs (i.e. some from bam, others from fastq, others from SRA). It is best to provide only <code>fq1</code>+<code>fq2</code>, <code>bam</code>, or <code>sra</code> for a single sample to be clear where that sample starts, leaving the other columns blank. If multiple starts are provided for the same sample, the bam file will override fastq or SRA entries, and the fastq will override SRA entries. Note that this means it is not currently possible to have multiple starting points for the same sample (i.e. FASTQ reads that would be processed then merged into an existing BAM).</p>"},{"location":"config/#configuration-file-configyaml","title":"Configuration file (<code>config.yaml</code>)","text":"<p><code>config.yaml</code> contains the configuration for the workflow, this is where you will put what analyses, filters, and options you want. Below I describe the configuration options. The <code>config.yaml</code> distributed in the workflow repository serves as a template and includes some 'default' parameters that may be good starting points for many users. If <code>--configfile</code> is not specified in the Snakemake command, the workflow will default to the file at <code>config/config.yaml</code>. I've also made a <code>config.yaml</code> 'cheat sheet' which overlays these descriptions onto an example configuration file to better see them laid out.</p>"},{"location":"config/#configuration-options","title":"Configuration options","text":""},{"location":"config/#dataset-configuration","title":"Dataset Configuration","text":"<p>Required configuration of the 'dataset'.</p> <ul> <li><code>samples:</code> An absolute or relative path from the working directory to the   <code>samples.tsv</code> file.</li> <li><code>units:</code> An absolute or relative path from the working directory to the   <code>units.tsv</code> file.</li> <li><code>dataset:</code> A name for this dataset run - an identifier for a batch of samples   to be analysed together with the same configuration.</li> </ul> <p>It is best to name your dataset something descriptive, but concise. This is because this name will be used in organizing the results. Outputs of analyses will be placed in the folder <code>results/{dataset}</code>, and files will be prefaced with the dataset name. This allows for multiple datasets to be run in the same working directory, even in parallel (if they aren't trying to make the same files), which is useful for multi-species projects or for testing out different filters. You can simply have a config for each dataset and choose which one to run with <code>--configfile</code>. A similar approach can be used to try out different analysis parameters. See this repository for an example of a project with configurations for many underlying datasets.</p> Example use of multiple datasets <p>Say you want to run PopGLen on two sets of samples, but use the same reference. You can have two sample lists: <code>dataset1_samples.tsv</code> and <code>dataset2_samples.tsv</code>, and two config files: <code>dataset1_config.tsv</code> and <code>dataset2_config.yaml</code>. In the configs, the <code>samples:</code> option should point to the corresponding sample list. The workflow for dataset1 can be run, if you pass <code>--configfile config/dataset1_config.yaml</code> to Snakemake, then the same can be done for dataset2. However, when dataset2 is run, it will use any outputs from dataset1 it can, such as reference indices, reference filters, etc. Additionally, if the two datasets share samples, those samples will not have to be remapped for dataset2, they'll be taken from the dataset1 run. The actual analyses are partitioned by dataset, into the folders <code>results/dataset1</code> and <code>results/dataset2</code>.</p>"},{"location":"config/#reference-configuration","title":"Reference Configuration","text":"<p>Required configuration of the reference.</p> <ul> <li> <p><code>chunk_size:</code> A size in bp (integer). Your reference will be analyzed in   'chunks' of contigs of this size to parallelize processing. This size should   be larger than the largest contig in your genome. A larger number means fewer   jobs that run longer. A smaller number means more jobs that run shorter. The   best fit will depend on the reference and the compute resources you have   available. Leaving this blank will not divide the reference up into chunks.</p> </li> <li> <p><code>reference:</code></p> </li> <li> <p><code>name:</code> A name for your reference genome, will go in the file names and be     used to organize the reference filter outputs.</p> </li> <li><code>fasta:</code> A path to the reference fasta file (currently only supports     uncompressed fasta files).</li> <li><code>mito:</code> Mitochondrial contig name(s), will be removed from analysis. Should     be listed as a list of strings within brackets []</li> <li><code>sex-linked:</code> Sex-linked contig name(s), will be removed from analysis.     Should be listed as a list of strings within brackets []</li> <li><code>exclude:</code> Additional contig name(s) to exclude from analysis. Should be     listed as a list of strings within brackets []</li> <li> <p><code>min_size:</code> A size in bp (integer). All contigs below this size will be     excluded from analysis.</p> </li> <li> <p><code>ancestral:</code> A path to a fasta file containing the ancestral states in your   reference genome. This is optional, and is used to polarize allele   frequencies in SAF files to ancestral/derived. If you leave this empty,   the reference genome itself will be used as ancestral, and you should be   sure the [<code>params</code>] [<code>realSFS</code>] [<code>fold</code>] is set to <code>1</code>. If you put a fasta   here, you can set that to <code>0</code>.</p> </li> </ul> <p>Reference genomes should be uncompressed, and contig names should be clear and concise. Alphanumeric characters, as well as <code>.</code> in contig names have been tested to work so far, other symbols have not been tested thoroughly. If you find issues with certain characters, please report them as a bug in the pipeline.</p> <p>Potentially the ability to use bgzipped genomes will be added, I just need to check that it works with all underlying tools. Currently, it will for sure not work, as calculating chunks is hard-coded to work on an uncompressed genome.</p>"},{"location":"config/#sample-set-configuration","title":"Sample Set Configuration","text":"<ul> <li><code>exclude_ind:</code> Sample name(s) that will be excluded from the workflow. Should   be a list in <code>[]</code>. As an alternative, putting a <code>#</code> in front of the sample in   the sample list also will remove it. Mainly used to drop samples with poor   quality after initial processing.</li> <li><code>excl_pca-admix:</code> Sample name(s) that will be excluded only from PCA and   Admixture analyses. Useful for close relatives that violate the assumptions   of these analyses, but that you want in others. Should be a list in <code>[]</code>. If   you   want relatives out of all downstream analyses, not just PCA/Admix, put   them in <code>exclude_ind</code> instead. Note this will trigger a re-run for relatedness   analyses, but you can just disable them now as they've already been run.</li> </ul>"},{"location":"config/#analysis-selection","title":"Analysis Selection","text":"<p>Here, you will define which analyses you will perform. It is useful to start with only a few, and add more in subsequent workflow runs, just to ensure you catch errors before you use compute time running all analyses. Most are set with (<code>true</code>/<code>false</code>) or a value, described below. Modifications to the settings for each analysis are set in the Software Configuration section.</p> <ul> <li> <p><code>populations:</code> A list of populations found in your sample list to limit   population analyses to. Might be useful if you want to perform individual   analyses on some samples, but not include them in any population level   analyses. Leave blank (<code>[]</code>) if you want population level analyses on all the   populations defined in your <code>samples.tsv</code> file.</p> </li> <li> <p><code>analyses:</code></p> </li> <li><code>mapping:</code><ul> <li><code>historical_only_collapsed:</code> Historical samples are expected to have   fragmented DNA. For this reason, overlapping (i.e. shorter, usually   &lt;270bp) read pairs are collapsed in this workflow for historical samples.   Setting this option to <code>true</code> will only map only these collapsed reads,   and is recommended to target primarily endogenous content. However, in   the event you want to map both the collapsed and uncollapsed reads, you   can set this to <code>false</code>. (<code>true</code>/<code>false</code>)</li> <li><code>historical_collapsed_aligner:</code> Aligner used to map collapsed historical   sample reads. <code>aln</code> is the recommended for this, but this is here in case   you would like to select <code>mem</code> instead. Uncollapsed historical reads   will be mapped with <code>mem</code> if <code>historical_only_collapsed</code> is set to   <code>false</code>, regardless of what is put here. (<code>aln</code>/<code>mem</code>)</li> </ul> </li> <li><code>pileup-mappability:</code> Filter out sites with low 'pileup mappability', which     describes how uniquely fragments of a given size can map to the reference     (<code>true</code>/<code>false</code>)</li> <li><code>repeatmasker:</code> (NOTE: Only one of the three options should be filled/true)<ul> <li><code>bed:</code> Supply a path to a bed file that contains regions with repeats.   This is for those who want to filter out repetitive content, but don't   need to run Repeatmodeler or masker in the workflow because it has   already been done for the genome you're using. Be sure the contig names   in the bed file match those in the reference supplied. GFF or other   filetypes that work with <code>bedtools subtract</code> may also work, but haven't   been tested.</li> <li><code>local_lib:</code> Filter repeats by supplying a repeat library you have locally   (such as ones downloaded for Darwin Tree of Life genomes) and using it to   identify repeats with RepeatMasker. Should be file path, not a URL.</li> <li><code>build_lib:</code> Use RepeatModeler to build a library of repeats from the   reference itself, then identify them with RepeatMasker and filter them out   (<code>true</code>/<code>false</code>).</li> </ul> </li> <li><code>extreme_depth:</code> Filter out sites with extremely high or low global     sequencing depth. Set the parameters for this filtering in the <code>params</code>     section of the yaml. (<code>true</code>/<code>false</code>)</li> <li><code>dataset_missing_data:</code> A floating point value between 0 and 1. Sites with     data for fewer than this proportion of individuals across the whole dataset     will be filtered out in all analyses using the filtered sites file. (This is     only needed if you need to ensure all your analyses are using exactly the     same sites, which I find may result in coverage biases in results,     especially heterozygosity. Unless you explicitly need to ensure all groups     and analyses use the same sites, I would leave this blank, instead using     the [<code>params</code>][<code>angsd</code>][<code>minind_dataset</code>] to set a minimum individual     threshold for dataset level analyses, allowing analyses to maximize sites     per group/sample. This is how most papers do it.)</li> <li><code>population_missing_data:</code> A floating point value between 0 and 1. Sites     with data for fewer than this proportion of individuals in any population     will be filtered out in all populations using the filtered sites file.     (This is only needed if you need to ensure all your populations are using     exactly the same sites, which I find may result in coverage biases in     results, especially heterozygosity. Unless you explicitly need to ensure all     groups and analyses use the same sites, I would leave this blank, instead     using the [<code>params</code>][<code>angsd</code>][<code>minind_pop</code>] to set a minimum individual     threshold for each analyses, allowing analyses to maximize sites per     group/sample. This is how most papers do it.)</li> <li><code>qualimap:</code> Perform Qualimap bamqc on bam files for general quality stats     (<code>true</code>/<code>false</code>)</li> <li><code>ibs_ref_bias:</code> Enable reference bias calculation. For each sample, one read     is randomly sampled at each position and compared to the reference base.     These are summarized as the proportion of the genome that is identical by     state to the reference for each sample to quantify reference bias. This is     done for all filter sets as well as for all sites without site filtering.     If transition removal or other arguments are passed to ANGSD, they are     included here. (<code>true</code>/<code>false</code>)</li> <li><code>damageprofiler:</code> Estimate post-mortem DNA damage on historical samples     with Damageprofiler (<code>true</code>/<code>false</code>)</li> <li><code>mapdamage_rescale:</code> Rescale base quality scores using MapDamage2 to help     account for post-mortem damage in analyses (if you only want to assess     damage, use damageprofiler instead, they return the same results)     (<code>true</code>/<code>false</code>) [docs]</li> <li><code>estimate_ld:</code> Estimate pairwise linkage disquilibrium between sites with     ngsLD for each popualation and the whole dataset. Note, only set this if     you want to generate the LD estimates for use in downstream analyses     outside this workflow. Other analyses within this workflow that require LD     estimates (LD decay/pruning) will function properly regardless of the     setting here. (<code>true</code>/<code>false</code>) [docs]</li> <li><code>ld_decay:</code> Use ngsLD to plot LD decay curves for each population and for     the dataset as a whole (<code>true</code>/<code>false</code>)     [docs]</li> <li><code>pca_pcangsd:</code> Perform Principal Component Analysis with PCAngsd. Currently     requires at least 4 samples to finish, as it will by default try to plot     PCs1-4. (<code>true</code>/<code>false</code>)     [docs]</li> <li><code>admix_ngsadmix:</code> Perform admixture analysis with NGSadmix (<code>true</code>/<code>false</code>)     [docs]</li> <li><code>relatedness:</code> Relatedness is estimated using two methods: IBSrelate and     NgsRelate v2. IBSrelate does not require allele frequencies, which is     useful if you do not have sufficient sample size to confidently estimate     allele frequencies for your populations. In this pipeline, it is can be run     three ways: using the (1) IBS and (2) SFS based methods described in Waples     et al. 2019, Mol. Ecol. using ANGSD or (3) using the SFS based method's     implementation in NgsRelate v2 (which still does not require allele     frequencies). NgsRelate v2 also offers an allele frequency based method,     which enables co-inference of inbreeding and relatedness coefficients. If     using this method, PopGLen will calculate the allele frequencies for your     populations and input them into NgsRelate. These different methods have     trade-offs in memory usage and run time in addition to the methodological     effects on the estimates themselves. Generally, I recommend starting with     NgsRelate, using IBSrelate only (<code>ngsrelate_ibsrelate-only</code>), adding the     other approaches as you need them.<ul> <li><code>ibsrelate_ibs:</code> Estimate pairwise relatedness with the IBS based method   from. This can use a lot of memory, as it has genotype likelihoods for all   sites from all samples loaded into memory, so it is done per 'chunk',   which still takes a lot of time and memory. NOTE: For those removing   transitions, this method does not include transition removal. All other   relatedness methods here do. (<code>true</code>/<code>false</code>)   [docs]</li> <li><code>ibsrelate_sfs:</code> Estimate pairwise relatedness with the SFS based method.   Enabling this can greatly increase the time needed to build the workflow   DAG if you have many samples. As a form of this method is implemented in   NGSrelate, it may be more efficient to only enable that. (<code>true</code>/<code>false</code>)   [docs]</li> <li><code>ngsrelate_ibsrelate-only:</code> Performs the IBSrelate SFS method, but on SNPs   only using NgsRelate. Does not need to estimate allele frequencies.   (<code>true</code>/<code>false</code>) [docs]</li> <li><code>ngsrelate_freqbased:</code> Performs the allele frequency based co-inference of   relatedness and inbreeding that NgsRelate is primarly intended for. Will   estimate allele frequencies per population and use them in the analysis.   Also runs the IBSrelate SFS method in NgsRelate. (<code>true</code>/<code>false</code>)   [docs]</li> </ul> </li> <li><code>1dsfs:</code> Generates a one dimensional site frequency spectrum for all     populations in the sample list. Automatically enabled if <code>thetas_angsd</code> is     enabled. (<code>true</code>/<code>false</code>)     [docs]</li> <li><code>1dsfs_boot:</code> Generates N bootstrap replicates of the 1D site frequency     spectrum for each population. N is determined from the <code>sfsboot</code> setting     below (<code>true</code>/<code>false</code>)     [docs]</li> <li><code>2dsfs:</code> Generates a two dimensional site frequency spectrum for all unique     populations pairings in the sample list. Automatically enabled if     <code>fst_angsd</code> is enabled. (<code>true</code>/<code>false</code>)     [docs]</li> <li><code>2dsfs_boot:</code> Generates N bootstrap replicates of the 2D site frequency     spectrum for each population pair. N is determined from the <code>sfsboot</code>     setting below (<code>true</code>/<code>false</code>)     [docs]</li> <li><code>thetas_angsd:</code> Estimate nucleotide diversity (pi), Watterson's theta, and     Tajima's D for each population in windows across the genome using ANGSD. If     polarized to an ancestral reference, additional measures of theta and     neutrality statistics in the output will be relevant (see 'Unknown ancestral     state (folded sfs)' in the ANGSD docs for more details on this)     (<code>true</code>/<code>false</code>)     [docs]</li> <li><code>heterozygosity_angsd:</code> Estimate individual genome-wide heterozygosity     using ANGSD. Calculates confidence intervals from bootstraps.     (<code>true</code>/<code>false</code>)     [docs]</li> <li><code>fst_angsd:</code> Estimate pairwise F<sub>ST</sub> using ANGSD. Set one or both of the     below options. Estimates both globally and in windows across the genome.<ul> <li><code>populations:</code> Pairwise F<sub>ST</sub> is calculated between all possible   population pairs (<code>true</code>/<code>false</code>)   [docs]</li> <li><code>individuals:</code> Pairwise F<sub>ST</sub> is calculated between all possible   individual pairs. NOTE: This can be really intensive on the DAG building   process, so I don't recommend enabling unless you're certain you want   this (<code>true</code>/<code>false</code>) [docs]</li> </ul> </li> <li><code>inbreeding_ngsf-hmm:</code> Estimates inbreeding coefficients and runs of     homozygosity using ngsF-HMM. Inbreeding coefficients are output in both the     model based form from ngsF-HMM, as well as converted into F<sub>RoH</sub>, which     describes the proportion of the genome in runs of homozygosity over a     certain length. (<code>true</code>/<code>false</code>)     [docs]</li> <li><code>ibs_matrix:</code> Estimate pairwise identity by state distance between all     samples using ANGSD. (<code>true</code>/<code>false</code>)     [docs]</li> <li><code>pop_allele_freqs:</code> Estimates population-specific minor allele frequencies     for each population in the dataset using ANGSD. Two outputs are generated     per population: (1) population-specific minor allele frequencies, where only     sites variable in the population are included and the minor allele is set     to the minor of the population, and (2) dataset-wide minor allele     frequencies, where the minor allele is set to the minor of the entire     dataset and includes sites that are fixed within the population if they are     variable in the dataset. Alternatively, major alleles can be polarized to     the reference or ancestral state in both outputs if <code>domajorminor</code> is set to     <code>4</code> or <code>5</code>. [docs]</li> </ul>"},{"location":"config/#subsampling-section","title":"Subsampling Section","text":"<p>As this workflow is aimed at low coverage samples, its likely there might be considerable variance in sequencing depth. For this reason, it may be good to subsample all your samples to a similar depth to examine if variation in depth is influencing results. To do this, set an integer value here to subsample all your samples down to and run specific analyses. This subsampling can be done in reference to the unfiltered sequencing depth, the mapping and base quality filtered sequencing depth, or the filtered sites sequencing depth. The latter is recommended, as this will ensure that sequencing depth is made uniform at the analysis stage, as it is these filtered sites that analyses are performed for.</p> <ul> <li><code>subsample_dp:</code> A list of mean depths to subsample your reads to. This will be   done per sample, and subsample from all the reads. Leaving the list empty   disables subsampling. The list can contain multiple depths to subsample to. If   a sample already has the same, or lower, depth than this number, it will just   be used as is in the analysis. (List <code>[]</code> of integers)</li> <li><code>subsample_by:</code> This determines how the 'full' sequencing depth of a sample   is calculated to determine the amount of subsampling needed to reach the   target depth. This should be one of three options: (1) <code>\"unfilt\"</code> will treat   the sequencing depth of all (unfiltered) reads and sites as the 'full' depth;   (2) <code>\"mapqbaseq\"</code> will filter out reads that don't pass the configured   mapping or base quality, then calculate depth across all sites as the 'full'   depth, (3) <code>\"sitefilt\"</code> will filter reads just as <code>\"mapqbaseq\"</code> does, but   will only calculate the 'full' depth from sites passing the sites filter. As   the main goal of subsampling is to make depth uniform for analyses, this   latter option is preferred, as it will most accurately bring the depth of the   samples to the target depth for analyses.   (<code>\"unfilt\"</code>/<code>\"mapqbaseq\"</code>/<code>\"sitefilt\"</code>)</li> <li><code>redo_depth_filts</code>: If <code>subsample_by</code> is set to <code>\"unfilt\"</code> or <code>\"mapqbaseq\"</code>,   then it would be possible to recaculate extreme depth filters for the   subsampled dataset. Enable this to do so, otherwise, the depth filters from   the full depth bams will be used. If <code>subsample_by</code> is set to <code>\"sitefilt\"</code>   this will have no effect, as the subsampling is already in reference to a set   site list. (<code>true</code>/<code>false</code>)</li> <li><code>drop_samples</code>: When performing depth subsampling, you may want to leave some   samples out that you kept in your 'full' dataset. These can be listed here and   they will be removed from ALL depth subsampled analyses. A use case for this   might be if you have a couple samples that are below your targeted subsample   depth, and you don't want to include them. Note that if you configure multiple   <code>subsample_dp</code>, these samples will be dropped from all of them. If you need to   perform mutliple depth subsamplings with different subsets of samples, its   best to run each depth individually. Alternatively, a config file can be made   for each subsampled depth, however you may run into issues of file locking   blocking both from running at the same time. (list of strings: <code>[]</code>)</li> <li><code>subsample_analyses:</code> Individually enable analyses to be performed with the   subsampled data. These have the same function as described in the   Analysis Selection section. Enabling here will   only run the analysis for the subsampled data, if you want to run it for the   full data as well, you need to enable it in the analyses section as well.   (<code>true</code>/<code>false</code>)</li> </ul>"},{"location":"config/#filter-sets","title":"Filter Sets","text":"<p>By default, this workflow will perform all analyses requested in the above section on all sites that pass the filters set in the above section. These outputs will contain <code>allsites-filts</code> in the filename and in the report. However, many times, it is useful to perform an analysis on different subsets of sites, for instance, to compare results for genic vs. intergenic regions, neutral sites, exons vs. introns, etc. Here, users can set an arbitrary number of additional filters using BED files. For each BED file supplied, the contents will be intersected with the sites passing the filters set in the above section, and all analyses will be performed additionally using those sites.</p> <p>For instance, given a BED file containing putatively neutral sites, one could set the following:</p> <pre><code>filter_beds:\n  neutral: \"resources/neutral_sites.bed\"\n</code></pre> <p>In this case, for each requested analysis, in addition to the <code>allsites-filts</code> output, a <code>neutral-filts</code> (named after the key assigned to the BED file in <code>config.yaml</code>) output will also be generated, containing the results for sites within the specified BED file that passed any set filters.</p> <p>More than one BED file can be set, up to an arbitrary number:</p> <pre><code>filter_beds:\n  neutral: \"resources/neutral_sites.bed\"\n  intergenic: \"resources/intergenic_sites.bed\"\n  introns: \"resources/introns.bed\"\n</code></pre> <p>It may also sometimes be desireable to skip analyses on <code>allsites-filts</code>, say if you are trying to only generate diversity estimates or generate SFS for a set of neutral sites you supply.</p> <p>To skip running any analyses for <code>allsites-filts</code> and only perform them for the BED files you supply, you can set <code>only_filter_beds: true</code> in the config file. This may also be useful in the event you have a set of already filtered sites, and want to run the workflow on those, ignoring any of the built in filter options by setting them to <code>false</code>.</p>"},{"location":"config/#software-configuration","title":"Software Configuration","text":"<p>This section contains the specific settings for each software, allowing users to customize the settings used. The default configuration file contains settings that are commonly used, and should be applicable to most datasets sequenced on patterened flow cells, but please check that they make sense for your analysis. If you are missing a configurable setting you need, open up an issue or a pull request and I'll gladly put it in if possible.</p> <p>Note to historical sample users wanting to remove transitions</p> <p>While most the defaults below are good for most datasets, including ones with historical samples and using MapDamage rescaling, transition removal is turned off by default. To enable transition removal to account for post-mortem DNA damage, enable the option <code>rmtrans</code> in the <code>angsd</code> section below. This will fill in the appropriate flag <code>-rmTrans</code> or <code>-noTrans</code> depending on the analysis, and remove transitions from all analyses. Only the IBS based IBSrelate method currently does not support transition removal.</p> <ul> <li><code>mapQ:</code> Phred-scaled mapping quality filter. Reads below this threshold will   be filtered out. (integer)</li> <li> <p><code>baseQ:</code> Phred-scaled base quality filter. Reads below this threshold will be   filtered out. (integer)</p> </li> <li> <p><code>params:</code></p> </li> <li><code>clipoverlap:</code><ul> <li><code>clip_user_provided_bams:</code> Determines whether overlapping read pairs will   be clipped in BAM files supplied by users. This is useful as many variant   callers will account for overlapping reads in their processing, but ANGSD   will double count overlapping reads. If BAMs were prepped without this in   mind, it can be good to apply before running through ANGSD. However, it   essentially creates a BAM file of nearly equal size for every sample, so   it may be nice to leave it off by applying it to the BAMs you feed into   the pipeline. (<code>true</code>/<code>false</code>)</li> </ul> </li> <li><code>fastp:</code><ul> <li><code>extra:</code> Additional options to pass to fastp trimming. (string)   [docs]</li> <li><code>min_overlap_hist:</code> Minimum overlap to collapse historical reads. Default   in fastp is 30. This effectively overrides the <code>--length_required</code> option   if it is larger than that. (INT)   [docs]</li> </ul> </li> <li><code>bwa_aln:</code><ul> <li><code>extra:</code> Additional options to pass to bwa aln for mapping of historical   sample reads. (string) [docs]</li> </ul> </li> <li><code>picard:</code><ul> <li><code>MarkDuplicates:</code> Additional options to pass to Picard MarkDuplicates.   <code>--REMOVE_DUPLICATES true</code> is recommended. (string)   [docs]</li> </ul> </li> <li><code>damageprofiler:</code><ul> <li><code>profile_modern:</code> Enable to run damage profiler on modern samples in   addition to historical (<code>true</code>/<code>false</code>)</li> </ul> </li> <li><code>genmap:</code> Parameters for pileup mappability analysis, see     GenMap's documentation for more     details.<ul> <li><code>K:</code> Length of k-mers to calculate mappability for. (integer)</li> <li><code>E:</code> Allowed mismatches per k-mer. (integer)</li> <li><code>map_thresh:</code> A threshold mappability score. Each site gets an average   mappability score taken by averaging the mappability of all K-mers that   would overlap it. A score of 1 means all K-mers are uniquely mappable,   allowing for <code>E</code> mismatches. This is done via a custom script, and may   eventually be replaced by the SNPable method, which is more common.   (float, 0-1)</li> </ul> </li> <li><code>extreme_depth_filt:</code> Parameters for excluding sites based on extreme high     and/or low global depth. The final sites list will contain only sites that     pass the filters for all categories requested (i.e the whole dataset     and/or the depth categories set in <code>samples.tsv</code>).<ul> <li><code>method:</code> Whether you will generate extreme thresholds as a multiple of   the median global depth (<code>\"median\"</code>) or as percentiles of the   global depth distribution (<code>\"percentile\"</code>)</li> <li><code>bounds:</code> The bounds of the depth cutoff, defined as a numeric list. For   the median method, the values will be multiplied by the median of the   distribution to set the thresholds (i.e. <code>[0.5,1.5]</code> would generate   a lower threshold at 0.5*median and an upper at 1.5*median). For the   percentile method, these define the lower and upper percentiles to filter   out (i.e [0.01,0.99] would remove the lower and upper 1% of the depth   distributions). (<code>[FLOAT, FLOAT]</code>)</li> <li><code>filt-on-dataset:</code> Whether to perform this filter on the dataset as a   whole (may want to set to false if your dataset global depth distribution   is multi-modal). (<code>true</code>/<code>false</code>)</li> <li><code>filt-on-depth-classes:</code> Whether to perform this filter on the depth   classes defined in the <code>samples.tsv</code> file. This will generate a global   depth distribution for samples in the same category, and perform the   filtering on these distributions independently. Then, the sites that pass   for all the classes will be included. (<code>true</code>/<code>false</code>)</li> </ul> </li> <li><code>samtools:</code><ul> <li><code>subsampling_seed:</code> Seed to use when subsampling bams to lower depth.   <code>\"$RANDOM\"</code> can be used to set a random seed, or any integer can be used   to set a consistent seed. (string or int)   [docs]</li> </ul> </li> <li><code>angsd:</code> General options in ANGSD, relevant doc pages are linked<ul> <li><code>gl_model:</code> Genotype likelihood model to use in calculation   (<code>-GL</code> option in ANGSD, [docs])</li> <li><code>maxdepth:</code> When calculating individual depth, sites with depth higher   than this will be binned to this value. Should be fine for most to leave   at <code>100</code>. (integer, [docs])</li> <li><code>mindepthind:</code> Individuals with sequencing depth below this value at a   position will be treated as having no data at that position by ANGSD.   ANGSD defaults to 1 for this. Note that this can be separately set for   individual heterozygosity estimates with <code>mindepthind_heterozygosity</code>   below. (integer, <code>-setMinDepthInd</code> option in ANGSD (INT)   [docs])</li> <li><code>minind_dataset:</code> Used to fill the <code>-minInd</code> option for any dataset wide   ANGSD outputs (like Beagles for PCA/Admix). Should be a floating point   value between 0 and 1 describing what proportion of the dataset must have   data at a site to include it in the output. (FLOAT)   [docs])</li> <li><code>minind_pop:</code> Used to fill the <code>-minInd</code> option for any population level   ANGSD outputs (like SAFs or Beagles for ngsF-HMM). Should be a floating   point value between 0 and 1 describing what proportion of the population   must have data at a site to include it in the output. (FLOAT)   [docs])</li> <li><code>rmtrans:</code> Removes transitions using ANGSD, effectively removing them   from downstream analyses. This is useful for removing DNA damage from   analyses, and will automatically set the appropriate ANGSD flags (i.e.   using   <code>-noTrans 1</code> for SAF files and   <code>-rmTrans 1</code> for Beagle   files.) (<code>true</code>/<code>false</code>)</li> <li><code>extra:</code> Additional options to pass to ANGSD during genotype likelihood   calculation at all times. This is primarily useful for adding BAM input   filters. Note that <code>--remove_bads</code> and <code>-only_proper_pairs</code> are enabled   by default, so they only need to be included if you want to turn them   off or explicitly show they are enabled. I've also found that for some   datasets, <code>-C 50</code> and <code>-baq 1</code> can create a strong relationship between   sample depth and detected diversity, effectively removing the benefits of   ANGSD for low/variable depth data. I recommend that these aren't included   unless you know you need them. Since the workflow uses BWA to map,   <code>-uniqueOnly 1</code> doesn't do anything if your minimum mapping quality is   &gt; 0, but you may wish to add it if you're bringing in your own files from   another mapper. Mapping and base quality thresholds are also not needed,   it will use the ones defined above automatically. If you prefer to correct   for historical damage by trimming the ends of reads, this is where you'd   want to put <code>-trim INT</code>. (string)   (string, [docs])</li> <li><code>extra_saf:</code> Same as <code>extra</code>, but only used when making SAF files (used   for SFS, thetas, F<sub>ST</sub>, IBSrelate (not NgsRelate version),   heterozygosity). Doesn't require options already in <code>extra</code> or defined via   other params in the YAML (such as <code>notrans</code>, <code>minind</code>, <code>GL</code>, etc.)   (string)</li> <li><code>extra_beagle:</code> Same as <code>extra</code>, but only used when making Beagle and Maf   files (used for PCA, Admix, ngsF-HMM, doIBS, NgsRelate, allele   frequencies). Doesn't require options already in <code>extra</code> or defined via   other params in the YAML (such as <code>rmtrans</code>, <code>minind</code>, <code>GL</code>, etc.)   (string)</li> <li><code>domajorminor:</code> Method for inferring the major and minor alleles. Set to   1 to infer from the genotype likelihoods, see   documentation   for other options. <code>1</code>, <code>2</code>, and <code>4</code> can be set without any additional   configuration. <code>5</code> must also have an ancestral reference provided in the   config, otherwise it will be the same as <code>4</code>. <code>3</code> is currently not   possible, and is used for generating the dataset polarized MAFs. If you   have a use for <code>3</code> that PopGLen is suited for, please open an issue and I   will look into it. (integer)</li> <li><code>snp_pval:</code> The p-value to use for calling SNPs (float or string)   [docs]</li> <li><code>domaf:</code> Method for inferring minor allele frequencies. Set to <code>1</code> to   infer from genotype likelihoods using a known major and minor from the   <code>domajorminor</code> setting above. Set to <code>2</code> to assume the minor is unknown.   See [docs] for   more information. <code>3</code> is possible, and will estimate the frequencies both   assuming a known and unknown minor. If you choose this option, you'll get   both in the MAF outputs, but only the known will be passed to NgsRelate if   you also use that. Other values are currently unsupported in PopGLen.   (integer)</li> <li><code>min_maf:</code> The minimum minor allele frequency required to call a SNP.   This is set when generating the Beagle file, so will filter SNPs for   PCAngsd, NGSadmix, ngsF-HMM, and NgsRelate. If you would like each tool   to handle filtering for maf on its own you can set this to <code>-1</code>   (disabled). (float, [docs])</li> <li><code>mindepthind_heterozygosity:</code> When estimating individual heterozygosity,   sites with sequencing depth lower than this value will be dropped.   (integer, <code>-setMinDepthInd</code> option in ANGSD) (integer)</li> </ul> </li> <li><code>ngsld:</code> Settings for ngsLD [docs]<ul> <li><code>max_kb_dist_est-ld:</code> For the LD estimates generated when setting   <code>estimate_ld: true</code> above, set the maximum distance between sites in kb   that LD will be estimated for (<code>--max_kb_dist</code> in ngsLD, integer)</li> <li><code>rnd_sample_est-ld:</code> For the LD estimates generated when setting   <code>estimate_ld: true</code> above, randomly sample this proportion of pairwise   linkage estimates rather than estimating all (<code>--rnd_sample</code> in ngsLD,   float)</li> <li><code>max_kb_dist_decay:</code> The same as <code>max_kb_dist_est-ld:</code>, but used when   estimating LD decay when setting <code>ld_decay: true</code> above (integer)</li> <li><code>rnd_sample_decay:</code> The same as <code>rnd_sample_est-ld:</code>, but used when   estimating LD decay when setting <code>ld_decay: true</code> above (float)</li> <li><code>fit_LDdecay_extra:</code> Additional plotting arguments to pass to   <code>fit_LDdecay.R</code> when estimating LD decay (string)</li> <li><code>fit_LDdecay_n_correction:</code> When estimating LD decay, should the sample   size corrected r<sup>2</sup> model be used? (<code>true</code>/<code>false</code>, <code>true</code> is the   equivalent of passing a sample size to <code>fit_LDdecay.R</code> in ngsLD using   <code>--n_ind</code>)</li> <li><code>max_kb_dist_pruning_dataset:</code> The same as <code>max_kb_dist_est-ld:</code>, but   used when linkage pruning SNPs as inputs for PCAngsd and NGSadmix. Pruning   is performed on the whole dataset. Any positions above this distance will   be assumed to be in linkage equilibrium during the pruning process.   (integer)</li> <li><code>pruning_min-weight_dataset:</code> The minimum r<sup>2</sup> to assume two positions are   in linkage disequilibrium when pruning for PCAngsd and NGSadmix analyses.   (float)</li> </ul> </li> <li><code>ngsrelate:</code> Settings for NGSrelate<ul> <li><code>ibsrelate-only-extra:</code> Any extra command line parameters to be passed to   NgsRelate when performing an IBSrelate only (no allele frequencies) run.   (string)</li> <li><code>freqbased-extra:</code> Any extra command line parameters to be passed to   NgsRelate when performing a standard, allele frequency based, run.   (string)</li> </ul> </li> <li><code>ngsf-hmm:</code> Settings for ngsF-HMM     [docs]<ul> <li><code>estimate_in_pops:</code> Set to <code>true</code> to run ngsF-HMM separately for each   population in your dataset. Set to <code>false</code> to run for whole dataset at   once. ngsF-HMM assumes Hardy-Weinberg Equilibrium (aside from inbreeding)   in the input data, so select the option that most reflects this in your   data. (<code>true</code>/<code>false</code>)</li> <li><code>prune:</code> Whether or not to prune SNPs for LD before running the analysis.   ngsF-HMM assumes independent sites, so it is preferred to set this to   <code>true</code> to satisfy that expectation. (<code>true</code>/<code>false</code>)</li> <li><code>max_kb_dist_pruning_pop:</code> The maximum distance between sites in kb   that will be treated as in LD when pruning for the ngsF-HMM input. (INT)</li> <li><code>pruning_min-weight_pop:</code> The minimum r<sup>2</sup> to assume two positions are in   linkage disequilibrium when pruning for the ngsF-HMM input. Note, that   this likely will be substantially higher for individual populations than   for the whole dataset, as background LD should be higher when no   substructure is present. (float)</li> <li><code>min_roh_length:</code> Minimum ROH size in base pairs to include in inbreeding   coefficient calculation. Set if short ROH might be considered low   confidence for your data. (integer)</li> <li><code>roh_bins:</code> A list of integers that describe the size classes in base   pairs you would like to partition the inbreeding coefficient by. This can   help visualize how much of the coefficient comes from ROH of certain size   classes (and thus, ages). List should be in ascending order and the first   entry should be greater than <code>min_roh_length</code>. The first bin will group   ROH between <code>min_roh_length</code> and the first entry, subsequent bins will   group ROH with sizes between adjacent entries in the list, and the final   bin will group all ROH larger than the final entry in the list. (list <code>[]</code>   of integers)</li> </ul> </li> <li><code>realSFS:</code> Settings for realSFS<ul> <li><code>fold:</code> Whether or not to fold the produced SFS. Set to 1 if you have not   provided an ancestral-state reference (0 or 1,   [docs])</li> <li><code>sfsboot:</code> Determines number of bootstrap replicates to use when   requesting bootstrapped SFS. Is used for both 1dsfs and 2dsfs (this is   very easy to separate, open an issue if desired). Automatically used   for heterozygosity analysis to calculate confidence intervals. (integer)</li> </ul> </li> <li><code>fst:</code> Settings for F<sub>ST</sub> calculation in ANGSD<ul> <li><code>whichFst:</code> Determines which F<sub>ST</sub> estimator is used by ANGSD. With 0   being the default Reynolds 1983 and 1 being the Bhatia-Hudson 2013   estimator. The latter is preferable for small or uneven sample sizes   (0 or 1, [docs])</li> <li><code>win_size:</code> Window size in bp for sliding window analysis (integer)</li> <li><code>win_step:</code> Window step size in bp for sliding window analysis (integer)</li> </ul> </li> <li><code>thetas:</code> Settings for pi, theta, and Tajima's D estimation<ul> <li><code>win_size:</code> Window size in bp for sliding window analysis (integer)</li> <li><code>win_step:</code> Window step size in bp for sliding window analysis (integer)</li> <li><code>minsites:</code> Minimum sites to include window in report plot. This does not   remove them from the actual output, just the report plot and averages.</li> </ul> </li> <li><code>ngsadmix:</code> Settings for admixture analysis with NGSadmix. This analysis is     performed for a set of K groupings, and each K has several replicates     performed. Replicates will continue until a set of N highest likelihood     replicates converge, or the number of replicates reaches an upper threshold     set here. Defaults for <code>reps</code>, <code>minreps</code>, <code>thresh</code>, and <code>conv</code> can be left     as default for most. Based on method described by     Pe\u010dnerov\u00e1 et al. 2021, Curr. Biol.<ul> <li><code>kvalues:</code> A list of values of K to fit the data to (list <code>[]</code> of   integers)</li> <li><code>reps:</code> The maximum number of replicates to perform per K. Default is 100.   (integer)</li> <li><code>minreps:</code> The minimum number of replicates to perform, even if   replicates have converged. Default is 20. (integer)</li> <li><code>thresh:</code> The convergence threshold - the top replicates must all be   within this value of log-likelihood units to consider the run converged.   Default is 2. (integer)</li> <li><code>conv:</code> The number of top replicates to include in convergence   assessment. Default is 3. (integer)</li> <li><code>extra:</code> Additional arguments to pass to NGSadmix (for instance,   increasing <code>-maxiter</code>). (string,   [docs])</li> </ul> </li> <li><code>ibs:</code> Settings for identity by state calculation with ANGSD<ul> <li><code>-doIBS:</code> Whether to use a random (1) or consensus (2) base in IBS   distance calculation   ([docs])</li> </ul> </li> </ul>"},{"location":"extending-workflows/","title":"Extending or modifying analyses in PopGLen","text":"<p>Using PopGLen as a Snakemake module makes it easy to incorporate extensions to it in the Snakefile, enabling cusomization of the workflow. This means that you can extend upon the workflow with your own rules that use PopGLen outputs as their input, keeping the project as one contained workflow that can be easily reproduced. This functionality could also allow for replacement of some of PopGLen's analyses with alternative options, such as estimating the covariance matrix for PCA with single read sampling in ANGSD rather than the likelihood based approach of PCAngsd. Most of this is covered in Snakemake's documentation on modularization, and all methods described there should be compatible with PopGLen.</p> <p>As an example, we will replace the PCA produced using PCAngsd in PopGLen with one produced using a covariance matrix from single read sampling using ANGSD. This should only really add one step, estimating the covariance matrix in ANGSD instead of PCAngsd. We start with a bare bones Snakefile that imports PopGLen as a module:</p> workflow/Snakefile<pre><code>from snakemake.utils import min_version\n\n\nmin_version(\"6.10.0\")\n\n\nconfigfile: \"config/config.yaml\"\n\n\n# declare https://github.com/zjnolen/PopGLen as a module\nmodule popglen:\n    snakefile:\n        github(\"zjnolen/PopGLen\", path=\"workflow/Snakefile\", tag=\"v0.4.1\")\n    config:\n        config\n\n\n# use all rules from https://github.com/zjnolen/PopGLen\nuse rule * from popglen\n</code></pre> <p>As is, it will run PopGLen as described in the documentation. To utilize a covariance matrix estimated by something other than PCAngsd when generating the PCA, we need to do two things: (1) add a rule to generate the covariance matrix with the other tool and (2) overwrite the input for the rule that runs the principal component analysis with this new covariance matrix.</p> <p>We will do this by adding the code block below to the end of the Snakefile. It first defines a rule, <code>angsd_doCov</code>, which generates the covariance matrix. This is very similar to the <code>angsd_doIBS</code> rule already in PopGLen, so much of that rule can be repurposed into this one. After we add the rule, we then simply tell Snakemake that when it imports the rule <code>plot_pca</code> from PopGLen, it should use this covariance matrix made by <code>angsd_doCov</code> instead of the one from PCAngsd defined by default.</p> workflow/Snakefile linenums=<pre><code># Add a rule which estimates the covariance matrix, but from single read\n# sampling in ANGSD rather than genotype likelihoods in PCAngsd\n\n\nrule angsd_doCov:\n    \"\"\"\n    Estimates covariance matrix for all individuals.\n    \"\"\"\n    input:\n        bamlist=\"results/datasets/{dataset}/bamlists/{dataset}.{ref}_{population}{dp}.bamlist\",\n        bams=popglen.get_bamlist_bams,\n        bais=popglen.get_bamlist_bais,\n        sites=\"results/datasets/{dataset}/filters/snps/{dataset}.{ref}_{population}{dp}_{sites}-filts_snps.sites\",\n        idx=\"results/datasets/{dataset}/filters/snps/{dataset}.{ref}_{population}{dp}_{sites}-filts_snps.sites.idx\",\n    output:\n        ibs=temp(\"results/datasets/{dataset}/analyses/covar/{dataset}.{ref}_{population}{dp}_{sites}-filts.ibs.gz\"),\n        covmat=\"results/datasets/{dataset}/analyses/covar/{dataset}.{ref}_{population}{dp}_{sites}-filts.covMat\",\n        arg=\"results/datasets/{dataset}/analyses/covar/{dataset}.{ref}_{population}{dp}_{sites}-filts.arg\",\n    container:\n        popglen.angsd_container\n    params:\n        mapQ=config[\"mapQ\"],\n        baseQ=config[\"baseQ\"],\n        trans=popglen.get_trans,\n        out=lambda w, output: os.path.splitext(output.arg)[0],\n    threads: 2\n    resources:\n        runtime=\"1d\",\n    shell:\n        \"\"\"\n        angsd -doIBS 1 -doCov 1 -bam {input.bamlist} -nThreads {threads} \\\n            -doCounts 1 -minMapQ {params.mapQ} -minQ {params.baseQ} \\\n            -sites {input.sites} -rmTrans {params.trans} -doMajorMinor 3 \\\n            -out {params.out}\n        \"\"\"\n\n\n# Update plot_pca rule to use the covariance matrix from angsd_doCov rather than\n# from PCAngsd\n\n\nuse rule plot_pca from popglen as plot_pca with:\n    input:\n        \"results/datasets/{dataset}/analyses/covar/{dataset}.{ref}_{population}{dp}_{sites}-filts.covMat\",\n        \"results/datasets/{dataset}/poplists/{dataset}_{population}{dp}.indiv.list\",\n</code></pre> <p>For a substitution like this, these are the only changes we need to make. You can try running this Snakefile using the tutorial data to see it in action. These two methods actually produce quite similar results with the tutorial dataset (though the axis of PC2 is inverted, as its direction is arbitrary):</p> PCAngsd covariance matrix ANGSD -doCov covar matrix <p>Extending workflows like this allows you to use PopGLen as a base for your project, adding additional analyses as you need them. If you have an analysis (or alternative tool for an analysis) that you feel suits PopGLen (i.e. is suited for low-coverage population genomics), but is not yet implemented, please feel free to open an issue and request its addition.</p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#tutorial","title":"Tutorial","text":"<p>A tutorial is available with a small(ish) dataset where biologically meaningful results can be produced. This can help get an understanding of a good workflow to use different modules. You can also follow along with your own data and just skip analyses you don't want. I recommend at least looking at this to get an idea of the scope of the pipeline as well as to see a few tips. If you prefer to just jump in instead, below describes how to get a new project up and running.</p>"},{"location":"getting-started/#requirements","title":"Requirements","text":"<p>This pipeline can be run on Linux systems with Conda and Apptainer/Singularity installed. All other dependencies will be handled with the workflow, and thus, sufficient storage space is needed for these installations (~10GB, though this is only when using all features of the pipeline, if you are not using any of the mapping module, it will be more like 3-5GB). It can be run on a local workstation with sufficient resources and storage space (dataset dependent), but is aimed at execution on high performance computing systems with job queuing systems.</p> <p>Data-wise, you'll need a reference genome (uncompressed) and some sequencing data for your samples. The latter can be either raw FASTQ files, BAM alignments to the reference, or SRA accession numbers for already published FASTQ files.</p>"},{"location":"getting-started/#deploying-the-workflow","title":"Deploying the workflow","text":"<p>The pipeline can be deployed in two ways: (1) using Snakedeploy which will deploy the pipeline as a module (recommended); (2) clone the repository at the version/branch you prefer (recommended if you will need to change workflow code beyond what is possible with module definitions). If you are curious how modularization works in Snakemake, take a look at the docs.</p> <p>Both methods require a Snakemake environment to run the pipeline in.</p>"},{"location":"getting-started/#preparing-the-environment","title":"Preparing the environment","text":"<p>First, create an environment for Snakemake, including Snakedeploy if you intend to deploy that way:</p> <pre><code>conda create -c conda-forge -c bioconda --name popglen snakemake snakedeploy\n</code></pre> <p>If you already have a Snakemake environment, you can use that instead. Snakemake versions &gt;=8 are likely to work, but most testing is on 8.20. If you intend to use a job queuing system with the pipeline, be sure to install the appropriate executor plugin. Most of the testing has been with <code>snakemake-executor-plugin-slurm</code>, which can be installed in the environment you are running Snakemake from.</p> <p>Activate the Snakemake environment:</p> <pre><code>conda activate snakemake\n</code></pre>"},{"location":"getting-started/#option-1-deploying-with-snakedeploy","title":"Option 1. Deploying with Snakedeploy","text":"<p>Make your working directory:</p> <pre><code>mkdir -p /path/to/work-dir\ncd /path/to/work-dir\n</code></pre> <p>And deploy the workflow, using the tag for the version you want to deploy:</p> <pre><code>snakedeploy deploy-workflow https://github.com/zjnolen/PopGLen . --tag v0.4.1\n</code></pre> <p>This will generate a simple Snakefile in a <code>workflow</code> folder that loads the pipeline as a module. It will also download the template <code>config.yaml</code>, <code>samples.tsv</code>, and <code>units.tsv</code> in the <code>config</code> folder.</p>"},{"location":"getting-started/#option-2-cloning-from-github","title":"Option 2. Cloning from GitHub","text":"<p>Go to the folder you would like you working directory to be created in and clone the GitHub repo:</p> <pre><code>git clone https://github.com/zjnolen/PopGLen.git\n</code></pre> <p>If you would like, you can change the name of the directory:</p> <pre><code>mv PopGLen work-dir-name\n</code></pre> <p>Move into the working directory (<code>PopGLen</code> or <code>work-dir-name</code> if you changed it) and checkout the version you would like to use:</p> <pre><code>git checkout v0.4.1\n</code></pre> <p>This can also be used to checkout specific branches or commits.</p>"},{"location":"getting-started/#configuring-the-workflow","title":"Configuring the workflow","text":"<p>Now you are ready to configure the workflow, see the documentation for that here.</p>"},{"location":"outfile-summ/","title":"Output files","text":"<p>Here is a brief overview of the file paths for output files. Files marked with temp are flagged as temporary and will be deleted when they are no longer needed by the workflow. Paths have wildcards in their names, indicated by <code>{}</code>. Some of these wildcards include:</p> <ul> <li><code>{dataset}</code> - The dataset name defined in the config file</li> <li><code>{ref}</code> - The reference genome name defined in the config file</li> <li><code>{sample}</code> - Sample names as written in the sample list</li> <li><code>{population}</code> - Population names as written in the sample list, but also   including depth groupings in some rare cases</li> <li><code>{unit}</code> - The sequencing unit (usually flow cell barcode + sequencing lane)   of the sequencing reads</li> <li><code>{lib}</code> - The sequencing library the sequencing reads come from</li> <li><code>{dp}</code> - The subsampled sequencing depth, if enabled. The full depth results   will have this wildcard empty in the path.</li> <li><code>{sites}</code> - The filter set used to produce the result. If no user provided   filters are given, this will always be <code>allsites</code>. If user provided filters   are in the config, it will use the name given to the BED file.</li> </ul>"},{"location":"outfile-summ/#mapping","title":"Mapping","text":""},{"location":"outfile-summ/#trimmedcollapsed-reads","title":"Trimmed/Collapsed reads","text":""},{"location":"outfile-summ/#trimmed-reads-uncollapsed-temp","title":"Trimmed reads (uncollapsed, temp)","text":"<p><code>results/preprocessing/fastp/{sample}_{unit}_{lib}.{read}.paired.fastq.gz</code></p>"},{"location":"outfile-summ/#collapsed-reads-temp","title":"Collapsed reads (temp)","text":"<p><code>results/preprocessing/fastp/{sample}_{unit}_{lib}.merged.fastq.gz</code></p>"},{"location":"outfile-summ/#processed-alignments","title":"Processed alignments","text":""},{"location":"outfile-summ/#bam-files","title":"BAM files","text":"<p><code>results/mapping/bams/{sample}.{ref}.rmdup.realn.clip.bam</code> (also symlinked to <code>results/datasets/{dataset}/bams/{sample}.{ref}.bam</code>)</p>"},{"location":"outfile-summ/#bam-files-mapdamage-rescaled","title":"BAM files (MapDamage rescaled)","text":"<p><code>results/mapping/bams/{sample}.{ref}.rmdup.realn.clip.rescaled.bam</code> (also symlinked to <code>results/datasets/{dataset}/bams/{sample}.{ref}.bam</code> when enabled)</p>"},{"location":"outfile-summ/#subsampled-bam-files","title":"Subsampled BAM files","text":"<p><code>results/datasets/{dataset}/bams/{sample}.{ref}{dp}.bam</code></p>"},{"location":"outfile-summ/#quality-control","title":"Quality control","text":""},{"location":"outfile-summ/#fastp-reports","title":"fastp reports","text":"<p><code>results/preprocessing/qc/fastp/{sample}_{unit}_{lib}_merged.html</code> (collapsed reads)</p> <p><code>results/preprocessing/qc/fastp/{sample}_{unit}_{lib}_paired.html</code> (trimmed, i.e. uncollapsed, reads)</p>"},{"location":"outfile-summ/#picard-markduplicates-metrics","title":"Picard MarkDuplicates metrics","text":"<p><code>results/mapping/qc/mark_duplicates/{sample}.{ref}.picard.metrics</code></p>"},{"location":"outfile-summ/#dedup-duplicate-removal-metrics","title":"Dedup duplicate removal metrics","text":"<p><code>results/mapping/qc/dedup/{sample}_{lib}.{ref}.dedup.json</code></p>"},{"location":"outfile-summ/#bamutil-overlap-clipping-metrics","title":"Bamutil overlap clipping metrics","text":"<p><code>results/mapping/qc/bamutil_clipoverlap/{sample}.{ref}.rmdup.realn.clipoverlap.stat</code></p>"},{"location":"outfile-summ/#samtools-flagstat-for-raw-alignments-pre-duplicate-removal","title":"Samtools flagstat for raw alignments (pre-duplicate removal)","text":"<p><code>results/mapping/mapped/{sample}_{library}.{ref}.merged.flagstat</code> (collapsed reads)</p> <p><code>results/mapping/mapped/{sample}.{ref}.paired.flagstat</code> (trimmed, i.e. uncollapsed, reads)</p>"},{"location":"outfile-summ/#qualimap-reports","title":"Qualimap reports","text":"<p><code>results/mapping/qc/qualimap/{sample}.{ref}/qualimapReport.html</code> (BAMs processed by PopGLen),</p> <p><code>results/datasets/{dataset}/qc/user-provided-bams/qualimap/{sample}.{ref}/qualimapReport.html</code> (BAMs brought by user),</p> <p><code>results/datasets/{dataset}/qc/qualimap/qualimap_all.{ref}_mqc.html</code> (MultiQC)</p>"},{"location":"outfile-summ/#mean-and-st-dev-depth-mapping-rates","title":"Mean and st. dev. depth, mapping rates","text":"<p><code>results/datasets/{dataset}/qc/{dataset}.{ref}_all{dp}.sampleqc.tsv</code></p>"},{"location":"outfile-summ/#identity-by-state-distance-to-reference","title":"Identity by state distance to reference","text":"<p><code>results/datasets/{dataset}/qc/ibs_refbias/{dataset}.{ref}_{population}{dp}_{filts}.refibs.tsv</code> (here <code>{filts}</code> refers to either unfiltered, or with one of the sites filters)</p> <p><code>results/datasets/{dataset}/plots/ibs_refbias/{dataset}.{ref}_all{dp}_{filts}.{grouping}.pdf</code> (plots)</p>"},{"location":"outfile-summ/#dna-damage","title":"DNA damage","text":"<p><code>results/mapping/qc/damageprofiler/{sample}.{ref}</code> (DamageProfiler)</p> <p><code>results/mapping/qc/mapdamage/{sample}.{ref}</code> (MapDamage 2)</p> <p><code>results/datasets/{dataset}/qc/dna-damage-mqc/dna-damage_all.{ref}_mqc.html</code> (MultiQC)</p>"},{"location":"outfile-summ/#filtering","title":"Filtering","text":""},{"location":"outfile-summ/#reference-links-and-indices","title":"Reference links and indices","text":"<p><code>results/ref/{ref}/{ref}.fa</code> (symlink to provided referenceindices are built off this symlink)</p> <p><code>results/ref/{ref}/{ref}.anc.fa</code> (symlink to provided ancestral state reference, if provided in config)</p>"},{"location":"outfile-summ/#reference-chunk-region-files","title":"Reference 'chunk' region files","text":"<p><code>results/datasets/{dataset}/filters/chunks/{ref}_chunk{chunk}.rf</code> (where <code>{chunk}</code> is the chunk number)</p>"},{"location":"outfile-summ/#mappability","title":"Mappability","text":""},{"location":"outfile-summ/#genmap-mappability-scores","title":"Genmap mappability scores","text":"<p><code>results/ref/{ref}/genmap/map/{ref}_k{k}_e{e}.bedgraph</code></p>"},{"location":"outfile-summ/#pileup-mappability-scores","title":"Pileup mappability scores","text":"<p><code>results/ref/{ref}/genmap/pileup/{ref}_pileup_mappability_k{k}_e{e}.bed</code></p>"},{"location":"outfile-summ/#pileup-mappability-filter-bed","title":"Pileup mappability filter BED","text":"<p><code>results/datasets/{dataset}/filters/pileupmap/{ref}_k{k}_e{e}_{thresh}.bed</code></p>"},{"location":"outfile-summ/#repeats","title":"Repeats","text":""},{"location":"outfile-summ/#repeat-library-generated-in-pipeline","title":"Repeat library generated in pipeline","text":"<p><code>results/ref/{ref}/repeatmodeler/{ref}-families.fa</code></p>"},{"location":"outfile-summ/#identified-repeats-either-from-provided-or-generated-library","title":"Identified repeats (either from provided or generated library)","text":"<p><code>results/ref/{ref}/repeatmasker/{ref}.fa.out.gff</code></p>"},{"location":"outfile-summ/#repeat-filter-bed","title":"Repeat filter BED","text":"<p><code>results/ref/{ref}/repeatmasker/{ref}.fa.out.bed</code></p>"},{"location":"outfile-summ/#combined-filter-bed","title":"Combined filter BED","text":"<p><code>results/datasets/{dataset}/filters/combined/{dataset}.{ref}{dp}_allsites-filts.bed</code></p> <p><code>results/datasets/{dataset}/filters/combined/{dataset}.{ref}{dp}_{sites}-filts.bed</code> (when user provides additional filter BEDs. <code>{sites}</code> will be the name in the config of the filter BED)</p>"},{"location":"outfile-summ/#genotype-likelihood-analyses","title":"Genotype likelihood analyses","text":""},{"location":"outfile-summ/#gl-files","title":"GL files","text":""},{"location":"outfile-summ/#beagles","title":"Beagles","text":"<p><code>results/datasets/{dataset}/beagles/{dataset}.{ref}_{population}{dp}_{sites}-filts.beagle.gz</code></p>"},{"location":"outfile-summ/#safs","title":"SAFs","text":"<p><code>results/datasets/{dataset}/safs/{dataset}.{ref}_{population}{dp}_{sites}-filts.saf.idx</code></p> <p><code>results/datasets/{dataset}/safs/{dataset}.{ref}_{population}{dp}_{sites}-filts.saf.pos.gz</code></p> <p><code>results/datasets/{dataset}/safs/{dataset}.{ref}_{population}{dp}_{sites}-filts.saf.gz</code></p> <p>(for the above files, <code>{population}</code> can also be from samples for 1 sample SAF)</p>"},{"location":"outfile-summ/#linkage-disequilibrium","title":"Linkage disequilibrium","text":""},{"location":"outfile-summ/#ld-estimates","title":"LD estimates","text":"<p><code>results/datasets/{dataset}/analyses/ngsLD/{dataset}.{ref}_{population}{dp}_{sites}-filts.ld_maxkbdist-{maxkb}_rndsample-{rndsmp}.gz</code> (where <code>{maxkb}</code> is the max distance between SNPs in kilobases and <code>{rndsnp}</code> is the proportion of SNPs to sample for the LD calculation)</p>"},{"location":"outfile-summ/#relatedness-tables","title":"Relatedness tables","text":""},{"location":"outfile-summ/#ibsrelate-sfs-based","title":"IBSrelate (SFS-based)","text":"<p><code>results/datasets/{dataset}/analyses/kinship/ibsrelate_sfs/{dataset}.{ref}_all{dp}_{sites}-filts.kinship</code></p>"},{"location":"outfile-summ/#ibsrelate-ibs-based","title":"IBSrelate (IBS-based)","text":"<p><code>results/datasets/{dataset}/analyses/kinship/ibsrelate_ibs/{dataset}.{ref}_all{dp}_{sites}-filts.kinship</code></p>"},{"location":"outfile-summ/#ibsrelate-ngsrelate-implementation","title":"IBSrelate (NgsRelate implementation)","text":"<p><code>results/datasets/{dataset}/analyses/kinship/ngsrelate/{dataset}.{ref}_all{dp}_{sites}-filts_ibsrelate-nofreq.tsv</code></p>"},{"location":"outfile-summ/#ngsrelate-allele-frequency-based","title":"NgsRelate (allele frequency-based)","text":"<p><code>results/datasets/{dataset}/analyses/kinship/ngsrelate/{dataset}.{ref}_all{dp}_{sites}-filts_ngsrelate-freq.tsv</code></p>"},{"location":"outfile-summ/#principal-component-analysis","title":"Principal component analysis","text":""},{"location":"outfile-summ/#covariance-matrix","title":"Covariance matrix","text":"<p>(for these, <code>{population}</code> is either <code>all</code> or <code>all_excl_pca-admix</code> depending on if relatives were removed)</p> <p><code>results/datasets/{dataset}/analyses/pcangsd/{dataset}.{ref}_{population}{dp}_{sites}-filts.cov</code></p>"},{"location":"outfile-summ/#admixture","title":"Admixture","text":""},{"location":"outfile-summ/#ngsadmix-outputs","title":"NGSadmix outputs","text":"<p>(for these, <code>{population}</code> is either <code>all</code> or <code>all_excl_pca-admix</code> depending on if relatives were removed)</p> <p><code>results/datasets/{dataset}/analyses/ngsadmix/{dataset}.{ref}_{population}{dp}_{sites}-filts_K{kvalue}.qopt</code></p> <p><code>results/datasets/{dataset}/analyses/ngsadmix/{dataset}.{ref}_{population}{dp}_{sites}-filts_K{kvalue}.fopt.gz</code></p> <p><code>results/datasets/{dataset}/analyses/ngsadmix/{dataset}.{ref}_{population}{dp}_{sites}-filts_K{kvalue}_optimization_wrapper.log</code> (this logs the replicate number, seed, and log-likelihood of the replicate NGSadmix runs for each value of K)</p>"},{"location":"outfile-summ/#evaladmix-correlation-of-residuals","title":"EvalAdmix correlation of residuals","text":"<p>(for these, <code>{population}</code> is either <code>all</code> or <code>all_excl_pca-admix</code> depending on if relatives were removed)</p> <p><code>results/datasets/{dataset}/analyses/ngsadmix/{dataset}.{ref}_{population}{dp}_{sites}-filts_K{kvalue}.corres</code></p>"},{"location":"outfile-summ/#site-frequency-spectrum-sfs","title":"Site frequency spectrum (SFS)","text":""},{"location":"outfile-summ/#one-populationone-sample","title":"One population/one sample","text":"<p><code>results/datasets/{dataset}/analyses/sfs/{dataset}.{ref}_{population}{dp}_{sites}-filts.sfs</code></p> <p><code>results/datasets/{dataset}/analyses/sfs/{dataset}.{ref}_{population}{dp}_{sites}-filts.boot.sfs</code> (bootstrapped)</p>"},{"location":"outfile-summ/#two-populationtwo-sample","title":"Two population/two sample","text":"<p><code>results/datasets/{dataset}/analyses/sfs/{dataset}.{ref}_{population1}-{population2}{dp}_{sites}-filts.sfs</code></p> <p><code>results/datasets/{dataset}/analyses/sfs/{dataset}.{ref}_{population1}-{population2}{dp}_{sites}-filts.boot.sfs</code> (bootstrapped)</p>"},{"location":"outfile-summ/#theta-and-neutrality-statistics-nucleotide-diversity-wattersons-theta-tajimas-d","title":"Theta and neutrality statistics (nucleotide diversity, Watterson's theta, Tajima's D)","text":""},{"location":"outfile-summ/#windowed-theta-estimates","title":"Windowed theta estimates","text":"<p><code>results/datasets/{dataset}/analyses/thetas/{dataset}.{ref}_{population}{dp}_{sites}-filts.thetaWindows.{win}_{step}.pestPG</code></p>"},{"location":"outfile-summ/#mean-and-confidence-intervals","title":"Mean and confidence intervals","text":"<p><code>results/datasets/{dataset}/analyses/thetas/{dataset}.{ref}_all{dp}_{sites}-filts.window_{win}_{step}.{stat}.mean.tsv</code> (where <code>{stat}</code> is <code>pi</code>, <code>watterson</code>, or <code>tajima</code>)</p>"},{"location":"outfile-summ/#genetic-differentiation-fst","title":"Genetic differentiation (F<sub>ST</sub>)","text":""},{"location":"outfile-summ/#windowed-fst-estimates","title":"Windowed F<sub>ST</sub> estimates","text":"<p><code>results/datasets/{dataset}/analyses/fst/{dataset}.{ref}_{population1}-{population2}{dp}_{sites}-filts.fst.window_{win}_{step}.tsv</code></p>"},{"location":"outfile-summ/#global-estimates","title":"Global estimates","text":"<p><code>results/datasets/{dataset}/analyses/fst/{dataset}.{ref}_{unit}pairs{dp}_{sites}-filts.fst.global.tsv</code> (where <code>{unit}</code> is either <code>pop</code> or <code>ind</code>, depending on whether F<sub>ST</sub> was requested between populations or individuals)</p>"},{"location":"outfile-summ/#individual-heterozygosity-table","title":"Individual heterozygosity table","text":"<p><code>results/datasets/{dataset}/analyses/heterozygosity/{dataset}.{ref}_all{dp}_{sites}-filts_heterozygosity.tsv</code></p>"},{"location":"outfile-summ/#inbreeding","title":"Inbreeding","text":""},{"location":"outfile-summ/#ngsf-hmm-model-based-inbreeding-coefficients","title":"ngsF-HMM model based inbreeding coefficients","text":"<p><code>results/datasets/{dataset}/analyses/ngsF-HMM/{dataset}.{ref}_{population}{dp}_{sites}-filts.indF</code></p>"},{"location":"outfile-summ/#ibd-segments-runs-of-homozygosity","title":"IBD segments (runs of homozygosity)","text":"<p><code>results/datasets/{dataset}/analyses/ngsF-HMM/{dataset}.{ref}_{population}{dp}_{sites}-filts.roh</code> (raw)</p> <p><code>results/datasets/{dataset}/analyses/ngsF-HMM/{dataset}.{ref}_all{dp}_{sites}-filts.all_roh.bed</code> (after filtering out runs less than minimum length in configuration file)</p>"},{"location":"outfile-summ/#froh-table","title":"F<sub>RoH</sub> table","text":"<p><code>results/datasets/{dataset}/analyses/ngsF-HMM/{dataset}.{ref}_all{dp}_{sites}-filts.ind_froh.tsv</code></p>"},{"location":"outfile-summ/#identity-by-state-matrix","title":"Identity by state matrix","text":"<p><code>results/datasets/{dataset}/analyses/IBS/{dataset}.{ref}_all{dp}_{sites}-filts.ibsMat</code></p>"},{"location":"outfile-summ/#population-specific-allele-frequencies","title":"Population specific allele frequencies","text":"<p><code>results/datasets/{dataset}/mafs/{dataset}.{ref}_{population}{dp}_{sites}-filts.pop-maj.mafs.gz</code> (sites segregating in population, major/minor polarized to population freqs unless using <code>-doMajorMinor 4</code> or <code>5</code>)</p> <p><code>results/datasets/{dataset}/mafs/{dataset}.{ref}_{population}{dp}_{sites}-filts.dataset-maj.mafs.gz</code> (sites segregating in dataset, including invariant in population, major/minor polarized to dataset wide frequencies unless using <code>-doMajorMinor 4</code> or <code>5</code>)</p>"},{"location":"resources/","title":"Altering resource usage","text":"<p>This workflow sets a default number of threads and runtime for each rule. Memory is not set per rule as development took place on clusters that did not allow direct requests of memory when submitting SLURM jobs, instead always allocating 6.4GB RAM per core for each job. The easiest way to get up and running with this workflow is to set <code>--default-resources mem_mb=\"XXXX * threads\"</code> when running the Snakemake command, replacing <code>XXXX</code> with the RAM available per core on your HPC system in MB (in the case of ours, this was 6400). See the profiles in the GitHub repository as an example.</p> <p>However, the default resources may not always work, your data may need more memory, or longer runtimes, or maybe you even need shorter if your HPC has shorter runtime limits than some of the defaults we set (up to 7 days). Snakemake makes it easy to alter resources in the command line using the <code>--set-resources</code> and <code>--set-threads</code> options, which will override anything set in the workflow already.</p> <p>This is even better set up in a profile. PopGLen includes a default profile in the profiles/default folder, which has all the rules already populated with the default resources already set. If you have downloaded the workflow via cloning the repository, all you need to do is change the values to match your needs (which may require adding an additional resource option, like <code>mem_mb</code>). If you have downloaded via Snakedeploy, just download the config file from the main repository, place it in the same directory in your deployed workflow, modify as needed and Snakemake will automatically pick it up when you run it.</p> <p>Here is an example of what it would look like to change the number of threads for the rule <code>bwa_index</code> from the default of 1 to a new value of 5:</p> profiles/default/config.yaml<pre><code>set-threads:\n  # Reference Prep\n  link_ref: 1\n  link_anc_ref: 1\n  bwa_index: 5\n  samtools_faidx: 1\n  ref_chunking: 1\n  picard_dict: 1\n</code></pre> <p>And if you wanted to update the runtime to give it only a maximum of 1 day and add a limit of 2GB for memory:</p> profiles/default/config.yaml<pre><code>set-resources:\n#   # Reference Prep\n  link_ref:\n    runtime:\n  link_anc_ref:\n    runtime:\n  bwa_index:\n    runtime: \"1d\"\n    mem_mb: 2048\n  samtools_faidx:\n    runtime:\n  ref_chunking:\n    runtime:\n  picard_dict:\n    runtime:\n</code></pre> <p>You can also use the term 'attempt' in these definitions, which allow you to scale the resources with the number of attempts a rule has made, automatically increasing threads, runtime, or memory with each attempt. This is already done for several rules in the config, largely to automatically request more memory due to OOM errors.</p>"},{"location":"tutorial/","title":"Tutorial","text":"<p>This tutorial aims to get users familiar with the general usage of PopGLen by walking through a small(ish) dataset. Users can walk through this dataset to get familiar with using PopGLen on their server/machine, or can use it as a template to follow along with their own data, modifying settings when suitable.</p> <p>The tutorial dataset is a subset of the data used in this manuscript and contains sequencing data for 20 Cyaniris semiargus butterfly individuals from southern Sweden. These samples all come from the province of Sk\u00e5ne, a region where grassland cover has considerably declined over the past century as agriculture has intensified. In this tutorial, 10 individuals come from historical samples from the entomological collections of the Biological Museum, Lund University, half from a locality in western Sk\u00e5ne and the other from a locality in eastern Sk\u00e5ne. The other 10 individuals come from modern samples collected in nearby localities in 2021, which will allow us to compare genomic metrics over time. This is one of the main use cases for PopGLen - a dataset with both modern and historical samples, but most of the functionality will be the same if your dataset has only modern or only historical samples.</p> <p>For reference, I have included a report that utilizes the full data, which can be useful to reference to get a better idea of what the results would look like with real data. [Full report]</p> How much of the original data is in the tutorial dataset? <p>To speed up processing times for the purpose of the tutorial, I only included ~10Mb across four chromosomes of the Cyaniris semiargus reference and enough reads to get 2-5X sequencing depth across that region. This is enough to get close to the results from the full genome, but some analyses will be more impacted than others. I will mention when this is the case, and include the results using the full data alongside the tutorial data for comparison.</p>"},{"location":"tutorial/#prepping-the-workflow","title":"Prepping the workflow","text":"<p>To run PopGLen, you will need access to Conda/Mamba and Apptainer/Singularity as these will be used to run and install the software utilized in the pipeline, including Snakemake. Due to the Apptainer/Singularity requirement, this means that it will only run on Linux machines. I recommend performing the tutorial on the machine you are likely to run your dataset on, rather than testing it on your local machine.</p> <p>Once you have installed Conda/Mamba and Apptainer/Singularity on your machine, you're ready to start setting up the environment for PopGLen to use. We will install a simple conda environment with Snakemake, the only dependency needed:</p> <pre><code>conda create -n popglen \\\n    -c conda-forge -c bioconda \\\n    snakemake=8.25.0 snakedeploy\n</code></pre> <p>Once run, this will install Snakemake and Snakedeploy to the environment 'popglen', which we will activate to download and run the pipeline:</p> <pre><code>conda activate popglen\n</code></pre> <p>Now, create a working directory for your project, deploy the workflow inside:</p> <pre><code>snakedeploy deploy-workflow https://github.com/zjnolen/PopGLen . --tag v0.4.1\n</code></pre> <p>This will download a <code>workflow</code> and <code>config</code> folder containing a the workflow file and a configuration template. Now we are ready to set up our dataset.</p>"},{"location":"tutorial/#prepping-the-dataset","title":"Prepping the dataset","text":"<p>If you are following along with the tutorial data, please download it from Figshare here. You can download it in the command line using the following command:</p> <pre><code>wget -O popglen-tut-data.zip https://figshare.com/ndownloader/articles/27453978/versions/1\n</code></pre> <p>This dataset contains three folders: <code>data</code>, <code>config</code>, and <code>resources</code>. These contain the raw fastq reads, configuration files, and subset reference genome, respectively. Move these three folders into the base of your working directory, where we deployed the workflow. You'll be replacing the contents of the <code>config</code> folder that was deployed with the workflow with what's in the <code>config</code> in the tutorial data.</p> Example set up code for tutorial <p>Here is an example of how the tutorial can be deployed, with explicit commands.</p> <pre><code># First, create a working dir\nmkdir popglen-tutorial\n\n# Change into it\ncd popglen-tutorial\n\n# deploy the workflow in here\nsnakedeploy deploy-workflow https://github.com/zjnolen/PopGLen . --tag v0.4.1\n\n# download the tutorial data\nwget -O popglen-tut-data.zip https://figshare.com/ndownloader/articles/27453978/versions/1\n\n# unzip the tutorial data\nunzip popglen-tut-data.zip\n\n# move the contents of the tutorial data folder into the working dir\nmv popglen-tutorial-data/data popglen-tutorial-data/resources ./\nmv popglen-tutorial-data/config/* config/\n\n# remove the zip file and unzipped folder\nrm -r popglen-tutorial-data popglen-tutorial-data.zip\n</code></pre> <p>This should get you set up as expected.</p> <p>Let's take a look at the configuration files in the tutorial data. First, our sample list:</p> config/samples.tsv<pre><code>sample\tpopulation\ttime\tdepth\nMZLU153246\tESkane1956\thistorical\tlow\nMZLU153247\tESkane1956\thistorical\tlow\nMZLU153248\tESkane1956\thistorical\tlow\nMZLU153251\tESkane1956\thistorical\tlow\nMZLU153250\tESkane1956\thistorical\tlow\nMZLU153204\tWSkane1951\thistorical\tlow\nMZLU153205\tWSkane1951\thistorical\tlow\nMZLU153206\tWSkane1951\thistorical\tlow\nMZLU153216\tWSkane1951\thistorical\tlow\nMZLU153221\tWSkane1951\thistorical\tlow\nMZLU107458\tESkane2021\tmodern\thigh\nMZLU107463\tESkane2021\tmodern\thigh\nMZLU107503\tESkane2021\tmodern\thigh\nMZLU107504\tESkane2021\tmodern\thigh\nMZLU107507\tESkane2021\tmodern\thigh\nMZLU107457\tWSkane2021\tmodern\thigh\nMZLU107460\tWSkane2021\tmodern\thigh\nMZLU107497\tWSkane2021\tmodern\thigh\nMZLU107498\tWSkane2021\tmodern\thigh\nMZLU107500\tWSkane2021\tmodern\thigh\n</code></pre> <p>Here, we list all the samples we will use, the population identifiers we will group them by, the time period they come from, and the sequencing depth category we will assign them to. There is one line per sample, and there can be as many or as few population groups as you'd like.</p> <p>Time must be 'modern' or 'historical' and decides how samples will be treated when modern and historical samples are treated differently. If all your samples are modern, you can simply leave all as modern.</p> <p>Depth groups samples into depth categories that allow depth filtering to be done per category. This can help to ensure that if you have samples with considerable differences in depth, your depth filters won't be exclusively acting on the upper tail of the distribution driven by high coverage samples and the lower tail driven by low coverage samples. You can have as many or as few depth categories as you want, with whatever names you would like, but they should not use names that also appear in other columns (i.e. use 'low' rather than 'historical' or 'pop1depth' rather than reusing 'pop1' from the population column).</p> <p>Now, for the units list, which points to our data files:</p> config/units.tsv<pre><code>sample\tunit\tlib\tplatform\tfq1\tfq2\nMZLU107457\tAHW5NGDSX2.3\tUDP0046\tILLUMINA\tdata/MZLU107457.R1.sub.fastq.gz\tdata/MZLU107457.R2.sub.fastq.gz\nMZLU107458\tAHW5NGDSX2.3\tUDP0047\tILLUMINA\tdata/MZLU107458.R1.sub.fastq.gz\tdata/MZLU107458.R2.sub.fastq.gz\nMZLU107460\tAHW5NGDSX2.3\tUDP0049\tILLUMINA\tdata/MZLU107460.R1.sub.fastq.gz\tdata/MZLU107460.R2.sub.fastq.gz\nMZLU107463\tAHW5NGDSX2.3\tUDP0052\tILLUMINA\tdata/MZLU107463.R1.sub.fastq.gz\tdata/MZLU107463.R2.sub.fastq.gz\nMZLU107497\tAHW5NGDSX2.3\tUDP0084\tILLUMINA\tdata/MZLU107497.R1.sub.fastq.gz\tdata/MZLU107497.R2.sub.fastq.gz\nMZLU107498\tAHW5NGDSX2.3\tUDP0085\tILLUMINA\tdata/MZLU107498.R1.sub.fastq.gz\tdata/MZLU107498.R2.sub.fastq.gz\nMZLU107500\tAHW5NGDSX2.3\tUDP0087\tILLUMINA\tdata/MZLU107500.R1.sub.fastq.gz\tdata/MZLU107500.R2.sub.fastq.gz\nMZLU107503\tAHW5NGDSX2.3\tUDP0090\tILLUMINA\tdata/MZLU107503.R1.sub.fastq.gz\tdata/MZLU107503.R2.sub.fastq.gz\nMZLU107504\tAHW5NGDSX2.3\tUDP0091\tILLUMINA\tdata/MZLU107504.R1.sub.fastq.gz\tdata/MZLU107504.R2.sub.fastq.gz\nMZLU107507\tAHW5NGDSX2.3\tUDP0094\tILLUMINA\tdata/MZLU107507.R1.sub.fastq.gz\tdata/MZLU107507.R2.sub.fastq.gz\nMZLU153221\tBHVN22DSX2.2\t208188\tILLUMINA\tdata/MZLU153221.R1.sub.fastq.gz\tdata/MZLU153221.R2.sub.fastq.gz\nMZLU153206\tBHVN22DSX2.2\t207139\tILLUMINA\tdata/MZLU153206.R1.sub.fastq.gz\tdata/MZLU153206.R2.sub.fastq.gz\nMZLU153204\tBHVN22DSX2.2\t003091\tILLUMINA\tdata/MZLU153204.R1.sub.fastq.gz\tdata/MZLU153204.R2.sub.fastq.gz\nMZLU153205\tBHVN22DSX2.2\t008096\tILLUMINA\tdata/MZLU153205.R1.sub.fastq.gz\tdata/MZLU153205.R2.sub.fastq.gz\nMZLU153248\tBHVN22DSX2.2\t021130\tILLUMINA\tdata/MZLU153248.R1.sub.fastq.gz\tdata/MZLU153248.R2.sub.fastq.gz\nMZLU153216\tBHVN22DSX2.2\t093148\tILLUMINA\tdata/MZLU153216.R1.sub.fastq.gz\tdata/MZLU153216.R2.sub.fastq.gz\nMZLU153251\tBHVN22DSX2.2\t028116\tILLUMINA\tdata/MZLU153251.R1.sub.fastq.gz\tdata/MZLU153251.R2.sub.fastq.gz\nMZLU153250\tBHVN22DSX2.2\t036137\tILLUMINA\tdata/MZLU153250.R1.sub.fastq.gz\tdata/MZLU153250.R2.sub.fastq.gz\nMZLU153247\tBHVN22DSX2.2\t043143\tILLUMINA\tdata/MZLU153247.R1.sub.fastq.gz\tdata/MZLU153247.R2.sub.fastq.gz\nMZLU153246\tBHVN22DSX2.2\t052142\tILLUMINA\tdata/MZLU153246.R1.sub.fastq.gz\tdata/MZLU153246.R2.sub.fastq.gz\n</code></pre> <p>Here, we point to the data for each sample, and assign sequencing metadata. We are using FASTQ inputs for this tutorial, so we will fill out (1) the 'unit', an identifier for the sequencing lane (here it is the run barcode and lane number), (2) the 'lib', an identifier for the library batch the reads come from, (3) the 'platform', the sequencing platform they were run on, and (4) 'fq1' and 'fq2', the paths to the forward and reverse read fastq files (either absolute or relative to the working directory). You can have multiple lines per sample if you have multiple combinations of sequencing runs and/or libraries per sample, and these will be merged at the appropriate steps by the workflow. You can also use SRA accession numbers to download the FASTQ data if it is not local by replacing the 'fq1' and 'fq2' columns with an 'sra' column and filling it out with the accession number. If we were starting with BAM files, we could omit all the columns except 'sample', and add one column 'bam' that points to the BAM file.</p> <p>Now, we are ready for the configuration.</p>"},{"location":"tutorial/#initial-configuration","title":"Initial configuration","text":"<p>The <code>config/config.yaml</code> file included with the tutorial dataset has the basic configuration filled out. It contains all the available options for the workflow, with some default parameters and most analyses turned off. There is an additional file, <code>config/config-full.yaml</code>, that enables all the tutorial analyses, allowing the entire tutorial to be run in one Snakemake run. For details on all the settings in the config file, please see the configuration documentation.</p> <p>No matter what analyses are to be run, the first two sections of the config are required:</p> config/config.yaml<pre><code>#=====================Dataset Configuration============================#\n\nsamples: config/samples.tsv\n\nunits: config/units.tsv\n\ndataset: \"popglen-tutorial\"\n\n#===================== Reference Configuration ========================#\n\nchunk_size: 12000000\n\nreference:\n  name: \"tutorial-ref\" \n  fasta: \"resources/ref/ilCyaSemi1.1_tutref.fa\"\n  mito: [\"mitochondrion\"]\n  sex-linked: [\"Z\"]\n  exclude: []\n  min_size: 1000000\n\nancestral:\n</code></pre> <p>Here, we set a few important settings:</p> <ul> <li><code>samples</code> points to the sample list file</li> <li><code>units</code> points to the unit list file</li> <li><code>dataset</code> gives a name for the dataset run (multiple datasets can be run in   the same working dir and reuse generated BAMs and reference resources when   possible)</li> <li><code>chunk_size</code> sets a size for the genome to be chunked into. This allows for   parallelizing over genomic regions where possible, but without running into   issues when genomes have a high number of contigs. It should be a value equal   to or greater than the largest contig length in your genome. If you have a   chromosome level genome, the size of the biggest chromosome is probably best.   If you have something less contiguous, it depends on how many jobs you expect   to run in parallel. I have mostly worked with 500Mb genomes, setting this to   30Mb usually. For the tutorial, we will actually set it to a value larger than   the tutorial genome, as it is already rather small, ~10Mb.</li> <li><code>reference/name</code> is a name for the reference genome you're mapping/mapped to.   It will be included in the output file names of the BAM alignments and many   other files.</li> <li><code>reference/fasta</code> is the path to the reference genome FASTA. It must be   locally available and should not be compressed. (Open an issue of using a   compressed reference FASTA is something you'd much prefer and I will look into   making it possible.)</li> <li><code>reference/mito</code> is a list of any mitochondrial scaffolds. They will be   excluded from analyses (future versions may include some mitochondrial   analyses). In the tutorial, we drop the sequence called 'mitochondrion'.</li> <li><code>reference/sex-linked</code> is a list of any sex linked scaffolds. They will be   excluded from analyses (future versions may include ways to analyze these).   In the tutorial, we drop the 'Z' chromosome.</li> <li><code>reference/exclude</code> is a list of scaffolds to exclude that aren't in any other   category of this section. We won't put anything here this time.</li> <li><code>reference/min_size</code> is a minimum size required to include a scaffold. If you   have many small scaffolds, you can set a threshold to drop them. We only have   one small scaffold in the tutorial reference, the mitochondrion, and since it   is smaller than the 1Mb we set here, it will be dropped by this setting, even   if it wasn't listed in the <code>reference/mito</code> section.</li> <li><code>ancestral</code> is a FASTA file containing ancestral states for your reference   genome. Several analyses can utilize ancestral states for polarization of the   site frequency spectrum, providing derived/ancestral allele counts rather than   major/minor. If you have one, you can put it here, but know that analyses that   use it will be limited to the sites with data in this sequence. We don't have   one, so we won't use it and will 'fold' our site frequency spectra to account   for this. The main outputs of the workflow largely are suitable with folded   spectra.</li> </ul>"},{"location":"tutorial/#running-the-pipeline","title":"Running the pipeline","text":"<p>With these basic configurations complete, the pipeline is ready to run. Snakemake allows us to do a dry run with the option <code>-n</code>:</p> <pre><code>snakemake \\\n  --use-conda --use-singularity \\\n  --default-resources \"mem_mb=(threads*1700)\" \\\n  --cores &lt;threads&gt; \\\n  -n\n</code></pre> <p>As this pipeline uses conda environments and singularity containers, <code>--use-conda</code> and <code>--use-singularity</code> are always required. <code>--default-resources</code> is needed for some rules that require knowledge of memory usage (primarily java tools), where we give 1700MB of memory per thread used in each rule. <code>--cores</code> is needed for local runs and should be set to the number of cores you'd like to give to PopGLen for a local run.</p> <p>Running with a job scheduler</p> <p>For this tutorial, I have shown the command for a local run. This is good if you are running on a machine without a job scheduler or if you want to submit the workflow to run inside a single large job on your cluster. The latter is probably the best way to run the tutorial, but for large datasets, it is likely most efficient to use Snakemake's executor plugins to have it submit each job in the workflow as its own job in your scheduler, maximizing the parallelization potential across multiple nodes. While the best place for how to set this up is the Snakemake documentation, I have also made a brief doc on this here.</p> <p>When we do a dry run, we should see that with the default options, Snakemake plans to do only the raw sequence data processing, i.e. adapter trimming, collapsing of historical read pairs, mapping to the reference, duplicate removal, indel realignment, clipping of overlapping reads for modern samples, and indexing of the final processed BAMs.</p>"},{"location":"tutorial/#quick-start-vs-walkthrough","title":"Quick start vs. Walkthrough","text":"<p>I've written the rest of this tutorial with two approaches in mind: a 'quick start' and a 'walkthrough'.</p> <p>The quick start approach starts with a configuration with all analyses enabled. This means Snakemake will run the entire tutorial in a single run, you can then examine the outputs to see what can be done using this pipeline. I recommend this if you are familiar with a lot of the analyses and with Snakemake and can use the configuration documentation yourself to adjust settings for your data.</p> <p>The walkthrough approach starts with a configuration with most analyses disabled and walks through enabling analyses and running the pipeline piece by piece. This may help some who are less familiar with the tools to understand how the analyses fit together.</p> <p>Regardless of the approach you use for the tutorial, realize that the settings in the tutorial may not work as well for other datasets, so look into them and update them for your needs when using your own data. Please feel to reach out in the issues tracker with questions or to suggest ways the pipeline may be improved to suit a greater breadth of datasets.</p>"},{"location":"tutorial/#option-1-quick-start","title":"Option 1: Quick Start","text":"<p>For this, we will perform a whole run using the <code>config/config_full.yaml</code>. Your Snakemake command should look something like this for a local run:</p> <pre><code>snakemake \\\n  --configfile config/config_full.yaml \\\n  --use-conda --use-singularity \\\n  --default-resources \"mem_mb=threads*1700\" \\\n  --cores &lt;threads&gt;\n</code></pre> <p>and should be similar, but with cluster/executor configuration options, if using a system with a job scheduler like SLURM. Note the <code>--configfile</code> option, which overrides the automatic config path (<code>config/config.yaml</code>) with the one we put here. Feel free to do a dry run with <code>-n</code> first to see what the pipeline plans to run.</p> How long does the tutorial take to run? <p>This will depend on the system it is run on, as both the number of available CPUs and the performance of those CPUs will impact the results. I tested it in two environments: (1) in an Ubuntu VM on a 2020 Macbook Pro with 8 cores and (2) inside a single job on PDC's Dardel cluster with 64 cores (this is half of a node on there). On the Macbook it took ~4 hours to complete the whole tutorial, while on Dardel it completed in under half an hour. The difference is largely from the number of cores available to perform analyses in parallel, but also as the performance of individual cores on the Macbook is a bit weaker. Note: I didn't include downloading the Singularity containers and setting up the conda environments in this estimate.</p> <p>First, Snakemake solves the directed action graph - the workflow required to get from the input files present on the system to the output files requested by our configuration. Once solved, the required containers and conda environments are set up and the workflow will begin, with outputs from each step flowing into the input of the next. Upon completion, we can make a report:</p> <pre><code>snakemake \\\n  --configfile config/config_full.yaml \\\n  --use-conda --use-singularity \\\n  --report tutorial_report.html\n</code></pre> <p>This report contains the main results of all the major analyses, including both tables and plots. The actual results can be found in the 'results' folder, which contains the folders 'datasets', 'mapping', 'preprocessing', and 'ref', which have outputs from the population genomic analyses for different datasets (each with a subfolder containing the dataset name, for the tutorial it is just one, which we named <code>popglen-tutorial</code> in the config), outputs from the mapping process, outputs from the trimming process, and outputs specific to the reference (indices, repeat libraries, etc.), respectively.</p>"},{"location":"tutorial/#option-2-walkthrough","title":"Option 2: Walkthrough","text":"<p>It can be useful to run analyses in batches, assess their outputs, and use these to inform set up for subsequent analyses. For this walkthrough, we will gradually change <code>config/config.yaml</code> to perform analyses, pointing out relevant settings in the file as we go. With the first sections matching the versions shown in Initial Configuration, we are ready to start the walkthrough. In each step, a report will be made with the results. These reports are included in this tutorial, so you can just click through them if you'd rather not run the tutorial on your end.</p> <p>A note on temporary files</p> <p>I have flagged several intermediate files in this workflow as temporary so that they will be cleaned up when they are no longer needed. While this is great when you're all done with them, if you are enabling pieces of the workflow at a time as we are in this tutorial, it can lead to intermediate files disappearing after some steps and needing to be re-generated when we enable other analyses that use them later down the line. While I've tried to avoid making most 'keystone' intermediate files (Beagles, SAFs, etc.) temporary, it can be good to use the <code>--notemp</code> option in <code>snakemake</code> commands, which will keep any files marked as temporary. Then, when you know you're done, you can run it again without this option to clean up the intermediate files.</p>"},{"location":"tutorial/#raw-sequence-data-processing-and-quality-control","title":"Raw sequence data processing and quality control","text":"I already have BAMs for my data, do I need this part? <p>If you already have BAMs this part may still be relevant to you, as it also will run some quality control analyses on the BAM files. I recommend quickly reading through it to see the QC options. To make the tutorial more similar to what processing your data would be like, I've included the BAM files in the tutorial dataset, so you can start from there. Just change the <code>units.tsv</code> file to look like this:</p> config/units.tsv<pre><code>sample\tbam\nMZLU107457\tdata/bams/MZLU107457.tutorial-ref.bam\nMZLU107458\tdata/bams/MZLU107458.tutorial-ref.bam\nMZLU107460\tdata/bams/MZLU107460.tutorial-ref.bam\nMZLU107463\tdata/bams/MZLU107463.tutorial-ref.bam\nMZLU107497\tdata/bams/MZLU107497.tutorial-ref.bam\nMZLU107498\tdata/bams/MZLU107498.tutorial-ref.bam\nMZLU107500\tdata/bams/MZLU107500.tutorial-ref.bam\nMZLU107503\tdata/bams/MZLU107503.tutorial-ref.bam\nMZLU107504\tdata/bams/MZLU107504.tutorial-ref.bam\nMZLU107507\tdata/bams/MZLU107507.tutorial-ref.bam\nMZLU153221\tdata/bams/MZLU153221.tutorial-ref.bam\nMZLU153206\tdata/bams/MZLU153206.tutorial-ref.bam\nMZLU153204\tdata/bams/MZLU153204.tutorial-ref.bam\nMZLU153205\tdata/bams/MZLU153205.tutorial-ref.bam\nMZLU153248\tdata/bams/MZLU153248.tutorial-ref.bam\nMZLU153216\tdata/bams/MZLU153216.tutorial-ref.bam\nMZLU153251\tdata/bams/MZLU153251.tutorial-ref.bam\nMZLU153250\tdata/bams/MZLU153250.tutorial-ref.bam\nMZLU153247\tdata/bams/MZLU153247.tutorial-ref.bam\nMZLU153246\tdata/bams/MZLU153246.tutorial-ref.bam\n</code></pre> <p>or, when you run the Snakemake command, you can override the units file configuration by adding <code>--config units=\"config/units_bams.tsv</code>.</p> <p>If you use this units list and run this section of the tutorial, no mapping will be done, only the QC, so it'll be a lot faster to run this section.</p> <p>The first stage of the pipeline will be to map the raw reads and perform a few quality control analyses. The mapping pipeline roughly looks like this for each sample type:</p> <p>Modern:</p> <ol> <li>Quality and adapter trim paired end reads using defaults with fastp</li> <li>Map paired end reads for each sample with bwa mem. Do this individually for    separate sequencing runs and libraries, then merge with samtools.</li> <li>Remove duplicates with Picard MarkDuplicates.</li> <li>Realign around insertions/deletions with GATK IndelRealigner.</li> <li>Clip overlapping read pairs to avoid double counting reads in ANGSD using    bamutil.</li> </ol> <p>Historical:</p> <ol> <li>Collapse overlapping paired end reads into single strand with fastp.</li> <li>Map collapsed reads to reference with bwa aln.</li> <li>Remove duplicates per library with dedup and merge libraries.</li> <li>Realign around insertions/deletions with GATK IndelRealigner.</li> <li>(Optional, we will not do this for the tutorial) Rescale base quality scores    to account for DNA damage with mapdamage2.</li> </ol> <p>Use the code blocks below to update the appropriate lines in the <code>config/config.yaml</code> file to set this run up. Since mapping is always enabled in PopGLen if starting from FASTQ, we only have to check some settings for the mapping and turn on some QC analyses:</p> config/config.yaml<pre><code>analyses:\n  # mapping options\n  mapping:\n    historical_only_collapsed: true\n    historical_collapsed_aligner: \"aln\"\n</code></pre> config/config.yaml<pre><code>  # quality control\n  qualimap: true\n  ibs_ref_bias: true\n  damageprofiler: true\n  mapdamage_rescale:\n</code></pre> <p>The first block describes settings for historical sample mapping and determines that we should only map reads with some overlap (i.e. fragments &lt;270bp) and we should use bwa aln to align historical reads rather than bwa mem. We will leave them this way for the tutorial, but you can change them if needed for your data.</p> <p>The next block enables some quality control analyses:</p> <ul> <li><code>qualimap</code> - a detailed report of mapping statistics</li> <li><code>ibs_ref_bias</code> - calculates the average identity state distance to the   reference for each sample using a custom script around ANGSD's -doIBS option</li> <li><code>damageprofiler</code> - estimate DNA damage on historical samples</li> <li><code>mapdamage_rescale</code> - rescale base quality scores for potentially damaged   bases in historical samples (adds step to BAM generation)</li> </ul> <p>We will turn on the first three options to get a good batch of quality control options for our data. While we could also use MapDamage to rescale the base quality scores of putatively damaged reads, we will handle DNA damage in a different way down the line. In this dataset, we find MapDamage correction can cause a substantial reference bias in historical samples. For this reason, it can be useful to check the results of <code>ibs_ref_bias</code> with and without rescaling enabled to determine the effect on your samples. Estimates of mean sequencing depth and mapping rates are always enabled in PopGLen, so don't need to be turned on.</p> Additional sequence processing settings <p>There are several settings available at the end of the configuration file for each of the tools in the PopGLen workflow. For the tutorial, these are already set to suitable defaults (and these should be suitable for most datasets using sequencing data generated by patterned flow cells), but you may want to look into them in the configuration docs to adapt them to your dataset.</p> config/config.yaml<pre><code>params:\n  clipoverlap:\n    clip_user_provided_bams: true\n  fastp:\n    extra: \"-p -g\" # don't put --merge or --overlap_len_require here, they're implicit\n    min_overlap_hist: 30\n  bwa_aln:\n    extra: \"-l 16500 -n 0.01 -o 2\"\n  picard:\n    MarkDuplicates: \"--REMOVE_DUPLICATES true --OPTICAL_DUPLICATE_PIXEL_DISTANCE 2500\"\n  damageprofiler:\n    profile_modern: false\n</code></pre> <p>Now we are ready to run our processing, its best to do a dry run first to make sure everything looks good. This run will likely take the longest of all the runs we will perform, as it involves all the mapping steps and will do a lot of the initial software installs.</p> <p>Once the run is complete, the easiest way to look at the results is to generate a report:</p> <pre><code>snakemake \\\n  --configfile config/config_full.yaml \\\n  --use-conda --use-singularity \\\n  --report tutorial_report.html\n</code></pre> <p>Here is an example for the tutorial report you should get after running these steps.</p> <p>You'll have quite a few QC outputs:</p> <ul> <li>fastp trimming reports per sample and combined in a multiqc file</li> <li>A filtering summary describing how much of genome will be used in downstream   analyses (for now, only removing small contigs and non-autosomal chrs)</li> <li>A multiqc DNA damage report for the historical samples from DamageProfiler</li> <li>A multiqc report of the Qualimap reports for all samples</li> <li>A table of endogenous content (mapping rate) and sequencing depth per sample</li> <li>Boxplots and tables of the identity by state distance to the reference for   all samples in various groupings.</li> </ul> <p>Explore the report outputs, but remember this is a tutorial dataset, so it may not be a great representation of 'good' data. As the first four are basically reports from their respective tools, we will only look at the last two.</p> <p>First, let's open the section 'Sample depth and endogenous content'. This is a searchable, sortable, adjustable table containing sample names and meta-data, the mapping rate for collapsed reads, uncollapsed reads, and all reads, as well as the mean and standard deviation of sequencing depth calculated three ways. Notice that modern samples only have uncollapsed mapping rates and historical only have collapsed because of the different processing paths of the sample types.</p> <p>The three depth metrics are calculated with no filtering (<code>allsites-unfilt.depth</code>), with only mapping and base quality filtering (<code>allsites-mapq30-baseq30-filt.depth</code>), and with the mapping and base quality filters plus the sites filters (<code>allsites-filts.depth</code>). We'll get more into these filters in the next section. The main thing to see here is that we can have different depths depending on how we calculate it. I tend to care about the last one the most, this is the sequencing depth at the sites you will use in your analyses.</p> <p>Next, let's look at the IBS results. This will help us see if the short fragment length of our historical samples has led to reference bias. For this, we will open the plot in the '7 Sample IBS to reference (ref bias)' section, looking at the one with 'allsites-filts' for the filtering and 'Time' for the grouping:</p> <p></p> <p>It would appear not, in fact, they are further from the reference. This is expected for two reasons: (1) we haven't corrected for DNA damage, so some excess differences are expected due to damage and (2) the historical samples are more distant in time from the reference than the modern and we will observe the genetic drift over that time as differences. This second point will likely depend on the sampling of your reference. Either way, we do not see evidence that we face reference bias due to the historical samples' small fragment size.</p> Is this useful if I only have modern samples? <p>Yes, this can help you determine if there is some potential sources of reference bias by population (see the other grouping boxplots), or if some samples seem to be especially strange, potentially suggesting that there is something off in their underlying data. For instance, a sample with considerably lower IBS than the rest may indicate a misidentified species or a contaminated sample.</p>"},{"location":"tutorial/#sites-filters","title":"Sites filters","text":"<p>When doing analyses, only reads passing the base and mapping quality thresholds in the config file can be used, which are set here:</p> config/config.yaml<pre><code>mapQ: 30\nbaseQ: 20\n</code></pre> <p>By default I've set mapping quality to 30 (representing a 1 in 1000 error rate), and base quality to 20 (representing a 1 in 100 error rate), which are common cutoffs, but can be adjusted based on what you feel suits your data. If you want to investigate the distributions of these scores in your data, consider checking the Qualimap reports (I don't believe this is in the MultiQC in the Snakemake report, but can be found in the individual sample reports in <code>results/mapping/qc/qualimap</code> if you've aligned your data with PopGLen or <code>results/datasets/{dataset}/qc/user-provided-bams/qualimap/</code> if you've brought your own BAMs).</p> <p>An important component of this workflow is the site based filtering. To do this, we limit our analyses to a list of 'good sites' determined by a set of filters. This filtering scheme was inspired by similar sites based filtering used in studies using ANGSD (e.g. Pe\u010dnerov\u00e1 et al. 2021, Curr. Biol. and Quinn et al. 2023, Mol. Ecol.). With everything disabled as we have it now, you should already have a sites file summarized in the report (under '3 Filtering Summary'), but it will only describe the removal of the contigs we specified in the beginning of the config file:</p> <p></p> <p>So, how else can we filter? We have a few options that can be enabled:</p> config/config.yaml<pre><code>  # sites file filters\n  pileup-mappability: true\n  repeatmasker:\n    bed:\n    local_lib:\n    build_lib:\n  extreme_depth: true\n  dataset_missing_data:\n  population_missing_data:\n</code></pre> <p>The above section of the config file handles enabling of the main filter sets. For this tutorial, we will turn on two as shown above:</p> <ul> <li><code>pileup-mappability</code> uses GenMap to calculate mappability scores for each base   and we use a custom script to calculate the average mappability of each site   for all overlapping fragments. This helps us to remove sites that are   difficult to map to due to being a part of sequences that are frequently   repeated.</li> <li><code>extreme_depth</code> removes sites that have extremely high or low global   sequencing depth across your dataset and/or the depth categories in your   sample list.</li> </ul> <p>In addition to these two filters, there are three others. First, we could filter out repeats, with three built-in ways to do this. One is to provide a BED file with the coordinates of repeats to be filtered out. The second is to use a local repeat library FASTA file to identify repeats with RepeatMasker. The third is to use RepeatModeler to build a repeat library de novo from the reference, then use RepeatMasker to use that library to identify the repeat regions</p> Repeat filter vs mappability filter <p>Generally, the repeat filter and the pileup mappability filter have a lot of overlap, so I only enable one usually. In this tutorial, we use the pileup mappability filter as it takes less time to run, but for your data you may wish to use the repeat based filters. For these, you should use the option highest on the list that you have the resources to use. If you have a BED of repeats, use that as it will take no time at all. If you only have a repeat library, then use the <code>local_lib</code> option. If you don't have anything, then you should build your own, but this often can take days. See the configuration docs for how to set these filters.</p> <p>The remaining two options allow you to filter out sites based on data missingness, removing sites with no usable reads in too many samples in the dataset or in any population. I do not recommend using these unless you know you need them (i.e. you require identical sites to be used across all analyses). Instead, it is better to set the missing data filter in ANGSD as is standard practice (and available later in configuration).</p> Alternate ways to filter extreme depth <p>We aren't diving into configuring the extreme depth filter, leaving it set to keep only positions between 0.5x and 1.5x the global median depth. You can customize these multipliers or set percentiles as filters instead. See the configuration docs for more details on this.</p> <p>All right, with that set up, let's run the workflow again. You'll see some rules are being rerun if you do a dry run. This is because with our new filters, calculations made on filtered sites must be redone. Snakemake is good about recognizing these changes, and I have tried to ensure that this pipeline is compatible with all of them, but please open an issue if you find they are not triggering when expected.</p> <p>If you make the report again (here is an example for this point in the tutorial), you'll find that the table under '3 Filtering Summary' should look more like this now:</p> <p></p> <p>As you can see, we've added a few lines, pileup mappability, and various depth filters. These are all calculated independently across the genome, and you can see how much of the genome passes each filter. By far mappability cuts out the most, and that's because we have a good bit of the genome being repetitive content that we can't reliably map to. Ultimately, we are using ~22% of the 'genome' for our analyses, but we can be quite confident in our ability to genotype that 22%.</p> <p>You might also want to take a look at the '6 Sample depth and endogenous content' section again. Before, the mean depths for <code>allsites-mapq30-baseq30-filt.depth</code> and <code>allsites-filts.depth</code> were quite similar. This is because the first applies only the mapping and base quality filters and the second applies these, plus the sites filter. Before this step, our sites filter barely did anything, but now it removes quite a bit. Now there is a slightly larger difference between the two measures, and the depth at our filtered sites is on average higher than we get with just the mapping and base quality filters. There's also a lower standard deviation, suggesting that we have successfully removed repeats that might get really variable mapping rates.</p> Can I use my own filters? <p>If you have extra filters you want to add, you can provide them to the pipeline in BED or GFF format. These files should contain the sites you want included. These are added in the filter sets section of the config:</p> config/config.yaml<pre><code>#=========================== Filter Sets ==============================#\n\nfilter_beds:\n  neutral: \"resources/neutral_sites.bed\"\n\nonly_filter_beds: false\n</code></pre> <p>In this example, we add a BED file containing neutral sites as a filter. This is intersected without our any filters used in PopGLen to create a filter scheme called <code>neutral-filts</code> in addition to the <code>allsites-filts</code> one generated by default and analyses will all be run once for each filter scheme. If you enable <code>only_filter_beds</code> and turn off any of the filters in PopGLen, you will essentially only use your own filter set. For more info, see the configuration docs for more details on these settings.</p>"},{"location":"tutorial/#population-genomic-analyses","title":"Population genomic analyses","text":"<p>Next, we will enable some population genomic analyses. We will do this slowly, enabling a few analyses at a time and looking at results before moving to the next.</p> <p>Note</p> <p>I often enable many analyses at once, or group together different analyses. See the section at the end of this tutorial for my recommendations on reasonable batches to run your analyses in that should get most of your results generated within a couple runs of PopGLen.</p>"},{"location":"tutorial/#angsd-settings","title":"ANGSD Settings","text":"<p>As most of these analyses are run with ANGSD or from genotype likelihoods generated in ANGSD, we will first take a look at the settings that are going to be passed to ANGSD:</p> config/config.yaml<pre><code>  angsd:\n    gl_model: 2 # gl_model - 1=SAMtools, 2=GATK, 3=SOAPsnp, 4=SYK\n    maxdepth: 100 # for depth calc only\n    mindepthind: 1\n    minind_dataset: 0.75 # as a proportion of N\n    minind_pop: 0.75 # as a proportion of N\n    rmtrans: true\n    extra: \"\" # goes to all ANGSD runs\n    extra_saf: \"\" # goes to all -doSaf runs\n    extra_beagle: \"\" # goes to all -doGlf 2 runs\n    domajorminor: 1 # Currently only compatible with values 1, 2, 4, 5\n    domaf: 1\n    snp_pval: \"1e-6\"\n    min_maf: 0.05 # Set -1 to disable\n    mindepthind_heterozygosity: 1\n</code></pre> <p>Users familiar with ANGSD will likely be able to determine what these options entail:</p> <ul> <li><code>gl_model</code> corresponds to <code>-GL</code> and sets what model to use to calculate   likelihoods. SAMtools and GATK are most popular, I mostly use GATK,   though see ANGSD's docs for a comparison.</li> <li><code>maxdepth</code> corresponds to <code>-maxDepth</code> on <code>-doDepth</code> calls, and sets the   highest depth histogram bin for single sample depth estimates. The same will   be set to this value times the number of samples for global depth calculation.   (ANGSD docs).</li> <li><code>mindepthind</code> corresponds to <code>-setMinDepthInd</code> and is applied at all times,   setting the minimum depth to include a site for an individual (1 is ANGSD   default, ANGSD docs).</li> <li><code>minind_dataset</code> is a proportion that will be multiplied by the number of   samples in the dataset and fed to <code>-minInd</code> for analyses that include the   whole dataset (i.e. PCA, Admix, IBS matrix). Sites with data for fewer than   this proportion of individuals are excluded from these analyses   (docs).</li> <li><code>minind_pop</code> is a proportion that will be multiplied by the number of   samples in a population fed to <code>-minInd</code> for analyses that include only that   population (i.e. thetas, SFS, F<sub>ST</sub>, runs of homozygosity). Sites with  data   for fewer than this proportion of individuals are excluded from the analysis   for that population (docs).</li> <li><code>rmtrans</code> determines whether transitions should be excluded as a means to   account for DNA damage in historical samples. This is applied across all   analyses using the correct option (<code>-rmTrans</code> for beagle files and <code>-noTrans</code>   for saf files,   docs for saf and   docs for beagle).</li> <li><code>extra*</code> are various extra parameters to pass to ANGSD at all times (<code>extra</code>)   or for certain analyses (<code>extra_saf</code> and <code>extra_beagle</code>). This is mostly   useful for options like <code>-trim</code> which can help correct for DNA damage or   <code>-baq</code> if you want to run baq recalibration on your quality scores. Mapping   and base quality filters are automatically added and need not be specified.   Leaving these blank is equivalent to ANGSD default filtering as seen   here. Note that though   many studies enable <code>-uniqueOnly 1</code>, it is unneeded here as BWA is used for   mapping, which sets multi-mapping reads to a mapping quality of 0.</li> </ul> A note about the <code>-baq</code> and <code>-C</code> options <p>I have noticed that the commonly used ANGSD options of <code>-baq 1 -C 50</code> (which themselves are originally from SAMtools) create a sequencing depth bias in my results when I have large differences in depth between samples. This has occurred across multiple datasets I've worked containing different species. Consider leaving these out, or at least checking that this does not apply to your data.</p> <p></p> <ul> <li><code>domajorminor</code> corresponds to <code>-doMajorMinor</code> and is used to set how major and   minor alleles are inferred for relevant analyses   (docs).</li> <li><code>domaf</code> correspons to <code>-doMaf</code> and sets how allele frequencies are inferred   where relevant (docs).</li> <li><code>snp_pval</code> corresponds to <code>-SNP_pval</code> and sets the p-value threshold for   calling a site as a SNP. 1e-6 is commonly used   (docs)</li> <li><code>min_maf</code> corresponds to <code>-minMaf</code> and determines the minimum minor allele   frequency required to include a SNP in analyses   (docs).</li> <li><code>mindepthind_heterozygosity</code> corresponds to <code>-setMinDepthInd</code> but only when   estimating individual heterozygosity. This is useful if you only want to   consider sites with a certain depth when estimating heterozygosity, which can   improve the reliability of estimates.</li> </ul> <p>Most of this will be kept to our defaults for this tutorial and the defaults are meant to match the average settings I've found for studies that utilize ANGSD in the ways PopGLen targets. The only changes we make are setting <code>rmtrans</code> to <code>true</code>, and <code>min_maf</code> to <code>0.05</code>. The first is to account for post-mortem DNA damage expected in our historical samples. For datasets with no historical samples or that correct via another method (such as MapDamage or using <code>-trim</code>), this can be kept on <code>false</code>. We set a minor allele frequency filter to remove low frequency variants from SNP based analyses. Whether you use this filter, and what value you set, will depend on your dataset's needs. Just remember that this only controls the filtering at SNP calling, and downstream tools may filter minor allele frequency as well (for instance, by default, NGSadmix and PCAngsd set a MAF filter of 0.05).</p> <p>These settings simply get us ready to run analyses, so we've added no new analyses. However, if you'd like, you can rerun now and see that some rules will be rerun. This is because they now will exclude transitions. This can be especially useful to look at in the IBS to reference QC results.</p>"},{"location":"tutorial/#linkage-disequilibrium","title":"Linkage Disequilibrium","text":"<p>First, we will look at linkage disequilibrium, as it can help inform some settings down the line, such as sliding window size and linkage pruning settings:</p> config/config.yaml<pre><code>  # population genomic analyses\n  estimate_ld:\n  ld_decay: true\n</code></pre> <p>Here, we will enable just the LD decay estimation with ngsLD. This plots a decay curve showing how linkage disequilibrium decreases with distance between SNPs. We can additionally enable the first option if we want linkage disequilibrium estimates for a random sample of the genome.</p> ngsLD configuration <p>There are a few parameters for the LD estimation available in the config file that we won't dive into here, and I've set the tutorial up to run quick rather than get the most robust estimates. Check the configuration docs for details.</p> <p>If we run now, genotype likelihoods will be estimated and SNPs called for the dataset and per population and handed off to ngsLD to do the LD analyses. Once completed, we can make another report (here is an example of the report at this stage). Inside you should find plots like below in the '01 Linkage Disequilibrium Decay' section:</p> <p></p> <p></p> <p>It seems to decay pretty fast in the dataset (top), and a bit slower with higher background LD in each population (bottom), which is expected as individuals in a population likely are more closely related on average than individuals across the dataset, assuming the dataset is not completely panmictic. With this knowledge, we might choose a window size of 25kb for our window based analyses and consider SNPs as independent after 25kb or below 0.2 r<sup>2</sup> for the dataset and after 25kb or below 0.4 r<sup>2</sup> for a population when pruning. We will look at these when we set up the respective analyses.</p>"},{"location":"tutorial/#principal-component-analysis","title":"Principal Component Analysis","text":"<p>Principal Component Analysis allows us to explore population genetic structure. As it assumes independence of sites, we will need to prune our SNP data first. The pipeline will use ngsLD and prune_graph to prune the Beagle file and then estimate a covariance matrix using PCAngsd and plot the results. To set up the run, enable it in your config and make sure the pruning settings are as we want for the dataset:</p> config/config.yaml<pre><code>  # population genomic analyses\n  estimate_ld:\n  ld_decay: true\n  pca_pcangsd: true\n</code></pre> config/config.yaml<pre><code>  ngsld:\n    max_kb_dist_est-ld: 4000\n    rnd_sample_est-ld: 0.001\n    max_kb_dist_decay: 100\n    rnd_sample_decay: 0.01\n    fit_LDdecay_extra: \"--fit_level 100 --fit_boot 100 --plot_size 3,6 --plot_data --plot_no_legend --plot_y_lim 0.5\"\n    fit_LDdecay_n_correction: true\n    max_kb_dist_pruning_dataset: 25 # used for PCA, Admix, not ngsF-HMM\n    pruning_min-weight_dataset: 0.2 # used for PCA, Admix, not ngsF-HMM\n</code></pre> <p>Now, we can run the pipeline again. Notice that we won't have to make the Beagle file, it has already been made for the LD analysis. Instead, we will go straight to pruning. Once the run is complete, you can make the report (example here), and we will have plots of PC1/2 and PC3/4:</p> <p></p> <p>There seems to be some population structure, with the modern Western Sk\u00e5ne population standing out the most. However, there is something strange here: two samples tightly clustered at the edge of PC1. This can commonly happen when there are close relatives in our dataset, and we should have actually checked for this first, as genomic PCAs assume there are not. I wanted to illustrate what this might look like, but we can easily remove them using our config file. First, however, we should identify which samples are related.</p>"},{"location":"tutorial/#relatedness","title":"Relatedness","text":"<p>Two ways to estimate relatedness are currently available in PopGLen, IBSrelate (Waples et al. 2019, Mol. Ecol.) and NgsRelate v2 (Hangh\u00f8j et al. 2019, GigaScience). For IBSrelate, both the IBS-based and SFS-based method implemented in ANGSD are available. The SFS-based method can also be run from ngsRelate using SNP data. Finally, if population samples are large enough to estimate allele frequencies, the allele frequency based method of NgsRelate is also available. You can enable as many of these as you want in the following lines, but for the tutorial we will perform the SFS-based IBSrelate method as it is implemented in NgsRelate.</p> config/config.yaml<pre><code>  relatedness:\n    ibsrelate_ibs:\n    ibsrelate_sfs:\n    ngsrelate_ibsrelate-only: true\n    ngsrelate_freqbased:\n</code></pre> <p>This is a relatively fast way to assess relatedness, and we already have the Beagle file ready to do it (currently in parallel processed chunks), so you'll see there's really only merging of the chunks and running ngsRelate to be done.</p> <p>Let's generate a report (find an example here) and look under '02 Relatedness'. There we will have a table. To find the most related individuals, sort it by the KING-kinship coefficient:</p> <p></p> <p>These coefficients are going to be a bit off for some individuals, as we are using such a small slice of the actual genome (&lt;1%). However, we are still able to see two relatively highly related pairs of individuals. The first are two individuals from WSkane1951, the ones that are likely skewing our PCA. If we look at the report using the full genome data, we can see that these are full siblings (expected KING = 0.25), so we are going to want to remove one from the PC and Admix analyses. There's also another pair with a high coefficient from WSkane2021. These actually are not very related (using the full data we see KING = 0.004), but by chance have high IBS similarity in this subset of the genome. As that's the data we are using for our analyses, we will remove one of them too. We can remove related individuals from the PCA and Admix by modifying the following option in the config:</p> config/config.yaml<pre><code>#===================== Sample Set Configuration =======================#\n\nexclude_ind: []\n\nexcl_pca-admix: [\"MZLU153216\", \"MZLU107497\"]\n</code></pre> <p>Only one individual needs to be removed from each pair of relatives. Let's re-run with this configuration to see how the PCA changes. Only the PCA will have to be re-run, after the individuals are dropped from the input Beagle file. This would generate a plot more like the following [example report]:</p> <p></p> <p>In this example we only see weak structure, but this may be due to our tutorial dataset having little data to infer this. Looking at the full data helps us see things a bit better, and is more what you might expect from a real dataset:</p> <p></p>"},{"location":"tutorial/#admixture","title":"Admixture","text":"<p>As a way to more formally look into population clustering, we can use admixture analysis, for which we will use NGSadmix. This tool uses the same pruned, relative removed Beagle file as PCAngsd. Turn it on with the following option:</p> config/config.yaml<pre><code>  # population genomic analyses\n  estimate_ld:\n  ld_decay: true\n  pca_pcangsd: true\n  admix_ngsadmix: true\n</code></pre> <p>PopGLen uses a custom wrapper to run up to 100 replicates per K of NGSadmix (Skotte et al. 2013; Genetics), and estimate the best K, using the method described by Pe\u010dnerov\u00e1 et al. (2021, Curr. Biol.). In this method, the best K is the highest K in which the three highest likelihood replicates converge (&lt;2 log likelihood unit difference by default). Additionally, we use EvalAdmix (Garcia-Erill &amp; Albrechtsen; Mol. Ecol. Res.) to plot the correlation of residuals for each value of K as another means to assess fit. Often, limited sampling prevents higher values of K from converging, but EvalAdmix may help determine how well a model fits even if sampling limits the ability to converge at higher Ks.</p> <p>We can set which values of K to run, and customize the convergence criteria in the config file, but the settings included are good for this tutorial:</p> config/config.yaml<pre><code>  ngsadmix:\n    kvalues: [1,2,3,4,5]\n    reps: 100\n    minreps: 20\n    thresh: 2\n    conv: 3\n    extra: \"-maxiter 4000\"\n</code></pre> <code>-maxiter 4000</code>? <p>It's good to make sure that your replicate runs are finishing because they converge, not because they hit the default maxiter value for NGSadmix. We keep it set to 4000 as it will not increase run times for datasets that easily converge, but have found it enables replicates to converge more reliably for datasets where structure is weak.</p> <p>If we run again now, we will see that only NGSadmix and EvalAdmix need to run. On real data, this can be one of the longest running analyses in PopGLen, so I recommend running it in parallel with others, but for the tutorial its fast enough to run now. After generating a report for the new run (example here), you will find the admixture results from NGSadmix and EvalAdmix under '03.2 Admixture'. The admixture plots for all values of K will be assembled in a plot like the one below:</p> <p></p> <p>In this plot, the K's for which multiple replicates converged will be in bold. This is only K=1 for the tutorial dataset, likely because the population structure here is quite weak and this subset of the genome isn't enough to get a good estimate of it. We can also check the correlation of residuals estimated by EvalAdmix to see how well just one cluster fits the data. In this plot, a positive correlation suggests samples/populations have a shared history that is not captured in the model and a negative suggest they have a more different history than the model suggests.</p> <p></p> <p>These correlations are positive between each population and itself, suggesting individuals within populations share more similar backgrounds than currently modeled. We also have negative correlations between the two historical populations and the modern Western Sk\u00e5ne population, suggesting that they are more distinct than currently modeled. Let's take a look at what the admixture results are when we perform the analysis using the whole genome, rather than just the tutorial subset:</p> <p></p> <p>Here we see that the results converge at K = 3, which divides our dataset into three clusters, one containing all the historical samples and one each for the two modern populations. The correlation of residuals for this K are much closer to zero for both samples and populations, suggesting that this model is quite a good fit for our data:</p> <p></p> <p>Take a look at the documenation for NGSadmix and EvalAdmix for more information on how to use and interpret these tools.</p>"},{"location":"tutorial/#site-frequency-spectra-genetic-diversity-and-fst","title":"Site frequency spectra, genetic diversity, and F<sub>ST</sub>","text":"<p>Let's try enabling a few things at once, to get an idea of what that's like. In this case, there are several metrics that use similar files, so we will work with them. To calculate thetas (genetic diversity metrics) and neutrality statistics, as well as F<sub>ST</sub>, first a SAF file is made for each population, containing the site allele frequencies, which is used to make a site frequency spectrum, which is used as a prior to estimate theta and F<sub>ST</sub> globally and in windows. Individual heterozygosity is estimated by making a SAF file for each individual, generating a single sample SFS, and using the middle value (heterozygous site count) divided by the total number of sites.</p> <p>Let's enable these:</p> config/config.yaml<pre><code>  1dsfs:\n  1dsfs_boot:\n  2dsfs:\n  2dsfs_boot:\n  thetas_angsd: true\n  heterozygosity_angsd: true\n  fst_angsd:\n    populations: true\n    individuals:\n</code></pre> <p>Here, we enable the generation of estimates of Watterson's \u03b8, nucleotide diversity (\u03c0), and Tajima's D (<code>thetas_angsd</code>), individual heterozygosity estimates (<code>heterozygosity_angsd</code>), and F<sub>ST</sub> for all population pairs (<code>fst_angsd/populations</code>). You might for some reason want to do F<sub>ST</sub> for all pairs of individuals, so that's an option too, but we won't do it here.</p> <p>Notice how I have not enabled any of the SFS options. PopGLen knows the SFS is a prerequisite for the latter analyses, so it doesn't need these to be enabled to generate it. If you only wanted the SFS and not the other things, you could turn these on (and it is a good idea to do so, to ensure that the prior going into these analyses looks reasonable). Here, we won't turn them on just as a means of illustrating that intermediate files are generated as needed, even if not requested. There are also options to generate bootstrapped versions of the SFS, which are often used for estimating uncertainty in many demographic modeling tools, but we won't go into that in this tutorial. Check the configuration docs for details on how to set that up.</p> <p>When enabling thetas and F<sub>ST</sub>, we will generate sliding window and global estimates. Let's look into the settings later in the config for this:</p> config/config.yaml<pre><code>  fst:\n    whichFst: 1\n    win_size: 25000\n    win_step: 10000\n  thetas:\n    win_size: 25000\n    win_step: 10000\n    minsites: 1000\n</code></pre> <p>Here, we set a few things. By default we use the Hudson-Bhatia estimator (Bhatia et al. 2013; Genome Res.) to estimate F<sub>ST</sub>, as it is more robust to small and uneven sample sizes (equivalent to <code>-whichFst 1</code> in the command. <code>-whichFst 0</code> is the ANGSD default). For both F<sub>ST</sub> and thetas we set our windows to a size of 25kb, with a 10kb step, based on the rather steep LD decay we saw in our earlier results. With quick LD decay, large window sizes may 'smooth' out local signal too much. However, it is useful to explore different window sizes, so you can easily change these and rerun the pipeline.</p> <p>Thetas have an extra option <code>minsites</code>. This refers to the number of sites a window must have data from to be included in the global theta estimates and the distribution plots generated for the report. Since global F<sub>ST</sub> estimation is made by ANGSD, and we only plot that for the report, not the windows, it is not needed for the pipeline. The window output from ANGSD remains unaltered for both, this setting is just for the reporting by the pipeline.</p> What about polarizing to ancestral states? <p>By default, we are assuming no ancestral states are available, but if you have them in a reference included in your config, you might want to make an additional change in your config to keep the SFS unfolded:</p> config/config.yaml<pre><code>  realsfs:\n    fold: 0 # Should only be set to 1, unless ancestral reference is given above\n    sfsboot: 100\n</code></pre> <p>One last thing for this section, since we are enabling heterozygosity, we also need to look at how many bootstrap replicates we will do for the SFS. These will be used to calculate confidence intervals for the estimates. This can be found in the realSFS section of the config:</p> config/config.yaml<pre><code>  realsfs:\n    fold: 1 # Should only be set to 1, unless ancestral reference is given above\n    sfsboot: 50\n</code></pre> <p>While for this you might want a high number (like 300) for the real data to get good confidence intervals, we will lower it to 50 here, just to make this run a bit faster. Note that currently confidence interval estimation for heterozygosity shares a configuration with bootstrap count for 1D and 2D SFS. Configuring each of these individually can be relatively simply separated out, so please request if desired and I will implement it.</p> <p>All right, now lets run it again, this time, quite a few analyses will process in parallel. Once done, we can make a report (example here) and you'll find a few new sections: '04.1 Watterson's Theta', '04.2 Nucleotide Diversity (Pi)', '04.3 Tajima's D', '04.4 Heterozygosity', and '05 Fst'. For the first three, you will get violin plots illustrating the distribution of windowed estimates for the given metric in each population:</p> <p></p> <p>As we can see, the general distributions of nucleotide diversity and Watterson's theta are lower in the modern populations than the historical, in line with a decline in genetic diversity. We also see that genome-wide estimates of Tajima's D are higher in modern populations, suggesting they've been going through a population bottleneck as rare variants are becoming less common compared to common ones. Note that the general estimates of Tajima's D are negative for all populations, which indicates a population expansion. However, this is expected for many populations in northern Europe, which expanded out of glacial refugia. This highlights how absolute estimates can be misleading, and proper baselines must be used to create context, such as the historical populations we use here.</p> <p>In addition to the violin plots, you'll get a table for each metric with the mean for each population along with 95% confidence intervals estimated from 1000 bootstrap estimates of the mean, resampling windows with replacement:</p> <p></p> <p>Under '04.4 Heterozygosity', we will find some similar files. There will be a table with heterozygosity estimates with confidence intervals for each individual, this time estimating the intervals from the bootstrapped SFS. Two plots are made for heterozygosity - a boxplot of the estimates grouped by population and a plot showing individual estimates with confidence intervals:</p> <p></p> <p>Finally, under '05 Fst', we will have one plot, a heat map of global F<sub>ST</sub> estimates between all population pairs. This plots the weighted F<sub>ST</sub> estimate produced by ANGSD:</p> <p></p> <p>Note that the optimization approach used by ANGSD can sometimes produce F<sub>ST</sub> estimates below zero, but these are a methodological artifact and should be considered equivalent to zero. With that in mind, we can see that the F<sub>ST</sub> between the historical populations is lower than between the modern, suggesting that populations have become more differentiated over the time period. This may be simply because they already were isolated and drift has progressed over the period, or even sped up due to the smaller population sizes. It can also mean that gene flow has become reduced. This possibility is especially likely given our clustering analyses and complete lack of differentiation in historical populations, suggesting that they were well connected in the past.</p> <p>All of these metrics have additionally been performed in windows, which are not included in the report, largely as I find the uses for window based versions of these estimates are quite diverse and likely better summarized using whatever scripts you make yourself. Check the output file summary for information on where you can find these files.</p> Impact of sample size on these metrics <p>Genetic diversity, differentiation and neutrality metrics like nucleotide diversity, Watterson's theta, F<sub>ST</sub>, and Tajima's D can have different responses to sample size. Many studies aim for 8+ samples per population, but variance in sample size can affect these metrics differently. We have tested how close estimates of these metrics can get to the estimate produced with 8 individuals at lower sample sizes (see Figure 4 in Nolen et al. 2024, Biol. Conserv.):</p> <p></p> <p>Nucleotide diversity is most influenced by common variants, which often can be captured at lower sample sizes, so as expected, we found that even just two samples captured similar signal to eight. F<sub>ST</sub> required closer to 4, at least when we used the Bhatia-Hudson estimator. Rare variants are more impactful to Watterson's theta and Tajima's D (which measures the deviation from the ratio of rare and common variants expected under neutrality). As such, the estimates can change considerably with each sample added. We have largely utilized repeated subsamples of populations to equivalent size to assess the variation in estimates driven by sample size, and code extending PopGLen to do this can be found in the above mentioned manuscript. If such subsampling would be useful to include natively, or if you have a suggestion for an easier way to implement it (currently we just make many replicate bamlists, then follow the SAF -&gt; SFS -&gt; Theta/Fst workflow), please make a feature or pull request.</p> <p>Sample size of course also affects our other analyses, but many of the tools used here have documented these effects in their original papers, so are best discussed there.</p>"},{"location":"tutorial/#inbreeding-and-runs-of-homozygosity","title":"Inbreeding and Runs of Homozygosity","text":"<p>Next up, let's look at estimating inbreeding using identical by descent tracts (runs of homozygosity) with ngsF-HMM. This tool uses a model based approach, that assumes your samples are in Hardy-Weinberg Equilibrium (except for inbreeding). It also assumes independence of sites. So, for our default pathway, we call SNPs and prune them per population defined in the sample file. Enabling this analysis is a single config line:</p> config/config.yaml<pre><code>  inbreeding_ngsf-hmm: true\n</code></pre> <p>Let's also take a look at the settings for the tool in the config file:</p> config/config.yaml<pre><code>  ngsf-hmm:\n    estimate_in_pops: true\n    prune: true\n    max_kb_dist_pruning_pop: 25 # used only for ngsf-hmm\n    pruning_min-weight_pop: 0.4 # used only for ngsf-hmm\n    min_roh_length: 50000\n    roh_bins: [ 100000, 250000, 500000, 1000000, 2500000, 5000000 ] # in ascending order, bins are between adjacent values and between min_roh_length and value 1 and the last value and infinity\n</code></pre> <p>Here, we can toggle whether we want to run the tool for each population (<code>true</code>) or for the dataset (<code>false</code>), whether we want to prune, and if so, what settings to use for pruning (remember, that background LD is often higher within populations, so probably requires a higher min weight than for the dataset or pruning may take a very long time or prune away too many sites). We will set them to the values we came up with in the LD decay section.</p> <p>We also have a few options about runs of homozygosity sizes. ngsF-HMM will output a model based value for the inbreeding coefficient per sample. Many studies, especially in conservation genomics, calculate the inbreeding coefficient as the proportion of the genome in runs of homozygosity greater than a certain size (F<sub>RoH</sub>). PopGLen will calculate F<sub>RoH</sub> for you from the tracts inferred by ngsF-HMM while keeping the F estimates from ngsF-HMM in the original outputs. F<sub>RoH</sub> will be estimated from the runs of homozygosity larger than <code>min_roh_length</code> and the autosomal length estimated from the reference, excluding any excluded contigs. A plot will be made to show the composition of F<sub>RoH</sub> by size classes of runs of homozygosity, given as a list of integers to <code>roh_bins</code>. This is just for plotting in the report, you still get the raw files from ngsF-HMM for downstream analyses. The settings here work fine for the tutorial.</p> <p>Now, let's give it a run and see what we get (example report). Note that we are looking for long runs of homozygosity, yet our tutorial dataset is from a tiny subset of the genome. In fact, I've included no continuous section of the genome longer than 3Mb, so it will be impossible to find tracts longer than this. Still, if you generate a report and check the plots under '06 Inbreeding' you should see that at least some runs of homozygosity were found:</p> <p></p> <p>This figure shows the total inbreeding coefficient for each individual (total height of the bar), grouped by population. This bar is segmented into a grayscale gradient, with each shade representing a range of IBD tract lengths. These sizes can tell us a bit about the time to the common ancestor of the identical by descent segments, so seeing how much of the inbreeding coefficient comes from IBD tracts of different sizes can help to understand inbreeding patterns in the population, with more recent inbreeding being composed of longer tracts, whereas shorter tracts illustrate historical or background inbreeding. Since the sizes that matter will depend on the recombination rate of your organism, these bins can be customized in the config.</p> <p>But this can be a bit hard to see in the tutorial dataset, since we have little information to actually estimate these tracts from. Let's take a look at the same samples, but using the whole genome to get a better idea of what we might see in this plot:</p> <p></p> <p>Here, we can see more of the variation in F<sub>RoH</sub> across individuals, as well as what size classes we estimate contribute to this. We can see that the modern populations have individuals with higher inbreeding coefficients than the historical, in line with the genetic diversity declines and increases in population structure we've already seen. Most of the coefficients come from IBD tracts shorter than 1Mb, which likely means that background relatedness has just increased, rather than close-kin matings, for which we would see more long IBD tracts.</p> <p>A second type of plot is available in the report that looks at the relationship between the number of runs of homozygosity and their cumulative length. This helps to identify individuals that have recent, close-kin inbreeding contributing to their inbreeding coefficients, as these individuals will have fewer, but longer runs of homozygosity, putting them more towards the bottom right of the plot (we will just look at the one for the full data as this plot really doesn't show anything in the tutorial subset):</p> <p></p> <p>We don't really see this much here, there's quite a linear relationship between the number of runs of homozygosity and their cumulative length. This is in line with what we saw in the barplot: most of the inbreeding coefficient comes from IBD tracts that are relatively short rather than long.</p>"},{"location":"tutorial/#identity-by-state-matrix","title":"Identity by state matrix","text":"<p>Some tools that are popular with low coverage datasets take an identity by state matrix as input, such as EEMS (Petkova et al. Nat. Genet.). ANGSD has a nice way to calculate this by randomly sampling a base at each position per sample. This is implemented in a basic form here, essentially calculating an IBS matrix from SNPs estimated for the Beagle file. Enable it with the toggle in the config:</p> config/config.yaml<pre><code>  ibs_matrix: true\n</code></pre> <p>You can also change how the state is calculated (the setting of ANGSD's <code>-doIBS</code> option). The default is randomly sampling a base (<code>1</code>), but you can also use the consensus base (<code>2</code>):</p> config/config.yaml<pre><code>  ibs:\n    doibs: 1\n</code></pre> <p>There's no plots here, you just get the matrix in your results folder at <code>results/datasets/{dataset}/analyses/IBS/{dataset}.{ref}_{population}{dp}_{sites}-filts.ibsMat</code> if you run it. Feel free to give it a try on the tutorial data and see what it looks like.</p>"},{"location":"tutorial/#population-specific-allele-frequencies","title":"Population-specific allele frequencies","text":"<p>While the SFS gives us a summary of the allele frequency distribution within a population, you may wish to have allele frequencies for each site within each population. This can be estimated using ANGSD by enabling the <code>pop_allele_freqs</code> option in the config:</p> config/config.yaml<pre><code>  pop_allele_freqs: true\n</code></pre> <p>This will generate minor allele frequency counts for each population using the <code>-doMaf</code> option in ANGSD. This will utilize the ANGSD settings in the config, in particular <code>domajorminor</code>, <code>domaf</code>, <code>snp_pval</code>, and <code>min_maf</code>. Enabling this analysis in the pipeline generates two files per population, one with the allele frequencies for SNPs segregating across the entire dataset and one for SNPs that are segregating within the population. Let's enable and run this, then look at the following output files. These are long tables, so aren't in the report, and we can just compare the first few lines.</p> <p>First, let's examine the output for the 2021 western Sk\u00e5ne population, looking at the sites that are segregating in the population, allowing the population to decide the major allele. This file will have <code>pop-maj</code> towards the end. You can see the first few lines with the command:</p> <p><code>zcat results/datasets/popglen-tutorial/mafs/popglen-tutorial.tutorial-ref_WSkane2021_allsites-filts.pop-maj.mafs.gz| head -n 5</code></p> results/datasets/popglen-tutorial/mafs/popglen-tutorial.tutorial-ref_WSkane2021_allsites-filts.pop-maj.mafs.gz<pre><code>chromo  position  major  minor  ref  anc  knownEM  pK-EM         nInd\n1       638       A      C      A    A    0.285317 0.000000e+00  5\n1       713       T      A      T    T    0.137963 9.956036e-12  4\n1       749       G      T      G    G    0.216111 2.936186e-10  5\n1       960       A      T      A    A    0.382065 0.000000e+00  5\n</code></pre> <p>We have a few columns here, the chromosome, position, major allele, minor allele, reference allele, ancestral allele, the allele frequency from the optimization process assuming a known major and minor, the p-value for the frequency and the number of individuals used to calculate the frequency.</p> <p>Now, let's look at the file produced using sites segregating across the dataset, allowing the dataset to determine the major allele. This file will have <code>dataset-maj</code> at the end and we can look at the first few lines with the command:</p> <p><code>zcat results/datasets/popglen-tutorial/mafs/popglen-tutorial.tutorial-ref_WSkane2021_allsites-filts.dataset-maj.mafs.gz| head -n 5</code></p> results/datasets/popglen-tutorial/mafs/popglen-tutorial.tutorial-ref_WSkane2021_allsites-filts.dataset-maj.mafs.gz<pre><code>chromo  position  major  minor  ref  anc  knownEM   nInd\n1       581       C      A      C    C    0.000000  5\n1       593       C      G      C    C    0.000000  5\n1       638       A      C      A    A    0.285317  5\n1       749       G      T      G    G    0.216111  5\n</code></pre> <p>The columns here are the same as in the previous file, except for a missing pK-EM column. This is because we just used the SNPs found in our dataset Beagle file, so we didn't actually calculate the SNP p-value. The rows are also different. We have two additional SNPs at positions 581 and 593, and we are missing the SNP at 713. This is because we are getting frequencies for SNPs called across the whole dataset. At position 581 and 593, this population is invariable, so we have an allele frequency (<code>knownEM</code>) of 0. In the previous file, we had SNPs that were called within the population, so these were not included as they were invariable. However, they are variable somewhere in the dataset, so they are included here. This means that the <code>dataset-maj</code> files for all populations will be exactly the same length, the major and minor allele will always be the same, and the allele frequency can range from 0-1. This is likely the file you will want for downstream analyses, as it allows direct comparison of frequencies at SNPs across your dataset.</p> <p>Now, what happened to the SNP at position 713? Well, this was variable in our population, but at low frequency: ~0.14 across four samples, which is likely just a singleton. If this singleton was the only time the minor allele appeared in the dataset, it likely was removed by the minor allele frequency or SNP p-value filter when calling SNPs across the dataset. This means that the <code>pop-maj</code> files will all be different lengths, and even at the same positions may have different major and minor alleles. In this case, the major and minor alleles across the dataset and the population are the same, so the frequencies at those positions (638 and 749) are also the same. Let's try to find a position where they are not the same. The easiest way to do this is to look for positions in the <code>dataset-maj</code> where the frequency is higher than 0.5, as these are sites where the minor allele across the dataset is the major allele within the population. This command should get us there:</p> <p><code>zcat results/datasets/popglen-tutorial/mafs/popglen-tutorial.tutorial-ref_WSkane2021_allsites-filts.dataset-maj.mafs.gz | awk '$7&gt;0.6' | head -n 2</code></p> results/datasets/popglen-tutorial/mafs/popglen-tutorial.tutorial-ref_WSkane2021_allsites-filts.dataset-maj.mafs.gz<pre><code>chromo  position  major  minor  ref  anc  knownEM   nInd\n1       5953      G      C      G    G    0.646212  5\n</code></pre> <p>Let's see what this SNP looks like in the <code>pop-maj</code> file:</p> <p><code>zcat results/datasets/popglen-tutorial/mafs/popglen-tutorial.tutorial-ref_WSkane2021_allsites-filts.pop-maj.mafs.gz | awk '$1==\"1\" &amp;&amp; $2==\"5953\"'</code></p> <pre><code>1       5953      C      G      G    G    0.353786  3.540612e-11  5\n</code></pre> <p>Notice that the major and minor are reversed, and the allele frequency is the inverse of that in the <code>dataset-maj</code> file. This is because the major and minor are defined by the frequency of the population, rather than the dataset.</p> <p>Hopefully this clarifies the two different formats you'll get. Most of the time, you'll want <code>dataset-maj</code>, as it lets you compare across all your populations directly at every SNP. <code>pop-maj</code> may be useful if you have downstream applications where you don't need to worry about keeping all the same sites, just having some per-site frequencies for a population. For example, the allele frequency based relatedness method in NgsRelate only requires population-specific allele frequencies to estimate relatedness within populations, so we pass it the <code>pop-maj</code> file along with the Beagle file in PopGLen.</p> Allele frequencies using reference/alternate instead of major/minor <p>If you are used to thinking of your SNPs in terms of reference and alternate alleles, and would prefer to polarize your frequencies this way, you can simply set <code>domajorminor</code> to <code>4</code> in the config. This will ensure the major allele always matches the reference, and the minor will be the inferred minor allele, just as it would be in Samtools. This will mean any analyses that use Beagle files will also get their input polarized this way, but as far as I am aware that won't affect the outcomes, except that you may lose some sites if the segregating alleles in the dataset do not include the reference allele.</p>"},{"location":"tutorial/#subsampling-sequencing-depth","title":"Subsampling sequencing depth","text":"<p>While genotype likelihoods can be very good at handling datasets with variable sequencing depth, it may be valuable to check that your results are not predominantly driven by sequencing depth variation. This is especially of concern for datasets with historical and modern samples where the modern 'treatment' is expected to be higher depth, or for studies combining genomic data from multiple sources.</p> <p>PopGLen allows you to rerun any of the included analyses after subsampling the BAM files to a lower sequencing depth with the following section of the config:</p> config/config.yaml<pre><code>#==================== Downsampling Configuration ======================#\n\nsubsample_dp: []\n\nsubsample_by: \"sitefilt\" # three options: unfilt, mapqbaseq, sitefilt; sitefilt recommended (subsamples bams to `subsample_dp` within filtered regions)\nredo_depth_filts: false # has no effect when `subsample_by` == \"sitefilt\"\ndrop_samples: [] # These samples won't be included in the downsampled dataset\n\nsubsample_analyses:\n  estimate_ld:\n  ld_decay:\n  pca_pcangsd:\n  admix_ngsadmix:\n  relatedness:\n    ibsrelate_ibs:\n    ibsrelate_sfs:\n    ngsrelate_ibsrelate-only:\n    ngsrelate_freqbased:\n  1dsfs:\n  1dsfs_boot:\n  2dsfs:\n  2dsfs_boot:\n  thetas_angsd:\n  heterozygosity_angsd:\n  fst_angsd:\n    populations:\n    individuals:\n  inbreeding_ngsf-hmm:\n  ibs_matrix:\n  pop_allele_freqs:\n</code></pre> <p>Here, you can give any number of mean sequencing depths to subsample to as a list of integers to <code>subsample_dp</code>. Then, enable any analyses you want to perform at subsampled depth. So, say we want to rerun our PCA and thetas with all samples subsampled to 3X mean sequencing depth (a bit higher than our lowest depth samples in this dataset), we would set it up like this:</p> config/config.yaml<pre><code>#==================== Downsampling Configuration ======================#\n\nsubsample_dp: [3]\n\nsubsample_by: \"sitefilt\" # three options: unfilt, mapqbaseq, sitefilt; sitefilt recommended (subsamples bams to `subsample_dp` within filtered regions)\nredo_depth_filts: false # has no effect when `subsample_by` == \"sitefilt\"\ndrop_samples: [] # These samples won't be included in the downsampled dataset\n\nsubsample_analyses:\n  estimate_ld:\n  ld_decay:\n  pca_pcangsd: true\n  admix_ngsadmix:\n  relatedness:\n    ibsrelate_ibs:\n    ibsrelate_sfs:\n    ngsrelate_ibsrelate-only:\n    ngsrelate_freqbased:\n  1dsfs:\n  1dsfs_boot:\n  2dsfs:\n  2dsfs_boot:\n  thetas_angsd: true\n  heterozygosity_angsd:\n  fst_angsd:\n    populations:\n    individuals:\n  inbreeding_ngsf-hmm:\n  ibs_matrix:\n  pop_allele_freqs:\n</code></pre> <p>If you would like, you can run this and see what happens. Several analyses will run run, but these aren't re-runs, rather they are new version of the analyses using BAM files subsampled to 3X filtered mean sequencing depth.</p> <p>If we generate the report and look at the QC tables for sequencing depth, we will see a new one for '3X' which is the QC for our new subsampled BAM files, you'll see that in the <code>allsites-filts.depth</code> column, we have a mean depth of ~3 for all samples, except those already lower than 3X, so the subsampling worked well. The other mean depth estimates are not uniform across samples and aren't 3. In order to ensure the sequencing depth is the same in the actual analyses, we use the mean sequencing depth only at sites passing our filter to determine how to subsample. If we used the other measures of depth that include regions outside the filtered sites file, we would get uniform depth in those measures, but not at the filtered sites, which means depth would still vary in the actual analyses. This is still possible to do, and is what the <code>subsample_by</code> option selects, so check the configuration documentation if you're interested in that.</p>"},{"location":"tutorial/#suggested-workflow-batches","title":"Suggested workflow batches","text":"<p>This tutorial has done two approaches that are a bit extreme - one is to run everything all at once, the other is to run each analysis almost one by one. If you do the first, you risk running some analyses before you know that your samples or settings are good. If you do the second one, you still get a lot of benefits from PopGLen (single file configuration, automated reproducible flow of inputs and outputs, software management), but will have to manually start it up several times.</p> <p>Likely, you will want something in between - check the results at key points that minimize the number of runs you'll do. When I do projects, I tend to follow this working path with PopGLen:</p> <ol> <li>Run all mapping and main QC. Determine if any samples need to be dropped    based on quality or some other concern and drop them if so. Also turn on my    expected filter sets and see if the filtered proportion of the genome makes    sense (I usually use ~30% of the butterfly genome, removing repeats, keeping    only complete autosomal chromosomes, and removing extreme depth). Usually,    the amount I use is not a whole lot lower than the amount left over after    repeat removal, if it is, the reference may be too distant to map well to.</li> <li>If I am not confident in my LD pruning settings, I run the LD decay analysis.    In this run I can also turn on anything that doesn't involve LD pruning, even    window based analyses as they are quick to rerun if their settings change. If    I already feel my LD pruning settings will be good enough (usually from    previous work), I might skip this batch. This is also a good point to do a    relatedness check to find out if any individuals should be removed from the    PCA and Admix, as those require pruning, but the relatedness methods do not.</li> <li>Next, using the LD results I set my pruning setting and run everything else.</li> <li>Run analyses at subsampled depth if desired.</li> </ol> <p>If all goes well, nothing will need to be modified and you can start working on your analyses downstream of PopGLen. Your config, samples, and units files together make a reproducible record of your analyses, allowing everything to be reproduced using your input data. If you want to change any settings or samples, you can simply update the configs and re-run, what needs to be updated will get updated.</p> <p>If PopGLen will form a large part of your workflow, I recommend using it as a module in the way we did with Snakedeploy in this tutorial. This allows you to add rules to the Snakefile for your downstream analyses. This makes it relatively easy to share your code with a manuscript in a way that will enable reproducing all steps. You can check out repositories for two papers I have used early versions of PopGLen for zjnolen/polyommatini-landcover-diversity and zjnolen/polyommatini-temporal-genomics for ideas on how to do this.</p>"}]}